{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8f2b7228-4fb3-4f06-b3ec-6ca0c2e3e50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "783e5990-fe21-44b7-9824-1a514588722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "44022779-7159-444d-a142-c9864cae7649",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = getpass()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "32be5697-7000-4c36-9b0c-ce186367e5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceInferenceAPI(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000001DBC6AA5AC0>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x000001DBBAE5B100>, completion_to_prompt=<function default_completion_to_prompt at 0x000001DBBAEC2660>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, model_name='mistralai/Mixtral-8x7B-Instruct-v0.1', token='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', timeout=None, headers=None, cookies=None, task=None, context_window=3900, num_output=256, is_chat_model=False, is_function_calling_model=False)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create llm model\n",
    "from llama_index.llms.huggingface import HuggingFaceInferenceAPI\n",
    "llm = HuggingFaceInferenceAPI(model_name=\"mistralai/Mixtral-8x7B-Instruct-v0.1\", token=HF_TOKEN)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0bb8f4ae-ba77-4870-a53e-a769c7a93e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import MetadataMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6cc35a21-49a8-4102-b865-f631e8c18b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.extractors import (\n",
    "    SummaryExtractor,\n",
    "    QuestionsAnsweredExtractor,\n",
    "    TitleExtractor,\n",
    "    KeywordExtractor,\n",
    "    BaseExtractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d6dfacbd-94e8-47d5-b855-5c981336e827",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.extractors.entity import EntityExtractor\n",
    "from llama_index.core.node_parser import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a9de50eb-0186-463d-ab52-a207d7745eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(\n",
    "    separator=\" \", chunk_size=512, chunk_overlap=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4373fbc3-1848-456d-a9ef-80f3a3f49e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractors = [\n",
    "    TitleExtractor(nodes=5, llm=llm),\n",
    "    QuestionsAnsweredExtractor(questions=3, llm=llm),\n",
    "    # EntityExtractor(prediction_threshold=0.5),\n",
    "    # SummaryExtractor(summaries=[\"prev\", \"self\"], llm=llm),\n",
    "    # KeywordExtractor(keywords=10, llm=llm),\n",
    "    # CustomExtractor()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9aa72381-54c8-4c9c-b034-5ec6a3d79b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = [text_splitter] + extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "debfdeba-474e-424d-a760-974f0b9f76a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8cde79d5-b0a5-4e0f-9dc5-59ead0ad5d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-54' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=ClientResponseError(RequestInfo(url=URL('https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B'), method='POST', headers=<CIMultiDictProxy('Host': 'api-inference.huggingface.co', 'user-agent': 'unknown/None; hf_hub/0.20.3; python/3.12.3; torch/2.3.0', 'authorization': 'Bearer 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', 'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate', 'Content-Length': '1762', 'Content-Type': 'application/json')>, real_url=URL('https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B')), (), status=403, message='Forbidden', headers=<CIMultiDictProxy('Date': 'Thu, 02 May 2024 13:52:55 GMT', 'Content-Type': 'application/json', 'Content-Length': '222', 'Connection': 'keep-alive', 'Access-Control-Allow-Credentials': 'true', 'Vary': 'Origin, Access-Control-Request-Method, Access-Control-Request-Headers', 'x-request-id': 'qGsOjGNEyYZGk9PxAHPnw')>)>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py\", line 357, in async_wrapper\n",
      "    result = await func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\async_utils.py\", line 116, in worker\n",
      "    return await job\n",
      "           ^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py\", line 307, in async_wrapper\n",
      "    result = await func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\llms\\llm.py\", line 519, in apredict\n",
      "    response = await self.acomplete(formatted_prompt, formatted=True)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\llms\\huggingface\\base.py\", line 673, in acomplete\n",
      "    response = await self._async_client.text_generation(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py\", line 1557, in text_generation\n",
      "    raise_text_generation_error(e)\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_text_generation.py\", line 534, in raise_text_generation_error\n",
      "    raise http_error\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py\", line 1532, in text_generation\n",
      "    bytes_output = await self.post(json=payload, model=model, task=\"text-generation\", stream=stream)  # type: ignore\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py\", line 257, in post\n",
      "    raise error\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py\", line 226, in post\n",
      "    response.raise_for_status()\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\aiohttp\\client_reqrep.py\", line 1070, in raise_for_status\n",
      "    raise ClientResponseError(\n",
      "aiohttp.client_exceptions.ClientResponseError: 403, message='Forbidden', url=URL('https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B')\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-52' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=ClientResponseError(RequestInfo(url=URL('https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B'), method='POST', headers=<CIMultiDictProxy('Host': 'api-inference.huggingface.co', 'user-agent': 'unknown/None; hf_hub/0.20.3; python/3.12.3; torch/2.3.0', 'authorization': 'Bearer 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', 'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate', 'Content-Length': '2715', 'Content-Type': 'application/json')>, real_url=URL('https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B')), (), status=403, message='Forbidden', headers=<CIMultiDictProxy('Date': 'Thu, 02 May 2024 13:52:55 GMT', 'Content-Type': 'application/json', 'Content-Length': '222', 'Connection': 'keep-alive', 'Vary': 'Origin, Access-Control-Request-Method, Access-Control-Request-Headers', 'x-request-id': '0d2HmtCfhlv3jJz5ZbeGl', 'Access-Control-Allow-Credentials': 'true')>)>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py\", line 357, in async_wrapper\n",
      "    result = await func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\async_utils.py\", line 116, in worker\n",
      "    return await job\n",
      "           ^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py\", line 307, in async_wrapper\n",
      "    result = await func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\llms\\llm.py\", line 519, in apredict\n",
      "    response = await self.acomplete(formatted_prompt, formatted=True)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\llms\\huggingface\\base.py\", line 673, in acomplete\n",
      "    response = await self._async_client.text_generation(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py\", line 1557, in text_generation\n",
      "    raise_text_generation_error(e)\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_text_generation.py\", line 534, in raise_text_generation_error\n",
      "    raise http_error\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py\", line 1532, in text_generation\n",
      "    bytes_output = await self.post(json=payload, model=model, task=\"text-generation\", stream=stream)  # type: ignore\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py\", line 257, in post\n",
      "    raise error\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py\", line 226, in post\n",
      "    response.raise_for_status()\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\aiohttp\\client_reqrep.py\", line 1070, in raise_for_status\n",
      "    raise ClientResponseError(\n",
      "aiohttp.client_exceptions.ClientResponseError: 403, message='Forbidden', url=URL('https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B')\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-64' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=ClientResponseError(RequestInfo(url=URL('https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B'), method='POST', headers=<CIMultiDictProxy('Host': 'api-inference.huggingface.co', 'user-agent': 'unknown/None; hf_hub/0.20.3; python/3.12.3; torch/2.3.0', 'authorization': 'Bearer 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', 'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate', 'Content-Length': '2629', 'Content-Type': 'application/json')>, real_url=URL('https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B')), (), status=403, message='Forbidden', headers=<CIMultiDictProxy('Date': 'Thu, 02 May 2024 13:53:16 GMT', 'Content-Type': 'application/json', 'Content-Length': '222', 'Connection': 'keep-alive', 'x-request-id': 'PNEVCvcD6M4jePJd8Un_K', 'Vary': 'Origin, Access-Control-Request-Method, Access-Control-Request-Headers', 'Access-Control-Allow-Credentials': 'true')>)>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py\", line 357, in async_wrapper\n",
      "    result = await func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\async_utils.py\", line 116, in worker\n",
      "    return await job\n",
      "           ^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py\", line 307, in async_wrapper\n",
      "    result = await func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\llms\\llm.py\", line 519, in apredict\n",
      "    response = await self.acomplete(formatted_prompt, formatted=True)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\llms\\huggingface\\base.py\", line 673, in acomplete\n",
      "    response = await self._async_client.text_generation(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py\", line 1557, in text_generation\n",
      "    raise_text_generation_error(e)\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_text_generation.py\", line 534, in raise_text_generation_error\n",
      "    raise http_error\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py\", line 1532, in text_generation\n",
      "    bytes_output = await self.post(json=payload, model=model, task=\"text-generation\", stream=stream)  # type: ignore\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py\", line 257, in post\n",
      "    raise error\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py\", line 226, in post\n",
      "    response.raise_for_status()\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\aiohttp\\client_reqrep.py\", line 1070, in raise_for_status\n",
      "    raise ClientResponseError(\n",
      "aiohttp.client_exceptions.ClientResponseError: 403, message='Forbidden', url=URL('https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B')\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-63' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=ClientResponseError(RequestInfo(url=URL('https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B'), method='POST', headers=<CIMultiDictProxy('Host': 'api-inference.huggingface.co', 'user-agent': 'unknown/None; hf_hub/0.20.3; python/3.12.3; torch/2.3.0', 'authorization': 'Bearer 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', 'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate', 'Content-Length': '2715', 'Content-Type': 'application/json')>, real_url=URL('https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B')), (), status=403, message='Forbidden', headers=<CIMultiDictProxy('Date': 'Thu, 02 May 2024 13:53:16 GMT', 'Content-Type': 'application/json', 'Content-Length': '222', 'Connection': 'keep-alive', 'Vary': 'Origin, Access-Control-Request-Method, Access-Control-Request-Headers', 'Access-Control-Allow-Credentials': 'true', 'x-request-id': 'WdPp6AULE7WblFwBOb_FA')>)>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py\", line 357, in async_wrapper\n",
      "    result = await func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\async_utils.py\", line 116, in worker\n",
      "    return await job\n",
      "           ^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py\", line 307, in async_wrapper\n",
      "    result = await func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\llms\\llm.py\", line 519, in apredict\n",
      "    response = await self.acomplete(formatted_prompt, formatted=True)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\llms\\huggingface\\base.py\", line 673, in acomplete\n",
      "    response = await self._async_client.text_generation(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py\", line 1557, in text_generation\n",
      "    raise_text_generation_error(e)\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_text_generation.py\", line 534, in raise_text_generation_error\n",
      "    raise http_error\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py\", line 1532, in text_generation\n",
      "    bytes_output = await self.post(json=payload, model=model, task=\"text-generation\", stream=stream)  # type: ignore\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py\", line 257, in post\n",
      "    raise error\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py\", line 226, in post\n",
      "    response.raise_for_status()\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\aiohttp\\client_reqrep.py\", line 1070, in raise_for_status\n",
      "    raise ClientResponseError(\n",
      "aiohttp.client_exceptions.ClientResponseError: 403, message='Forbidden', url=URL('https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B')\n"
     ]
    }
   ],
   "source": [
    "uber_docs = SimpleDirectoryReader(input_files=[\"CFR.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "97c8fd17-79e8-4877-b25a-a6c52cb5f2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_front_pages = uber_docs[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1dd23602-9b2b-4251-b031-974530394ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_content = uber_docs[63:69]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "677f29c3-3881-4092-b8c7-bc68bd86488b",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_docs = uber_front_pages + uber_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "bb590513-fe29-49d2-aca1-12695b938051",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  7.45it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00,  5.05it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  9.97it/s]\n",
      "100%|██████████| 12/12 [00:53<00:00,  4.47s/it]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "pipeline = IngestionPipeline(transformations=transformations)\n",
    "uber_nodes = pipeline.run(documents=uber_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "db57fcd6-1f54-4209-9e0a-ad8ac429f439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_label': '1',\n",
       " 'file_name': 'CFR.pdf',\n",
       " 'file_path': 'CFR.pdf',\n",
       " 'file_type': 'application/pdf',\n",
       " 'file_size': 4407076,\n",
       " 'creation_date': '2024-05-02',\n",
       " 'last_modified_date': '2024-05-02',\n",
       " 'document_title': ' Deep Counterfactual Regret Minimization for Imperfect-Information Games.',\n",
       " 'questions_this_excerpt_can_answer': \"1. What is the CFR algorithm and how does it converge to a Nash equilibrium in two-player zero-sum games?\\n2. How have forms of tabular CFR been used in the benchmark domain of poker and in the Annual Computer Poker Competition?\\n3. What are the limitations of using tabular CFR with abstraction and how does Deep Counterfactual Regret Minimization address these limitations?\\n\\n1. The CFR algorithm is an iterative method for finding a Nash equilibrium in two-player zero-sum games. It works by having each player simulate playing the game many times, keeping track of the regret they feel for not playing a different strategy. Over time, the regret decreases and the players' strategies converge to a Nash equilibrium.\\n2. Tabular CFR has been used in all recent milestones in the benchmark domain of poker and in all competitive agents in the Annual Computer Poker Competition for at least six years. It has been used to solve large imperfect-information games by abstracting them and solving the simplified game using tabular CFR. However, this approach requires extensive domain knowledge and the abstract solution may only be a coarse approximation of a\"}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uber_nodes[1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8b6cae55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex\n",
    "index = SummaryIndex.from_documents(uber_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "31626f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "12feae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = input(\"Please write your prompt here:\")\n",
    "\n",
    "response = query_engine.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "de6370ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "\n",
       "Counterfactual Regret Minimization (CFR) is an iterative algorithm that converges to a Nash equilibrium in any ﬁnite two-player zero-sum game. It is a theoretically grounded algorithm with a convergence bound of O(1√\n",
       "T) and has been used in practice to solve large imperfect-information games. CFR works by iteratively traversing the game tree and computing the counterfactual value of each player's actions at each infoset. The counterfactual value is the expected payoff to the player if they had taken a particular action, weighted by the probability that the player would have reached that infoset if they had tried to do so. The algorithm then updates the player's strategy based on the counterfactual values, using a regret minimization algorithm such as regret matching.\n",
       "\n",
       "In practice, faster convergence is achieved by using Monte Carlo CFR (MCCFR), in which only a portion of the game tree is traversed on each iteration. In MCCFR, a subset of nodes Qtin the game tree is sampled at each iteration, and sampled regrets ˜rtare tracked rather than exact regrets. For infosets that</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
