{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee2fe244-8fd6-4288-9ba3-8b8226a61b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create api key here\n",
    "# https://cloud.llamaindex.ai/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e0050d-efee-4611-8135-002f355f1a85",
   "metadata": {},
   "source": [
    "# LlamaParse\n",
    "- LlamaParse is an API created by LlamaIndex to efficiently parse and represent files for efficient retrieval and context augmentation using LlamaIndex frameworks.\n",
    "\n",
    "#### NOTE: Currently, only PDF files are supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b45a500b-b761-41be-adf1-42fe47535cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b75daf0-3959-4420-a176-68c964abff85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_parse import LlamaParse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bba053aa-1150-4ffd-9008-3c3eac0df553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LLAMA_CLOUD_API_KEY\"] = \"llx-ZZPmYFkICGNlltwVQIcraiwYVGAzbV11ETl5meJyoeG9D03o\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68d00cc0-e624-472f-af10-44adbdcd50c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 3394k  100 3394k    0     0  5757k      0 --:--:-- --:--:-- --:--:-- 5783k\n"
     ]
    }
   ],
   "source": [
    "!curl -o attention.pdf \"https://arxiv.org/pdf/1811.00164\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c72491a3-6e95-4cc7-8d39-4d040ef6b665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id a355067c-1fd7-4978-b42f-ccab52ad46e5\n"
     ]
    }
   ],
   "source": [
    "# llama-parse is async-first, running the sync code in a notebook requires the use of nest_asyncio\n",
    "# As a text result type\n",
    "from llama_parse import LlamaParse\n",
    "documents = LlamaParse(result_type=\"text\").load_data(\"./attention.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9fd86ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='d11cfdd8-73c8-4837-a387-cbb527e79601', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='arXiv:1811.00164v3 [cs.AI] 22 May 2019\\n\\n\\n\\n                                                     Deep Counterfactual Regret Minimization\\n\\n\\n\\n                                                                   * 1 2                   * 2               2                            1\\n\\n\\nNoam Brown Adam Lerer Sam Gross Tuomas Sandholm\\n                                               Abstract                                         been used in all recent milestones in the benchmark domain\\n                       Counterfactual Regret Minimization (CFR) is the                          of poker (Bowling et al., 2015; Morav cˇ´ık et al., 2017; Brown\\n                       leading framework for solving large imperfect-                           & Sandholm, 2017) and have been used in all competitive\\n                       information games. It converges to an equilibrium                        agents in the Annual Computer Poker Competition going\\n                       by iteratively traversing the game tree. In order                        back at least six years.      1  In order to deal with extremely\\n                       to deal with extremely large games, abstraction                          large imperfect-information games, abstraction is typically\\n                       is typically applied before running CFR. The ab-                         used to simplify a game by bucketing similar states together\\n                       stracted game is solved with tabular CFR, and its                        and treating them identically. The simplified (abstracted)\\n                       solution is mapped back to the full game. This                           game is approximately solved via tabular CFR. However,\\n                       process can be problematic because aspects of                            constructing an effective abstraction requires extensive do-\\n                       abstraction are often manual and domain specific,                        main knowledge and the abstract solution may only be a\\n                       abstraction algorithms may miss important strate-                        coarse approximation of a true equilibrium.\\n                       gic nuances of the game, and there is a chicken-                         In constrast, reinforcement learning has been successfully\\n                       and-egg problem because determining a good ab-                           extended to large state spaces by using function approx-\\n                       straction requires knowledge of the equilibrium                          imation with deep neural networks rather than a tabular\\n                       of the game. This paper introduces Deep Counter-                         representation of the policy (deep RL). This approach has\\n                       factual Regret Minimization, a form of CFR that                          led to a number of recent breakthroughs in constructing\\n                       obviates the need for abstraction by instead using                       strategies in large MDPs (Mnih et al., 2015) as well as in\\n                       deep neural networks to approximate the behavior                         zero-sum perfect-information games such as Go (Silver\\n                       of CFR in the full game. We show that Deep CFR                           et al., 2017; 2018).    2 Importantly, deep RL can learn good\\n                       is principled and achieves strong performance in                         strategies with relatively little domain knowledge for the\\n                       large poker games. This is the first non-tabular                         specific game (Silver et al., 2017). However, most popular\\n                       variant of CFR to be successful in large games.                          RL algorithms do not converge to good policies (equilibria)\\n                                                                                                in imperfect-information games in theory or in practice.\\n                 1. Introduction                                                                Rather than use tabular CFR with abstraction, this paper\\n                 Imperfect-information games model strategic interactions                       introduces a form of CFR, which we refer to as Deep Coun-\\n                 between multiple agents with only partial information. They                    terfactual Regret Minimization, that uses function approx-\\n                 are widely applicable to real-world domains such as negoti-                    imation with deep neural networks to approximate the be-\\n                 ations, auctions, and cybersecurity interactions. Typically                    havior of tabular CFR on the full, unabstracted game. We\\n                 in such games, one wishes to find an approximate equilib-                      prove that Deep CFR converges to an \\x0f-Nash equilibrium\\n                 rium in which no player can improve by deviating from the                      in two-player zero-sum games and empirically evaluate per-\\n                 equilibrium.                                                                   formance in poker variants, including heads-up limit Texas\\n                                                                                                hold’em. We show Deep CFR outperforms Neural Ficti-\\n                 The most successful family of algorithms for imperfect-                        tious Self Play (NFSP) (Heinrich & Silver, 2016), which\\n                 information games have been variants of Counterfactual                         was the prior leading function approximation algorithm for\\n                 Regret Minimization (CFR) (Zinkevich et al., 2007). CFR is                     imperfect-information games, and that Deep CFR is com-\\n                 an iterative algorithm that converges to a Nash equilibrium                    petitive with domain-specific tabular abstraction techniques.\\n                 in two-player zero-sum games. Forms of tabular CFR have                            1www.computerpokercompetition.org\\n                    *Equal contribution2    1Computer Science Department, Carnegie                  2Deep RL has also been applied successfully to some partially\\n                 Mellon University      Facebook AI Research. Correspondence to:                observed games such as Doom (Lample & Chaplot, 2017), as long\\n                 Noam Brown <noamb@cs.cmu.edu>.                                                 as the hidden information is not too strategically important.\\n\\n\\n\\n                 Proceedings of the 36      th International Conference on Machine\\n                 Learning, Long Beach, California, PMLR 97, 2019. Copyright\\n                 2019 by the author(s).\\n---\\n                                                    Deep Counterfactual Regret Minimization\\n2. Notation and Background                                                       of a strategy σ   p in a two-player zero-sum game is how much\\nIn an imperfect-information extensive-form (that is, tree-                       worse σ    p does versus BR(σ ∗       p) compared to how a∗Nash\\n                                                                                 equilibrium strategy σ        p does against BR(σ         p). Formally,\\nform) game there is a finite set of players, P. A node                           e(σ  p) = u    p( σ ∗, BR(σ    ∗) ) − u  p (σ p, BR(σ    p) ). We mea-\\n(or history) h is defined by all information of the current                                         p           p\\nsituation, including private knowledge known to only one                         sure total exploitability      ∑  p∈P   e(σ p )3.\\nplayer. A(h) denotes the actions available at a node and\\nP (h) is either chance or the unique player who acts at′that                     2.1. Counterfactual Regret Minimization (CFR)\\nnode. If action a ∈ A(h) leads from h to h , then we write                       CFR is an iterative algorithm that converges to a Nash equi-\\n            ′                       ′\\nh · a = h . We write h @ h           if a sequence of actions leads              librium in any finite two-player zero-sum game with a the-\\nfrom h to h . H′is the set of all nodes. Z ⊆ H are terminal                      oretical convergence bound of O(              √1T ). In practice CFR\\nnodes for which no actions are available. For each player                        converges much faster. We provide an overview of CFR be-\\np ∈ P, there is a payoff function u             p  : Z → R. In this              low; for a full treatment, see Zinkevich et al. (2007). Some\\npaper we assume P = {1, 2} and u               1 = −u    2 (the game is          recent forms of CFR converge in O(                1   ) in self-play set-\\ntwo-player zero-sum). We denote the range of payoffs in                                                                         T 0.75\\n                                                                                 tings (Farina et al., 2019), but are slower in practice so we\\nthe game by ∆.                                                                   do not use them in this paper.\\nImperfect information is represented by information sets                         Let σ   t be the strategy profile on iteration t. The counter-\\n(infosets) for each player p ∈ P. For any infoset I be-                          factual value v    σ (I) of player p = P (I) at I is the expected\\nlonging to p, all nodes h, h        ′ ∈ I are indistinguishable to               payoff to p when reaching I, weighted by the probability\\np. Moreover, every non-terminal node h ∈ H belongs to                            that p would reached I if she tried to do so that iteration.\\nexactly one infoset for each p. We represent the set of all                      Formally,\\ninfosets belonging to p where p acts by I               p. We call the\\nset of all terminal nodes with a prefix in I as Z              I, and we                   vσ (I ) =   ∑     π σ  (z[I ])π  σ (z[I ] → z)u     (z)       (1)\\ncall the particular prefix z[I ]. We assume the game features                                         z∈Z  I   −p                            p\\nperfect recall, which means if h and h          ′do not share a player\\np infoset then all nodes following h do not share a player p                     and v   σ (I, a) is the same except it assumes that player p\\ninfoset with any node following h           ′.                                   plays action a at infoset I with 100% probability.\\n\\n\\n\\nA strategy (or policy) σ(I) is a probability vector over ac-                     The instantaneous regret r        t(I, a) is the difference between\\ntions for acting player p in infoset I. Since all states in an                   P (I)’s counterfactual value from playing a vs. playing σ\\ninfoset belonging to p are indistinguishable, the strategies                     on iteration t\\nin each of them must be identical. The set of actions in I is\\ndenoted by A(I). The probability of a particular action a is                                        rt(I, a) = v    σt(I, a) − v   σ t(I)                (2)\\ndenoted by σ(I, a). We define σ            p to be a strategy for p in\\nevery infoset in the game where p acts. A strategy profile σ                     The counterfactual regret for infoset I action a on iteration\\nis a tuple of strategies, one for each player. The strategy of                   T is\\nevery player other than p is represented as σ          −p . u p(σ p , σ−p )                            R  T (I, a) =   ∑ T  r t(I, a)                    (3)\\nis the expected payoff for p if player p plays according to\\nσ p and the other players play according to σ            −p .                                                          t=1\\nπ σ (h) = Π       ′     σ     ′ (h  ′, a) is called reach and is the             Additionally, R     T (I, a) = max{R       T  (I, a), 0} and R    T (I ) =\\n                h ·avh    P (h )                                                                     +\\nprobability h is reached if all players play according to σ.                     max   a {R  T (I, a)}. Total regret for p in the entire game is\\nπ σ (h) is the contribution of p to this probability. π             σ   (h)      R  T  = max    σ ′ ∑  T    (u p(σ  ′, σt   ) − u  p(σ  t, σt   )).\\n  p                                                                 −p              p            p     t=1          p   −p             p    −p\\nis the contribution of chance and all players other than p.                      CFR determines an iteration’s strategy by applying any of\\nFor an infoset I belonging to p, the probability of reaching                     several regret minimization algorithms to each infoset (Lit-\\nI if p chooses actions leading toward I but chance and all                       tlestone & Warmuth, 1994; Chaudhuri et al., 2009). Typi-\\nplayers other than p play according to σ             −p   is denoted by\\nπ σ  (I) =    ∑       π σ   (h). For h v z, define π       σ (h → z) =           cally, regret matching (RM) is used as the regret minimiza-\\n  −p′        ′   h∈I    −p  ′                                                    tion algorithm within CFR due to RM’s simplicity and lack\\nΠ                σ      ′ (h , a)\\n  h  ·avz,h 6@h    P (h )                                                        of parameters (Hart & Mas-Colell, 2000).\\nA best response to σ        −p   is a player p strategy BR(σ           −p )\\nsuch that u    p (BR(σ    −p  ), σ−p  )  = max     σ′  up (σ ′, σ−p  ). A        In RM, a player picks a distribution over actions in an in-\\n                            ∗                       p        p                   foset in proportion to the positive regret on those actions.\\nNash equilibrium σ             is a strategy profile where ev-                   Formally, on each iteration t + 1, p selects actions a ∈ A(I)\\neryone plays a best response:                  ∀p, u   p (σ ∗ , σ∗  )    =\\n                                                            p    −p\\nmax   σ ′ up (σ ′ , σ∗  ) (Nash, 1950). The exploitability e(σ           p)          3Some prior papers instead measure average exploitability\\n        p       p    −p                                                          rather than total (summed) exploitability.\\n---\\n                                                     Deep Counterfactual Regret Minimization\\n\\n\\n\\naccording to probabilities                                                        who is traversing the game tree on the iteration as the tra-\\n                                         R  t (I, a)                              verser. Regrets are updated only for the traverser on an\\n                σ t+1 (I, a) =    ∑         +     t       ′             (4)       iteration. At infosets where the traverser acts, all actions are\\n                                       ′        R + (I, a )                       explored. At other infosets and chance nodes, only a single\\n                                     a ∈A(I)\\nIf ∑    ′       R  t (I, a ′) = 0 then any arbitrary strategy may                 action is explored.\\n       a ∈A(I)     +\\nbe chosen. Typically each action is assigned equal proba-                         External-sampling MCCFR probabilistically converges to\\nbility, but in this paper we choose the action with highest                       an equilibrium. For any ρ ∈ (0, 1], total regret is bounded\\n                                                                                        T      (      √ 2 )        √       √\\ncounterfactual regret with probability 1, which we find em-                       by R  p  ≤    1 +   √ ρ  |I p |∆    |A|    T with probability 1 − ρ.\\npirically helps RM better cope with approximation error\\n(see Figure 4).                                                                   3. Related Work\\nIf a player plays according to regret matching in in-             T               CFR is not the only iterative algorithm capable of solving\\nfoset I on every iteration, then on iteration T , R                 (I) ≤\\n∆  √  |A(I )| √  T (Cesa-Bianchi & Lugosi, 2006). Zinkevich                       large imperfect-information games. First-order methods\\net al. (2007) show that the sum of the counterfactual regret                      converge to a Nash equilibrium in O(1/T ) (Hoda et al.,\\nacross all infosets upper bounds the total regret. Therefore,                     2010; Kroer et al., 2018b;a), which is far better than CFR’s\\nif player p plays according to CFR on every iteration, then                       theoretical bound. However, in practice the fastest variants\\nR  T  ≤  ∑         R  T (I). So, as T → ∞,        R pT  → 0.                      of CFR are substantially faster than the best first-order meth-\\n   p         I∈I p                                 T                              ods. Moreover, CFR is more robust to error and therefore\\nThe average strategy σ¯       T (I) for an infoset I on iteration T               likely to do better when combined with function approxima-\\n                ∑  T   (  σt  p   t   )                                           tion.\\n     T             t=1   πp  (I)σ p(I)\\nis σ¯p (I) =         ∑ T   π σ t(I)     .                                         Neural Fictitious Self Play (NFSP) (Heinrich & Silver,\\n                       t=1   p\\nIn two-player zero-sum games, if both players’ average                            2016) previously combined deep learning function approx-\\n                           R T                                                    imation with Fictitious Play (Brown, 1951) to produce an\\ntotal regret satisfies      Tp  ≤ \\x0f, then their average strategies                AI for heads-up limit Texas hold’em, a large imperfect-\\n〈¯σ T, ¯σT 〉 form a 2\\x0f-Nash equilibrium (Waugh, 2009). Thus,                      information game. However, Fictitious Play has weaker\\n    1    2\\nCFR constitutes an anytime algorithm for finding an \\x0f-Nash                        theoretical convergence guarantees than CFR, and in prac-\\nequilibrium in two-player zero-sum games.                                         tice converges slower. We compare our algorithm to NFSP\\nIn practice, faster convergence is achieved by alternating                        in this paper. Model-free policy gradient algorithms have\\nwhich player updates their regrets on each iteration rather                       been shown to minimize regret when parameters are tuned\\nthan updating the regrets of both players simultaneously                          appropriately (Srinivasan et al., 2018) and achieve perfor-\\neach iteration, though this complicates the theory (Farina                        mance comparable to NFSP.\\net al., 2018; Burch et al., 2018). We use the alternating-                        Past work has investigated using deep learning to esti-\\nupdates form of CFR in this paper.                                                mate values at the depth limit of a subgame in imperfect-\\n                                                                                  information games (Morav cˇ´ık et al., 2017; Brown et al.,\\n2.2. Monte Carlo Counterfactual Regret Minimization                               2018). However, tabular CFR was used within the sub-\\nVanilla CFR requires full traversals of the game tree, which                      games themselves. Large-scale function approximated CFR\\nis infeasible in large games. One method to combat this is                        has also been developed for single-agent settings (Jin et al.,\\nMonte Carlo CFR (MCCFR), in which only a portion of                               2017). Our algorithm is intended for the multi-agent set-\\nthe game tree is traversed on each iteration (Lanctot et al.,                     ting and is very different from the one proposed for the\\n2009). In MCCFR, a subset of nodes Q               t in the game tree is          single-agent setting.\\ntraversed at each iteration, where Q         tt is sampled from some              Prior work has combined regression tree function approxi-\\ndistribution Q. Sampled regrets ˜r             are tracked rather than            mation with CFR (Waugh et al., 2015) in an algorithm called\\nexact regrets. For infosets that are sampled at iteration t,                      Regression CFR (RCFR). This algorithm defines a number\\n˜rt(I, a) is equal to r     t(I, a) divided by the probability of                 of features of the infosets in a game and calculates weights\\nhaving sampled I; for unsampled infosets r˜ (I, a) = 0.tSee                       to approximate the regrets that a tabular CFR implemen-\\nAppendix B for more details.                                                      tation would produce. Regression CFR is algorithmically\\nThere exist a number of MCCFR variants (Gibson et al.,                            similar to Deep CFR, but uses hand-crafted features similar\\n2012; Johanson et al., 2012; Jackson, 2017), but for this                         to those used in abstraction, rather than learning the features.\\npaper we focus specifically on the external sampling variant                      RCFR also uses full traversals of the game tree (which is\\ndue to its simplicity and strong performance. In external-                        infeasible in large games) and has only been evaluated on\\nsampling MCCFR the game tree is traversed for one player                          toy games. It is therefore best viewed as the first proof of\\nat a time, alternating back and forth. We refer to the player                     concept that function approximation can be applied to CFR.\\n---\\n                                                     Deep Counterfactual Regret Minimization\\n\\n\\n\\nConcurrent work has also investigated a similar combina-                          Once a player’s K traversals are completed, a new network\\ntion of deep learning with CFR, in an algorithm referred                          is trained from scratch to determine parameters θ             t  by mini-\\nto as Double Neural CFR (Li et al., 2018). However, that                          mizing MSE between predicted advantage V                 p (I,pa|θ t) and\\napproach may not be theoretically sound and the authors                           samples of instantaneous regrets from prior iterations t             ′ ≤ t\\nconsider only small games. There are important differences                        ˜t′\\n                                                                                   r (I, a) drawn from the memory. The average over all\\nbetween our approaches in how training data is collected                          sampled instantaneous advantages r˜           t′(I, a) is proportional\\nand how the behavior of CFR is approximated.                                      to the total sampled regret R       ˜ t(I, a) (across actions in an\\n                                                                                  infoset), so once a sample is added to the memory it is never\\n4. Description of the Deep Counterfactual                                         removed except through reservoir sampling, even when the\\n    Regret Minimization Algorithm                                                 next CFR iteration begins.\\nIn this section we describe Deep CFR. The goal of Deep                            One can use any loss function for the value and average\\nCFR is to approximate the behavior of CFR without calcu-                          strategy model that satisfies Bregman divergence (Banerjee\\nlating and accumulating regrets at each infoset, by general-                      et al., 2005), such as mean squared error loss.\\nizing across similar infosets using function approximation                       While almost any sampling scheme is acceptable so long\\nvia deep neural networks.                                                         as the samples are weighed properly, external sampling\\nOn each iteration t, Deep CFR conducts a constant num-                            has the convenient property that it achieves both of our\\nber K of partial traversals of the game tree, with the path                       desired goals by assigning all samples in an iteration equal\\nof the traversal determined according to external sampling                        weight. Additionally, exploring all of a traverser’s actions\\nMCCFR. At each infoset I it encounters, it plays a strategy                       helps reduce variance. However, external sampling may\\nσ t(I ) determined by regret matching on the output of a neu-                     be impractical in games with extremely large branching\\nral network V : I → R          |A| defined by parameters θ        t−1  that       factors, so a different sampling scheme, such as outcome\\ntakes as input the infoset I and outputs values V (I, a|θ         p   t−1 ).      sampling (Lanctot et al., 2009), may be desired in those\\nOur goal is for V (I, a|θ       t−1  ) to be approximately propor-                cases.\\ntional to the regret R     t−1 (I, a) that tabular CFR would have                 In addition to the value network, a separate policy network\\nproduced.                                                                         Π : I → R     |A|  approximates the average strategy at the end\\nWhen a terminal node is reached, the value is passed back up.                     of the run, because it is the average strategy played over all\\nIn chance and opponent infosets, the value of the sampled                         iterations that converges to a Nash equilibrium. To do this,\\naction is passed back up unaltered. In traverser infosets, the                    we maintain a separate memory M                Π  of sampled infoset\\nvalue passed back up is the weighted average of all action                        probability vectors for both players. Whenever an infoset\\nvalues, where action a’s weight is σ           t(I, a). This produces             I belonging to player p is traversed during the opposing\\nsamples of this iteration’s instantaneous regrets for various                     player’s traversal of the game tree via external sampling,\\nactions. Samples are added to a memory M                    v,p, where p          the infoset probability vector σ         t(I ) is added to M       Π  and\\nis the traverser, using reservoir sampling (Vitter, 1985) if                      assigned weight t.\\ncapacity is exceeded.                                                             If the number of Deep CFR iterations and the size of each\\nConsider a nice property of the sampled instantaneous re-                         value network model is small, then one can avoid training\\ngrets induced by external sampling:                                               the final policy network by instead storing each iteration’s\\n                                                                                  value network (Steinberger, 2019). During actual play, a\\nLemma 1. For external sampling MCCFR, the sampled                                 value network is sampled randomly and the player plays the\\ninstantaneous regrets are an unbiased estimator of the ad-                        CFR strategy resulting from the predicted advantages of that\\nvantage,ti.e. the difference in expected payoff for playingt                      network. This eliminates the function approximation error\\na vs σ  p (I) at I , assuming both players play σ            everywhere           of the final average policy network, but requires storing all\\nelse.                                                                             prior value networks. Nevertheless, strong performance and\\n\\n\\n\\n            [    t       ∣                ]     v σt (I, a) − v  σ t(I)           low exploitability may still be achieved by storing only a\\n               σ         ∣\\n  E Q∈Q   t  r˜p  (I, a)∣Z  I  ∩ Q  6= ∅    =          π σ t (I)         .        subset of the prior value networks (Jackson, 2016).\\n                                                         −p                       Theorem 1 states that if the memory buffer is sufficiently\\nThe proof is provided in Appendix B.2.                                            large, then with high probability Deep CFR will result in\\n                                                                                  average regret being bounded by a constant proportional to\\nRecent work in deep reinforcement learning has shown                              the square root of the function approximation error.\\nthat neural networks can effectively predict and generalize\\nadvantages in challenging environments with large state                           Theorem 1. Let T denote the number of Deep CFR itera-\\nspaces, and use that to learn good policies (Mnih et al.,                         tions, |A| the maximum number of actions at any infoset,\\n2016).                                                                            and K the number of traversals per iteration. Let L                  t  be\\n                                                                                                                                                       V\\n---\\n                                                     Deep Counterfactual Regret Minimization\\n\\n\\n\\nthe average MSE loss for V          p(I, a|θ  t) on a sample in M        V,p         Hole\\nat iteration t , and let L     t ∗  be the minimum loss achievable                                1 3\\nfor any function V . Let L     V  t − L   t ∗  ≤ \\x0f L .\\n                                  V       V                                          Board                                                  normalize     Fold\\n                                                                                                                                                          Call\\nIf the value memories are sufficiently large, then with proba-                                                     192  192                               Raise\\nbility 1 − ρ total regret at time T is bounded by                                           Rank Emb\\n                                                                                            Suit Emb      Bet Occurred                   Linear;\\n                                                                                            Card Emb           @LL                   FC := [Skip,]\\n                                                                                                           Bet Pot Frac\\n R T  ≤   ( 1 +   √ √  2  )  ∆|I   p|√  |A| √  T + 4T |I    p |√  |A|∆\\x0f   L                              Betting Position                ReLU\\n   p                 ρK                                                           Figure 1. The neural network architecture used for Deep CFR.\\n                                                                         (5)      The network takes an infoset (observed cards and bet history) as\\nwith probability 1 − ρ.                                                           input and outputs values (advantages or probability logits) for each\\n                                                      R T                         possible action.\\nCorollary 1. As T → ∞, average regret                  Tp  is bounded by\\n                            4|I  p|√  |A|∆\\x0f    L                                  (1-4), and a card embedding (1-52). These embeddings\\nwith high probability.                                                            are summed for each set of permutation invariant cards\\n                                                                                  (hole, flop, turn, river), and these are concatenated. In\\nThe proofs are provided in Appendix B.4.                                          each of the N     rounds rounds of betting there can be at most 6\\n                                                                                  sequential actions, leading to 6N           rounds total unique betting\\nWe do not provide a convergence bound for Deep CFR when                           positions. Each betting position is encoded by a binary\\nusing linear weighting, since the convergence rate of Linear                      value specifying whether a bet has occurred, and a float\\nCFR has not been shown in the Monte Carlo case. However,                          value specifying the bet size.\\nFigure 4 shows moderately faster convergence in practice.                         The neural network model begins with separate branches for\\n                                                                                  the cards and bets, with three and two layers respectively.\\n5. Experimental Setup                                                             Features from the two branches are combined and three\\nWe measure the performance of Deep CFR (Algorithm 1)                              additional fully connected layers are applied. Each fully-\\nin approximating an equilibrium in heads-up flop hold’em                          connected layer consists of x          i+1   = ReLU(Ax[+x]). The\\npoker (FHP). FHP is a large game with over 10                    12  nodes        optional skip connection [+x] is applied only on layers that\\nand over 10     9 infosets. In contrast, the network we use has                   have equal input and output dimension. Normalization (to\\n98,948 parameters. FHP is similar to heads-up limit Texas                         zero mean and unit variance) is applied to the last-layer\\nhold’em (HULH) poker, but ends after the second betting                           features. The network architecture was not highly tuned, but\\nround rather than the fourth, with only three community                           normalization and skip connections were used because they\\ncards ever dealt. We also measure performance relative to                         were found to be important to encourage fast convergence\\ndomain-specific abstraction techniques in the benchmark                           when running preliminary experiments on pre-computed\\ndomain of HULH poker, which has over 10                    17  nodes and          equilibrium strategies in FHP. A full network specification\\nover 10   14  infosets. The rules for FHP and HULH are given                      is provided in Appendix C.\\nin Appendix A.                                                                    In the value network, the vector of outputs represented pre-\\nIn both games, we compare performance to NFSP, which                              dicted advantages for each action at the input infoset. In the\\nis the previous leading algorithm for imperfect-information                       average strategy network, outputs are interpreted as logits\\ngame solving using domain-independent function approx-                            of the probability distribution over actions.\\nimation, as well as state-of-the-art abstraction techniques\\ndesigned for the domain of poker (Johanson et al., 2013;                          5.2. Model training\\nGanzfried & Sandholm, 2014; Brown et al., 2015).                                  We allocate a maximum size of 40 million infosets to each\\n                                                                                  player’s advantage memory M             V,p  and the strategy memory\\n5.1. Network Architecture                                                         M   Π . The value model is trained from scratch each CFR\\nWe use the neural network architecture shown in Figure 5.1                        iteration, starting from a random initialization. We perform\\nfor both the value network V that computes advantages for                         4,000 mini-batch stochastic gradient descent (SGD) itera-\\neach player and the network Π that approximates the final                         tions using a batch size of 10,000 and perform parameter\\naverage strategy. This network has a depth of 7 layers and                        updates using the Adam optimizer (Kingma & Ba, 2014)\\n98,948 parameters. Infosets consist of sets of cards and                          with a learning rate of 0.001, with gradient norm clipping\\nbet history. The cards are represented as the sum of three                        to 1. For HULH we use 32,000 SGD iterations and a batch\\nembeddings: a rank embedding (1-13), a suit embedding                             size of 20,000. Figure 4 shows that training the model from\\n---\\n                                                   Deep Counterfactual Regret Minimization\\n\\n\\n\\nAlgorithm 1 Deep Counterfactual Regret Minimization\\n   function DEEPCFR\\n        Initialize each player’s advantage network V (I, a|θ            p ) with parameters θ     p  so that it returns 0 for all inputs.\\n        Initialize reservoir-sampled advantage memories M                 V,1 , M  V,2 and strategy memory M          Π  .\\n        for CFR iteration t = 1 to T do\\n             for each player p do\\n                 for traversal k = 1 to K do\\n                      TRAVERSE(∅, p, θ       1, θ2 , M  V,p, M   Π )          . Collect data from a game traversal with external sampling\\n                                                                            ′          [  ′ ∑    (  t′                      ) 2]\\n                 Train θ  p from scratch on loss L(θ        p) = E   (I,t′,˜t            t         ˜r  (a) − V (I, a|θ   p )\\n                                                                            r )∼M  V,p         a\\n        Train θ  Π  on loss L(θ   Π ) = E   (I,t′,σt′)∼M   Π [ t′∑   a (σ t′(a) − Π(I, a|θ     Π )) 2 ]\\n        return θ   Π\\n\\n\\n\\nAlgorithm 2 CFR Traversal with External Sampling\\n   function TRAVERSE(h, p, θ            , θ , M    , M     , t)\\n                                      1   2      V      Π\\n        Input: History h, traverser player p, regret network parameters θ for each player, advantage memory M                              V for player\\n   p, strategy memory M         Π , CFR iteration t.\\n\\n\\n\\n        if h is terminal then\\n             return the payoff to player p\\n        else if h is a chance node then\\n             a ∼ σ(h)\\n             return TRAVERSE(h · a, p, θ         1, θ2 , M  V , M  Π , t)\\n        else if P (h) = p then                                                                                   . If it’s the traverser’s turn to act\\n             Compute strategy σ       t(I) from predicted advantages V (I (h), a|θ           p) using regret matching.\\n             for a ∈ A(h) do\\n                 v(a) ← TRAVERSE(h · a, p, θ           1, θ2 , M  V , M  Π , t)                                               . Traverse each action\\n             for a ∈ A(h) do            ∑                     ′        ′\\n                 ˜r(I, a) ← v(a) −          a′∈A(h)   σ(I, a ) · v(a )                                                       . Compute advantages\\n        else Insert the infoset and its action advantages (I, t, ˜r        t(I)) into the advantage memory M    . If it’sVthe opponent’s turn to act\\n             Compute strategy σ       t(I) from predicted advantages V (I (h),ta|θ           3−p ) using regret matching.\\n             Insert the infoset and its action probabilities (I, t, σ         (I)) into the strategy memory M            Π\\n             Sample an action a from the probability distribution σ              t(I).\\n             return TRAVERSE(h · a, p, θ         1, θ2 , M  V , M  Π , t)\\n\\n\\n\\nscratch at each iteration, rather than using the weights from                   not appear to lead to better performance asymptotically, but\\nthe previous iteration, leads to better convergence.                            does result in faster convergence in our experiments.\\n                                                                                LCFR is like CFR except iteration t is weighed by t. Specif-\\n5.3. Linear CFR                                                                 ically, we maintain a weight on each entry stored in the\\nThere exist a number of variants of CFR that achieve much                       advantage memory and the strategy memory, equal to t\\nfaster performance than vanilla CFR. However, most of                           when this entry was added. When training θ                p  each itera-\\nthese faster variants of CFR do not handle approximation                        tion T , we rescale all the batch weights by          2  and minimize\\nerror well (Tammelin et al., 2015; Burch, 2017; Brown &                         weighted error.                                       T\\nSandholm, 2019; Schmid et al., 2019). In this paper we use\\nLinear CFR (LCFR) (Brown & Sandholm, 2019), a variant                           6. Experimental Results\\nof CFR that is faster than CFR and in certain settings is\\nthe fastest-known variant of CFR (particularly in settings                      Figure 2 compares the performance of Deep CFR to\\nwith wide distributions in payoffs), and which tolerates                        different-sized domain-specific abstractions in FHP. The ab-\\napproximation error well. LCFR is not essential and does                        stractions are solved using external-sampling Linear Monte\\n---\\n                                                                         Deep Counterfactual Regret Minimization\\n\\n\\n\\n                  Carlo CFR (Lanctot et al., 2009; Brown & Sandholm, 2019),                             model. This is presumably because the model loss decreases\\n                  which is the leading algorithm in this setting. The 40,000                            as the number of training steps is increased per iteration (see\\n                  cluster abstraction means that the more than 10                   9 different         Theorem 1). Increasing the model size also decreases final\\n                  decisions in the game were clustered into 40,000 abstract                             exploitability up to a certain model size in FHP.\\n                  decisions, where situations in the same bucket are treated                            In Figure 4 we consider ablations of certain components of\\n                  identically. This bucketing is done using K-means clustering                          Deep CFR. Retraining the regret model from scratch at each\\n                  on domain-specific features. The lossless abstraction only                            CFR iteration converges to a substantially lower exploitabil-\\n                  clusters together situations that are strategically isomorphic                        ity than fine-tuning a single model across all iterations. We\\n                  (e.g., flushes that differ only by suit), so a solution to this                       suspect that this is because a single model gets stuck in bad\\n                  abstraction maps to a solution in the full game without error.                        local minima as the objective is changed from iteration to\\n                  Performance and exploitability are measured in terms of                               iteration. The choice of reservoir sampling to update the\\n                  milli big blinds per game (mbb/g), which is a standard                                memories is shown to be crucial; if a sliding window mem-\\n                  measure of win rate in poker.                                                         ory is used, the exploitability begins to increase once the\\n                  The figure shows that Deep CFR asymptotically reaches a                               memory is filled up, even if the memory is large enough to\\n                  similar level of exploitability as the abstraction that uses 3.6                      hold the samples from many CFR iterations.\\n                  million clusters, but converges substantially faster. Although                        Finally, we measure head-to-head performance in HULH.\\n                  Deep CFR is more efficient in terms of nodes touched, neu-                            We compare Deep CFR and NFSP to the approximate solu-\\n                  ral network inference and training requires considerable                              tions (solved via Linear Monte Carlo CFR) of three different-\\n                  overhead that tabular CFR avoids. However, Deep CFR                                   sized abstractions: one in which the more than 10                    14  deci-\\nExploitability (mbb/g)                                                                                                                           6\\n                  does not require advanced domain knowledge. We show                                   sions are clustered into 3.3 · 10          buckets, one in which there\\n                  Deep CFR performance for 10,000 CFR traversals per step.                              are 3.3·10    7 buckets and one in which there are 3.3·10             8  buck-\\n                  Using more traversals per step is less sample efficient and                           ets. The results are presented in Table 1. For comparison,\\n                  requires greater neural network training time but requires                            the largest abstractions used by the poker AI Polaris in its\\n                  fewer CFR steps.                                                                      2007 HULH man-machine competition against human pro-\\n                  Figure 2 also compares the performance of Deep CFR to                                 fessionals contained roughly 3·10            8 buckets. When variance-\\n                  NFSP, an existing method for learning approximate Nash                                reduction techniques were applied, the results showed that\\n                  equilibria in imperfect-information games. NFSP approx-                               the professional human competitors lost to the 2007 Polaris\\n                  imates fictitious self-play, which is proven to converge to                           AI by about 52 ± 10 mbb/g (Johanson, 2016). In contrast,\\n                  a Nash equilibrium but in practice does so far slower than                            our Deep CFR agent loses to a 3.3 · 10              8  bucket abstraction\\n                  CFR. We observe that Deep CFR reaches an exploitability                               by only −11 ± 2 mbb/g and beats NFSP by 43 ± 2 mbb/g.\\n                  of 37 mbb/g while NFSP converges to 47 mbb/g.                      4 We also                          Convergence of Deep CFR, NFSP, and Domain-Specific Abstractions\\n                  observe that Deep CFR is more sample efficient than NFSP.\\n                  However, these methods spend most of their wallclock time\\n                  performing SGD steps, so in our implementation we see a                                 103\\n                  less dramatic improvement over NFSP in wallclock time\\n                  than sample efficiency.\\n                  Figure 3 shows the performance of Deep CFR using differ-\\n                  ent numbers of game traversals, network SGD steps, and\\n                  model size. As the number of CFR traversals per iteration                               102      Deep CFR\\n                  is reduced, convergence becomes slower but the model con-                                        NFSP (1,000 infosets / update)\\n                                                                                                                   NFSP (10,000 infosets / update)\\n                  verges to the same final exploitability. This is presumably                                      Abstraction (40,000 Clusters)\\n                                                                                                                   Abstraction (368,000 Clusters)\\n                  because it takes more iterations to collect enough data to                                       Abstraction (3,644,000 Clusters)\\n                                                                                                                   Lossless Abstraction (234M Clusters)\\n                  reduce the variance sufficiently. On the other hand, reduc-                                   106           107           108          109           1010          1011\\n                  ing the number of SGD steps does not change the rate of                                                                     Nodes Touched\\n                  convergence but affects the asymptotic exploitability of the\\n                       4We run NFSP with the same model architecture as we use                          Figure 2. Comparison of Deep CFR with domain-specific tabular\\n                  for Deep CFR. In the benchmark game of Leduc Hold’em, our                             abstractions and NFSP in FHP. Coarser abstractions converge faster\\n                  implementation of NFSP achieves an average exploitability (total                      but are more exploitable. Deep CFR converges with 2-3 orders of\\n                  exploitability divided by two) of 37 mbb/g in the benchmark game                      magnitude fewer samples than a lossless abstraction, and performs\\n                  of Leduc Hold’em, which is substantially lower than originally                        competitively with a 3.6 million cluster abstraction. Deep CFR\\n                  reported in Heinrich & Silver (2016). We report NFSP’s best                           achieves lower exploitability than NFSP, while traversing fewer\\n                  performance in FHP across a sweep of hyperparameters.                                 infosets.\\n---\\nExploitability (mbb/g)                                                          Deep Counterfactual Regret Minimization\\n\\n\\n\\nExploitability (mbb/g)                                                                        Opponent Model                          Abstraction Size\\n                                      Model                     NFSP                   Deep CFR                  3.3 · 10   6              3.3 · 10  7               3.3 · 10  8\\n                                      NFSP                         -               −43 ± 2 mbb/g            −40 ± 2 mbb/g             −49 ± 2 mbb/g             −55 ± 2 mbb/g\\n                                      Deep CFR           +43 ± 2 mbb/g                       -                +6 ± 2 mbb/g             −6 ± 2 mbb/g             −11 ± 2 mbb/g\\n\\n\\n\\n                   Table 1. Head-to-head expected value of NFSP and Deep CFR in HULH against converged CFR equilibria with varying abstraction sizes.\\n                    For comparison, in 2007 an AI using abstractions of roughly 3 · 10                         8  buckets defeated human professionals by about 52 mbb/g (after\\n                    variance reduction techniques were applied).\\n\\n\\n\\n                   103                                                          103                                                                 dim=8\\n                                                               Traversals per iter                                          SGD steps per iter\\n                                                                   3,000                                                        1,000\\n                                                                   10,000                                                       2,000\\n                                                                   30,000                                                       4,000\\n                                                                   100,000                                                      8,000\\nExploitability (mbb/g)                                             300,000                                                      16,000\\n                                                                   1,000,000                                                    32,000         2\\n                   102                                             Linear CFR   102                                             Linear CFR   10               dim=16\\n\\n\\n\\n                                                                                                                                                                        dim=32   dim=64\\n\\n\\n\\n                                                                                                                                                                                        dim=128 dim=256\\n\\n\\n\\n                             101                        102                               101                        102                                      104               105               106\\nExploitability (mbb/g)                      CFR Iteration                                                CFR Iteration                                              # Model Parameters\\n                   Figure 3. Left: FHP convergence for different numbers of training data collection traversals per simulated LCFR iteration. The dotted\\n                    line shows the performance of vanilla tabular Linear CFR without abstraction or sampling. Middle: FHP convergence using different\\n                    numbers of minibatch SGD updates to train the advantage model at each LCFR iteration. Right: Exploitability of Deep CFR in FHP for\\n                    different model sizes. Label indicates the dimension (number of features) in each hidden layer of the model.\\n\\n\\n\\n                                                                Deep CFR (5 replicates)                               Deep CFR\\n                                     3                          Deep CFR without Linear Weighting             3       Deep CFR with Sliding Window Memories\\n                                  10                            Deep CFR without Retraining from Scratch    10\\nExploitability (mbb/g)                                          Deep CFR Playing Uniform when All Regrets < 0\\n\\n\\n\\n                                  10 2                                                                      102\\n\\n\\n\\n                                              101                          102                                         10 1                         10 2\\n                                                              CFR Iteration                                                             CFR Iteration\\n                   Figure 4. Ablations of Deep CFR components in FHP. Left: As a baseline, we plot 5 replicates of Deep CFR, which show consistent\\n                    exploitability curves (standard deviation at t = 450 is 2.25 mbb/g). Deep CFR without linear weighting converges to a similar\\n                    exploitability, but more slowly. If the same network is fine-tuned at each CFR iteration rather than training from scratch, the final\\n                    exploitability is about 50% higher. Also, if the algorithm plays a uniform strategy when all regrets are negative (i.e. standard regret\\n                    matching), rather than the highest-regret action, the final exploitability is also 50% higher. Right: If Deep CFR is performed using\\n                    sliding-window memories, exploitability stops converging once the buffer becomes full                             6 . However, with reservoir sampling, convergence\\n                    continues after the memories are full.\\n\\n\\n\\n                    7. Conclusions                                                                               for tabular methods and where abstraction is not straight-\\n                   We describe a method to find approximate equilibria in                                        forward. Extending Deep CFR to larger games will likely\\n                    large imperfect-information games by combining the CFR                                       require more scalable sampling strategies than those used in\\n                    algorithm with deep neural network function approxima-                                       this work, as well as strategies to reduce the high variance\\n                    tion. This method is theoretically principled and achieves                                   in sampled payoffs. Recent work has suggested promising\\n                    strong performance in large poker games relative to domain-                                  directions both for more scalable sampling (Li et al., 2018)\\n                    specific abstraction techniques without relying on advanced                                  and variance reduction techniques (Schmid et al., 2019). We\\n                    domain knowledge. This is the first non-tabular variant of                                   believe these are important areas for future work.\\n                    CFR to be successful in large games.\\n                    Deep CFR and other neural methods for imperfect-\\n                    information games provide a promising direction for tack-\\n                    ling large games whose state or action spaces are too large\\n---\\n                                                   Deep Counterfactual Regret Minimization\\nReferences                                                                      Ganzfried, S. and Sandholm, T. Potential-aware imperfect-\\nBanerjee, A., Merugu, S., Dhillon, I. S., and Ghosh, J. Clus-                      recall abstraction with earth mover’s distance in\\n   tering with bregman divergences. Journal of machine                             imperfect-information games. In AAAI Conference on\\n   learning research, 6(Oct):1705–1749, 2005.                                      Artificial Intelligence (AAAI), 2014.\\n                                                                                Gibson, R., Lanctot, M., Burch, N., Szafron, D., and Bowl-\\nBowling, M., Burch, N., Johanson, M., and Tammelin, O.                             ing, M. Generalized sampling and variance in coun-\\n   Heads-up limit hold’em poker is solved. Science, 347                            terfactual regret minimization. In Proceedins of the\\n   (6218):145–149, January 2015.                                                   Twenty-Sixth AAAI Conference on Artificial Intelligence,\\nBrown, G. W. Iterative solutions of games by fictitious play.                      pp. 1355–1361, 2012.\\n   In Koopmans, T. C. (ed.), Activity Analysis of Production                    Hart, S. and Mas-Colell, A. A simple adaptive procedure\\n   and Allocation, pp. 374–376. John Wiley & Sons, 1951.                           leading to correlated equilibrium. Econometrica, 68:\\nBrown, N. and Sandholm, T. Superhuman AI for heads-up                              1127–1150, 2000.\\n   no-limit poker: Libratus beats top professionals. Science,                   Heinrich, J. and Silver, D. Deep reinforcement learning\\n   pp. eaao1733, 2017.                                                             from self-play in imperfect-information games. arXiv\\nBrown, N. and Sandholm, T. Solving imperfect-information                           preprint arXiv:1603.01121, 2016.\\n   games via discounted regret minimization. In AAAI Con-                       Hoda, S., Gilpin, A., Pe na, J., and Sandholm, T. Smoothing˜\\n   ference on Artificial Intelligence (AAAI), 2019.                                techniques for computing Nash equilibria of sequential\\n                                                                                   games. Mathematics of Operations Research, 35(2):494–\\nBrown, N., Ganzfried, S., and Sandholm, T. Hierarchical ab-                        512, 2010. Conference version appeared in WINE-07.\\n   straction, distributed equilibrium computation, and post-                    Jackson, E. Targeted CFR. In AAAI Workshop on Computer\\n   processing, with application to a champion no-limit texas                       Poker and Imperfect Information, 2017.\\n   hold’em agent. In Proceedings of the 2015 International\\n   Conference on Autonomous Agents and Multiagent Sys-                          Jackson, E. G. Compact CFR. In AAAI Workshop on Com-\\n   tems, pp. 7–15. International Foundation for Autonomous                         puter Poker and Imperfect Information, 2016.\\n   Agents and Multiagent Systems, 2015.                                         Jin, P. H., Levine, S., and Keutzer, K. Regret minimiza-\\nBrown, N., Sandholm, T., and Amos, B. Depth-limited                                tion for partially observable deep reinforcement learning.\\n   solving for imperfect-information games. In Advances in                         arXiv preprint arXiv:1710.11424, 2017.\\n   Neural Information Processing Systems, 2018.                                 Johanson, M., Bard, N., Lanctot, M., Gibson, R., and\\nBurch, N. Time and Space: Why Imperfect Information                                Bowling, M. Efficient nash equilibrium approximation\\n   Games are Hard. PhD thesis, University of Alberta, 2017.                        through monte carlo counterfactual regret minimization.\\n                                                                                   In Proceedings of the 11th International Conference on\\nBurch, N., Moravcik, M., and Schmid, M. Revisiting cfr+                            Autonomous Agents and Multiagent Systems-Volume 2,\\n   and alternating updates. arXiv preprint arXiv:1810.11542,                       pp. 837–846. International Foundation for Autonomous\\n   2018.                                                                           Agents and Multiagent Systems, 2012.\\nCesa-Bianchi, N. and Lugosi, G. Prediction, learning, and                       Johanson, M., Burch, N., Valenzano, R., and Bowling,\\n   games. Cambridge University Press, 2006.                                        M. Evaluating state-space abstractions in extensive-form\\n                                                                                   games. In Proceedings of the 2013 International Con-\\nChaudhuri, K., Freund, Y., and Hsu, D. J. A parameter-free                         ference on Autonomous Agents and Multiagent Systems,\\n   hedging algorithm. In Advances in neural information                            pp. 271–278. International Foundation for Autonomous\\n   processing systems, pp. 297–305, 2009.                                          Agents and Multiagent Systems, 2013.\\nFarina, G., Kroer, C., and Sandholm, T. Online convex opti-                     Johanson, M. B. Robust Strategies and Counter-Strategies:\\n   mization for sequential decision processes and extensive-                       From Superhuman to Optimal Play. PhD thesis, Univer-\\n   form games. In AAAI Conference on Artificial Intelli-                           sity of Alberta, 2016.\\n   gence (AAAI), 2018.                                                          Kingma, D. P. and Ba, J. Adam: A method for stochastic\\nFarina, G., Kroer, C., Brown, N., and Sandholm, T. Stable-                         optimization. arXiv preprint arXiv:1412.6980, 2014.\\n   predictive optimistic counterfactual regret minimization.                    Kroer, C., Farina, G., and Sandholm, T. Solving large\\n   In International Conference on Machine Learning, 2019.                          sequential games with the excessive gap technique. In\\n---\\n                                                    Deep Counterfactual Regret Minimization\\n\\n\\n\\n   Advances in Neural Information Processing Systems, pp.                        Schmid, M., Burch, N., Lanctot, M., Moravcik, M., Kadlec,\\n   864–874, 2018a.                                                                  R., and Bowling, M. Variance reduction in monte carlo\\n                                                                                    counterfactual regret minimization (VR-MCCFR) for ex-\\nKroer, C., Waugh, K., Kılınc¸ -Karzan, F., and Sandholm, T.                         tensive form games using baselines. In AAAI Conference\\n   Faster algorithms for extensive-form game solving via                            on Artificial Intelligence (AAAI), 2019.\\n   improved smoothing functions. Mathematical Program-\\n   ming, pp. 1–33, 2018b.                                                        Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou,\\n                                                                                    I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M.,\\nLample, G. and Chaplot, D. S. Playing FPS games with                                Bolton, A., et al. Mastering the game of go without\\n   deep reinforcement learning. In AAAI, pp. 2140–2146,                             human knowledge. Nature, 550(7676):354, 2017.\\n   2017.                                                                         Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai,\\nLanctot, M. Monte carlo sampling and regret minimization                            M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Grae-\\n   for equilibrium computation and decision-making in large                         pel, T., et al. A general reinforcement learning algorithm\\n   extensive form games. 2013.                                                      that masters chess, shogi, and go through self-play. Sci-\\nLanctot, M., Waugh, K., Zinkevich, M., and Bowling, M.                              ence, 362(6419):1140–1144, 2018.\\n   Monte Carlo sampling for regret minimization in exten-                        Srinivasan, S., Lanctot, M., Zambaldi, V., P erolat, J., Tuyls,´\\n   sive games. In Proceedings of the Annual Conference                              K., Munos, R., and Bowling, M. Actor-critic policy opti-\\n   on Neural Information Processing Systems (NIPS), pp.                             mization in partially observable multiagent environments.\\n   1078–1086, 2009.                                                                 In Advances in Neural Information Processing Systems,\\nLi, H., Hu, K., Ge, Z., Jiang, T., Qi, Y., and Song, L. Double                      pp. 3426–3439, 2018.\\n   neural counterfactual regret minimization. arXiv preprint                     Steinberger, E. Single deep counterfactual regret minimiza-\\n   arXiv:1812.10607, 2018.                                                          tion. arXiv preprint arXiv:1901.07621, 2019.\\nLittlestone, N. and Warmuth, M. K. The weighted majority                         Tammelin, O., Burch, N., Johanson, M., and Bowling, M.\\n   algorithm. Information and Computation, 108(2):212–                              Solving heads-up limit texas hold’em. In Proceedings\\n   261, 1994.                                                                       of the International Joint Conference on Artificial Intelli-\\n                                                                                    gence (IJCAI), pp. 645–652, 2015.\\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,                      Vitter, J. S. Random sampling with a reservoir. ACM\\n   J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje-                         Transactions on Mathematical Software (TOMS), 11(1):\\n   land, A. K., Ostrovski, G., et al. Human-level control                           37–57, 1985.\\n   through deep reinforcement learning. Nature, 518(7540):\\n   529, 2015.                                                                    Waugh, K. Abstraction in large extensive games. Master’s\\nMnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap,                           thesis, University of Alberta, 2009.\\n   T., Harley, T., Silver, D., and Kavukcuoglu, K. Asyn-                         Waugh, K., Morrill, D., Bagnell, D., and Bowling, M. Solv-\\n   chronous methods for deep reinforcement learning. In                             ing games with functional regret estimation. In AAAI\\n   International conference on machine learning, pp. 1928–                          Conference on Artificial Intelligence (AAAI), 2015.\\n   1937, 2016.                                                                   Zinkevich, M., Johanson, M., Bowling, M. H., and Pic-\\nMorav cˇ´ık, M., Schmid, M., Burch, N., Lis ´y, V., Morrill, D.,                    cione, C. Regret minimization in games with incomplete\\n   Bard, N., Davis, T., Waugh, K., Johanson, M., and Bowl-                          information. In Proceedings of the Annual Conference\\n   ing, M. Deepstack: Expert-level artificial intelligence in                       on Neural Information Processing Systems (NIPS), pp.\\n   heads-up no-limit poker. Science, 2017. ISSN 0036-8075.                          1729–1736, 2007.\\n   doi: 10.1126/science.aam6960.\\n\\n\\n\\nMorrill, D. R. Using Regret Estimation to Solve Games\\n   Compactly. PhD thesis, University of Alberta, 2016.\\n\\n\\n\\nNash, J. Equilibrium points in n-person games. Proceedings\\n   of the National Academy of Sciences, 36:48–49, 1950.\\n\\n\\n\\nPaszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E.,\\n   DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer,\\n   A. Automatic differentiation in pytorch. 2017.\\n---\\n                                                        Deep Counterfactual Regret Minimization\\n   A. Rules for Heads-Up Limit Texas Hold’em and Flop Hold’em Poker\\n   Heads-up limit Texas hold’em is a two-player zero-sum game. There are two players and the position of the two players\\n   alternate after each hand. On each betting round, each player can choose to either fold, call, or raise. Folding results in the\\n   player losing and the money in the pot being awarded to the other player. Calling means the player places a number of chips\\n   in the pot equal to the opponent’s share. Raising means that player adds more chips to the pot than the opponent’s share. A\\n   round ends when a player calls (if both players have acted). There cannot be more than three raises in the first or second\\n   betting round or more than four raises in the third or fourth betting round, so there is a limited number of actions in the\\n   game. Raises in the first two rounds are $100 and raises in the second two rounds are $200.\\n   At the start of each hand of HULH, both players are dealt two private cards from a standard 52-card deck. P                                   1 must place\\n   $50 in the pot and P       2 must place $100 in the pot. A round of betting then occurs starting with P                       1 . When the round ends,\\n   three community cards are dealt face up that both players can ultimately use in their final hands. Another round of betting\\n   occurs, starting with P       2  this time. Afterward another community card is dealt face up and another betting round occurs.\\n   Then a final card is dealt face up and a final betting round occurs. At the end of the betting round, unless a player has folded,\\n   the player with the best five-card poker hand constructed from their two private cards and the five community cards wins the\\n   pot. In the case of a tie, the pot is split evenly.\\n   Flop Hold’em Poker is identical to HULH except there are only the first two betting rounds.\\n\\n\\n\\n   B. Proofs of Theorems\\n   B.1. Review of MCCFR\\n   We begin by reviewing the derivation of convergence bounds for external sampling MCCFR from Lanctot et al. 2009.\\n   An MCCFR scheme is completely specified by a set of blocks Q = {Q                              i} which each comprise a subset of all terminal\\n   histories Z. On each iteration MCCFR samples one of these blocks, and only considers terminal histories within that block.\\n   Let q  j > 0 be the probability of considering block Q               j in an iteration.\\n   Let Z   I be the set of terminal nodes that contain a prefix in I, and let z[I] be that prefix. Define π                   σ(h → z) as the probability\\n   of playing to z given that player p is at node h with both players playing σ.\\n\\n\\n\\n                                                           π σ (h → z) =       ∑     π σ (z[I])  π σ(z).\\n                                                                              z∈Z  I   π σ(I)\\n\\n\\n\\n   π σ (I → z) is undefined when π(I) = 0.\\n   Let q(z) =     ∑   j:z∈Q  j qj be the probability that terminal history z is sampled in an iteration of MCCFR. For external sampling\\n   MCCFR, q(z) = π          σ  (z).\\n   The sampled value v˜     −iσ (I|j) when sampling block j is\\n\\n\\ni\\n                                                σ               ∑          1             σ            σ\\n                                               ˜v (I|j) =                      u p (z)π     (z[I])π    (z[I] → z)                                            (6)\\n                                                p            z∈Q  j∩Z  I q(z)           −p\\n\\n\\n\\n   For external sampling, the sampled value reduces to   ˜σ               ∑                 σ\\n                                                          v (I|j) =                up (z)π    (z[I] → z)                                                     (7)\\n                                                          p            z∈Q  j∩Z  I          p\\n\\n\\n\\n   The sampled value is an unbiased estimator of the true value v                   p(I). Therefore the sampled instantaneous regret ˜r              t(I, a) =\\n   ˜vσ t(I, a) − ˜   σ t                                          t\\n     p            v  p (I) is an unbiased estimator of r           (I, a).\\n   The sampled regret is calculated as R          ˜ T (I, a) =    ∑  T    ˜t\\n                                                                     t=1   r (I, a).\\n   We first state the general bound shown in (Lanctot, 2013), Theorem 3.\\n---\\n                                                  Deep Counterfactual Regret Minimization\\n\\n\\n\\nLanctot 2013 defines B      p to be a set with one element per distinct action sequence ~a played by p, containing all infosets that\\nmay arise when p plays ~a. M       p is then defined by    ∑  B∈B   p|B|. Let ∆ be the difference between the maximum and minimum\\npayoffs in the game.\\nTheorem 2. (Lanctot 2013, Theorem 3) For any p ∈ (0, 1], when using any algorithm in the MCCFR family such that for\\nall Q ∈ Q and B ∈ B       p ,                ∑    \\uf8eb    ∑       π σ (z[I] → z)π    σ  (z[I])  \\uf8f6 2      1\\n                                             I∈B  \\uf8ed  z∈Q∩Z   I             q(z)   −p         \\uf8f8    ≤  δ2                                      (8)\\n\\n\\n\\nwhere δ ≤ 1, then with probability at least 1 − ρ, total regret is bounded by\\n                                               R T ≤   ( M    +   √  2|I p||B p|)  (  1 )  ∆ √  |A|T                                         (9)\\n.                                                p          p          √ ρ            δ\\n\\n\\n\\nFor the case of external sampling MCCFR, q(z) = π              σ  (z). Lanctot et al. 2009, Theorem 9 shows that for external sampling,\\n                                                               −i\\nfor which q(z) = π      σ  (z), the inequality in (8) holds for δ = 1, and thus the bound implied by (9) is\\n                        −i\\n\\n\\n\\n                              ¯T     (          √  2|I p||B p|)     √√ |A|\\n                             R p  ≤  ( M  p + √     )√ ρ      √  ∆      T                                                                  (10)\\n                                  ≤    1 + √     2     ∆|I  p|  √ |A|                  because |B    p| ≤ M   p ≤ |I  p|                   (11)\\n.                                              ρK                 T\\n\\n\\n\\nB.2. Proof of Lemma 1\\nWe show                                              [    t    ∣                ]        t        t\\n                                            E          ˜σ      ∣                       σ         σ\\n                                              Q  ∼Q     v  (I)∣Z  I ∩ Q  j 6= ∅   = v     (I)/π     (I).\\n                                                j       p                                        −p\\n\\n\\n\\nLet q j  = P (Q  [j).     ∣               ]        E  Q  ∼Q  [v˜σ t(I) ]\\n       E          ˜ σt    ∣                            j        p\\n         Q j∼Q     vp  (I)∣Z  I ∩ Q  6= ∅   =   P Q j∼Q  (Z I ∩ Q   j 6= ∅)\\n                                                ∑          qj ∑             u p(z)π  σ t(z[I])π  σ t(z[I] → z)/q(z)\\n                                                   Q j∈Q         z∈Z I ∩Q j          −p\\n                                            =                                   πσ t (I)\\n                                                ∑             ( ∑             qj)−pup (z)π σ t (z[I])π  σt(z[I] → z)/q(z)\\n                                            =      z∈Z  I∩Q  j     Q j:z∈Q  j      π σt (I)−p\\n                                                                                     −p\\n                                                ∑         q(z)u  p(z)π  σt (z[I])π  σ t(z[I] → z)/q(z)\\n                                                   z∈Z  I               −p\\n                                            =                            π σt (I)                                   By definition of q(z)\\n                                                v σt (I)                   −p\\n                                            =   π σt (I)\\n                                                  −p\\n\\n\\n\\nThe result now follows directly.\\n\\n\\n\\nB.3. K-external sampling\\nWe first show that performing MCCFR with K external sampling traversals per iteration (K-ES) shares a similar convergence\\nbound with standard external sampling (i.e. 1-ES). We will refer to this result in the next section when we consider the full\\n---\\n                                                              Deep Counterfactual Regret Minimization\\n\\n\\n\\n              Deep CFR algorithm. This convergence bound is rather obvious and the derivation pedantic, so the reader is welcome to\\n              skip this section.\\n              We model T rounds of K-external sampling as T × K rounds of external sampling, where at each round t · K + d (for\\n              integer t ≥ 0 and integer 0 ≤ d < K) we play\\n\\n\\n\\n                                                                              \\uf8f1  R+  (a)      +\\n                                                                              \\uf8f2   tK     if R       > 0\\n                                                                                 R +          Σ,tK\\n                                                              σ tK+d  (a) =        Σ,tK                                                              (12)\\n                                                                              \\uf8f3  arbitrary, otherwise\\n\\n\\n\\n              In prior work, σ is typically defined to play      1   when R   +   (a) ≤ 0, but in fact the convergence bounds do not constraint σ’s\\n                                                                |A|           Σ,T\\n              play in these situations, which we will demonstrate explicitly here. We need this fact because minimizing the loss L(V ) is\\n              defined only over the samples of (visited) infosets and thus does not constrain the strategy in unvisited infosets.\\n              Lemma 2. If regret matching is used in K-ES, then for 0 ≤ d < K\\n                                                                      ∑    R +  (a)r tK+d  (a) ≤ 0                                                   (13)\\n                                                                      a∈A    tK\\n\\n\\n\\n              Proof. If R   +     ≤ 0, then R   +  (a) = 0 for all a and the result follows directly. For R       +     > 0,\\n\\n\\nΣ,tK tK Σ,tK\\n                               ∑    R +  (a)r tK+d   (a) =  ∑    R + (a)(u  tK+d  (a) − u  tK+d  (σ tK ))                                            (14)\\n                               a∈A    tK                    a∈A    T\\n                                                         =  (  ∑   R  +  (a)u tK+d  (a) )  −  ( u tK+d  (σ tK )∑    R  +  (a))                       (15)\\n                                                            ( a∈A     tK                )     (                a∈A     tK   )\\n                                                               ∑      +                         ∑                                +\\n                                                         =    a∈A  R  tK (a)u tK+d  (a)    −    a∈A  σ tK+d  (a)u tK+d   (a)   R Σ,tK  (a)           (16)\\n                                                            (  ∑      +                 )     ( ∑      R +  (a)              )    +\\n                                                         =         R  tK (a)u tK+d  (a)    −          R +tK  (a) u tK+d  (a)    R Σ,tK (a)           (17)\\n                                                            ( a∈A                       )     ( a∈A     Σ,tK                  )\\n                                                         =     ∑   R  +  (a)u tK+d  (a)    −    ∑    R +   (a)(a)u  tK+d  (a)                        (18)\\n                                                         = 0  a∈A     tK                        a∈A    tK                                            (19)\\n\\n\\n\\n              Theorem 3. Playing according to Equation 12 guarantees the following bound on total regret\\n                                                                    ∑   (R +   (a)) 2  ≤ |A|∆   2K  2T                                               (20)\\n                                                                    a∈A    T K\\n\\n\\n\\n              Proof. We prove by recursion on T .    (                   K−1              ) 2\\n                          ∑    (R +   (a)) 2 ≤  ∑      R +        (a) +   ∑    rtK−d  (a)                                                            (21)\\n                          a∈A     T K           a∈A      (T −1)K          d=0\\n                                                    (                       K−1                           K−1 K−1                             )\\n                                             =  ∑     R  +       (a) 2 + 2  ∑    r d(a)R  +        (a) +  ∑     ∑    rT K−d  (a)r T K−d  ′(a)        (22)\\n                                                         (T −1)K                          (T −1)K                ′\\n                                                a∈A                         d=0                           d=0 d =0\\n\\n\\n\\n              By Lemma 2,\\n---\\n                                                Deep Counterfactual Regret Minimization\\n\\n\\n\\n                                                                                 K−1 K−1\\n                           ∑   (R  +   (a)) 2 ≤  ∑   (R  +        (a)) 2 +  ∑    ∑     ∑    rT K−d  (a)r  T K−d ′(a)                     (23)\\n                                   T K                   (T −1)K                        ′\\n                          a∈A                    a∈A                        a∈A d=0   d =0\\n\\n\\n\\nBy induction,                                     ∑   (R  +        (a)) 2 ≤ |A|∆   2(T − 1)                                              (24)\\n                                                  a∈A     (T −1)K\\n\\n\\n\\nFrom the definition, |r   T K−d  (a)| ≤ ∆\\n\\n\\n\\n                                    ∑       +       2           2                2       2          2   2\\n                                    a∈A (R  T K(a))   ≤ |A|∆     (T − 1) + K       |A|∆    = |A|∆     K   T                              (25)\\n\\n\\n\\nTheorem 4. (Lanctot 2013, Theorem 3 & Theorem 5) After T iterations of K-ES, average regret is bounded by\\n                                                  R¯ T K ≤   ( 1 + √  √ 2   ) |I p|∆  √√ |A|                                             (26)\\n                                                     p                 ρK                T\\n\\n\\n\\nwith probability 1 − ρ.\\n\\n\\n\\nProof. The proof follows Lanctot 2013, Theorem 3. Note that K-ES is only different from ES in terms of the choice of\\nσ T, and the proof in Lanctot 2013 only makes use of σ            T  via the bound on (    ∑    R T (a)) 2 that we showed in Theorem 3.\\nTherefore, we can apply the same reasoning to arrive at  R˜T K  ≤   ∆M   p√  |A|T K           a   +                                      (27)\\n(Lanctot 2013, Eq. (4.30)).                                p                δ\\n\\n\\n\\nLanctot et al. 2009 then shows that R     ˜ T K  and R  T K are similar with high probability, leading to\\n                                      \\uf8ee \\uf8eb   p           p                 \\uf8f6 2 \\uf8f9\\n                                    E \\uf8ef \\uf8ed  ∑    (R T K (I) − R ˜ T K (I)) \\uf8f8   \\uf8fa  ≤  2|I p||B p||A|T K∆    2                              (28)\\n                                      \\uf8f0   I∈I  p   p             p            \\uf8fb               δ 2\\n\\n\\n\\n(Lanctot 2013, Eq. (4.33), substituting T → T K).\\nTherefore, by Markov’s inequality, with probability at least 1 − ρ,\\n\\n\\n\\n                                           R T K  ≤  √  2|I p ||Bp ||A|T K∆    +   ∆M    √ |A|T K                                        (29)\\n                                             p                 δ √ ρ                       δ\\n, where external sampling permits δ = 1 (Lanctot, 2013).\\nUsing the fact that M ≤ |I      p| and |B p | < |I p| and dividing through by KT leads to the simplified form\\n                                                  R¯ T K ≤   ( 1 + √  √ 2   ) ∆|I  p| √√ |A|                                             (30)\\nwith probability 1 − ρ.                              p                 ρK                T\\n---\\n                                                   Deep Counterfactual Regret Minimization\\n\\n\\n\\nWe point out that the convergence of K-ES is faster as K increases (up to a point), but it still requires the same order of\\niterations as ES.\\n\\n\\n\\nB.4. Proof of Theorem 1\\nProof. Assume that an online learning scheme plays\\n\\n\\n\\n                                                            {    y t(I,a)       ∑\\n                                                               ∑   + t       if       yt (I, a) > 0\\n                                              σ t(I, a) =         a y+(I,a)         a  +                  .                                         (31)\\n                                                                arbitrary, otherwise\\n\\n\\n\\nMorrill 2016, Corollary 3.0.6 provides the following bound on the total regret as a function of the L2 distance between y                              +\\nand R  T,+   at each infoset.                                                                                                                          t\\n\\n\\n\\n                              max(R     T (I, a)) 2 ≤ |A|∆     2T + 4∆|A|      ∑T∑       √   (R t (I, a) − y   t (I, a)) 2                          (32)\\n                               a∈A                                             t=1 a∈A          +              +\\n                                                               2               ∑T∑       √      t             t        2\\n                                                    ≤ |A|∆      T + 4∆|A|      t=1 a∈A       (R  (I, a) − y (I, a))                                 (33)\\n\\n\\n\\nSince σ   t(I, a) from Eq. 31 is invariant to rescaling across all actions at an infoset, it’s also the case that for any C(I) > 0\\n\\n\\n\\n                                                                              T\\n                            max(R    T  (I, a)) 2 ≤ |A|∆    2 T + 4∆|A|     ∑ ∑ √         (R  t(I, a) − C(I)y     t(I, a)) 2                        (34)\\n                             a∈A                                             t=1 a∈A\\n\\n\\n\\nLet x t (I ) be an indicator variable that is 1 if I was traversed on iteration t. If I was traversed then ˜r              t(I) was stored in M      V,p ,\\n              t\\notherwise r˜ (I) = 0. Assume for now that M               V,p is not full, so all sampled regrets are stored in the memory.\\nLet Π  t(I) be the fraction of iterations on which x          t(I) = 1, and let\\n                                               t         ∥    [  t       t          ]               t ∥\\n                                              \\x0f (I) =    ∥E  t  r˜ (I)|x  (I) = 1     − V (I, a|θ    )∥ 2 .\\nInserting canceling factors of       ∑   t    x t′(I) and setting C(I) =        ∑  t     x t′(I), 7\\n                                         ′                                          ′\\n                                         t =1                                      t =1\\n                                                                     (               )       √   (                                 )\\n                                                                  T       t                  √                                       2\\n                                                                ∑       ∑       ′       ∑    √         R˜t(I, a)\\n                        ˜ T         2           2                              t             √                            t\\n                max( R      (I, a))    ≤|A|∆     T + 4∆|A|                   x   (I)                ∑  t       ′      − y (I, a)                    (35)\\n                 a∈A                                            t=1     t′=1            a∈A            t′=1  x t(I)\\n                                                2               ∑ T  (  ∑ t    t′    )  ∥    [  t       t          ]               t  ∥\\n                                       =|A|∆     T + 4∆|A|      t=1     t′=1 x   (I)    ∥ E t  ˜r (I)|x  (I) = 1     − V (I, a|θ    ) ∥2            (36)\\n                                                2               ∑ T      t     t\\n                                       =|A|∆     T + 4∆|A|      t=1  tΠ   (I)\\x0f (I)       by definition                                              (37)\\n                                       ≤|A|∆    2T + 4∆|A|T        ∑T   Π t(I )\\x0f t(I)                                                               (38)\\n                                                                   t=1                                                                              (39)\\n\\n\\n\\nThe first term of this expression is the same as Theorem 3, while the second term accounts for the approximation error.\\n\\n\\n\\n    7The careful reader may note that C(I) = 0 for unvisited infosets, but σ          t(I, a) can play an arbitrary strategy at these infosets so it’s\\nokay.\\n---\\n                                              Deep Counterfactual Regret Minimization\\n\\n\\n\\nIn the case of K-external sampling, the same derivation as shown in Theorem 3 leads to\\n\\n\\n\\n                                        ˜T        2          2     2        √          2 ∑T    t     t\\n                                max( R     (I, a))  ≤ |A|∆     T K   + 4∆      |A|T K        Π (I)\\x0f   (I)                    (40)\\nin this case. We elide the proof.a∈A                                                     t=1\\n\\n\\n\\nThe new regret bound in Eq. (40) can be plugged into Lanctot 2013, Theorem 3 as we do for Theorem 4, leading to\\n\\n\\n\\n                                          \\uf8eb  (             )                   √                          \\uf8f6\\n                                                     √          √              √          T\\n                                     ∑                  2          |A|      4  √         ∑\\n                             ¯ T          \\uf8ed         √            √         √   √                t     t   \\uf8f8\\n                             R p  ≤  I∈Ip      1 +    ρK     ∆      T   +    T     |A|∆  t=1  Π (I)\\x0f (I)                     (41)\\n\\n\\n\\nSimplifying the first term and rearranging,\\n\\n\\n\\n             (             )                                    √\\n                      √              √           √              √   T\\n       T                2               |A|    4   |A|∆    ∑    √∑\\n      ¯                                                         √         t    t\\n     R p  ≤  ( 1 + √   ρK  )  ∆|I p | √ T   +     √  T     I∈Ip    t=1 √Π (I)\\x0f (I)                                           (42)\\n                      √              √           √             ∑       √    T\\n                        2               |A|    4   |A|∆           I∈I p√  ∑\\n      ¯T                                                               √         t     t\\n     R p  ≤    1 + √   ρK     ∆|I p | √ T   +     √  T    |Ip | |I p|     t=1  Π (I)\\x0f   (I)        Adding canceling factors  (43)\\n             (        √    )         √           √            √\\n                                                              √  ∑T   ∑\\n                        2               |A|    4   |A|∆|I   p|√              t     t\\n          ≤    1 + √   ρK     ∆|I p | √ T   +        √ T      √   t=1 I∈I p Π (I)\\x0f (I)          by Jensen’s inequality       (44)\\n\\n\\n\\nNow, lets consider the average MSE loss L        T (M  T ) at time T over the samples in memory M         T.\\nWe start by stating two well-known lemmas:       V\\nLemma 3. The MSE can be decomposed into bias and variance components\\n\\n\\n\\n                                               E x[(x − θ)  2] = (θ − E[x])   2+ Var(θ)                                      (45)\\n\\n\\n\\nLemma 4. The mean of a random variable minimizes the MSE loss\\n\\n\\n\\n                                                     argminE    x[(x − θ) 2 ] = E[x]                                         (46)\\n                                                        θ\\n\\n\\n\\nand the value of the loss at when θ = E[x] is Var(x).\\n\\n\\n\\n                                   T               1            ∑     ∑T    t    ∥  t              T  ∥2\\n                                 L V  =  ∑        ∑  T     t              x (I)  ∥r˜ (I) − V (I|θ    )∥2                     (47)\\n                                            I∈I p    t=1 x (I)  I∈I p t=1\\n                                           1     ∑    ∑T    t    ∥  t              T  ∥2\\n                                      ≥  |Ip|T  I∈I p t=1 x (I)  ∥r˜ (I) − V (I|θ    )∥2                                     (48)\\n                                          1    ∑               [∥                    ∥ 2∣            ]\\n                                      =             Π T (I) E   ∥ ˜ t              T ∥  ∣  t\\n                                         |Ip| I∈I p           t    r (I) − V (I|θ   )  2∣x (I) = 1                           (49)\\n\\n\\n\\nLet V  ∗ be the model that minimizes L     T  on M   T . Using Lemmas 3 and 4,\\n---\\n                                                                  Deep Counterfactual Regret Minimization\\n\\n\\n\\n                                                T        1     ∑      T      ( ∥         T         [  t    ∣ t          ]∥ 2      T  )\\n                                              L   ≥                 Π   (I)    ∥ V (I|θ   ) − E     ˜      ∣             ∥\\n                                                V     |I p|T  I∈I p                              t   r (I) x   (I) = 1     2 + L  V ∗                       (50)\\n\\n\\n\\n                So,\\n\\n\\n\\n                                                                    L T  − L  T ∗ ≥     1   ∑     Π T (I) \\x0f T (I)                                           (51)\\n                                                                    ∑ V    T  V    T  |I p| I∈I p    T       T\\n                                                                   I∈I p Π   (I) \\x0f   (I) ≤ |I   p|(L V  − L  V ∗ )                                          (52)\\n\\n\\n\\n                Plugging this into Eq. 42, we arrive at\\n\\n\\n\\n                                                     (               )                                     √\\n                                                               √               √            √              √         T\\n                                                                 2                |A|      4   |A|∆|I   p |√       ∑\\n                                             R¯T  ≤     1 + √           ∆|I  p| √      +        √          √  |I p|     (L t  − L  t ∗)                     (53)\\n                                               p                ρK                 T               T                t=1    V       V\\n                                                  ≤  (  1 +  √ √ 2   )  ∆|I  p|√√ |A|  + 4|I  p |√  |A|∆\\x0f  L                                                (54)\\n\\n\\nρKT\\n                So far we have assumed that M            V  contains all sampled regrets. The number of samples in the memory at iteration t is\\n                bounded by K · |I      p | · t. Therefore, if K · |I    p| · T < |M     V | then the memory will never be full, and we can make this\\n                assumption.    8\\n\\n\\n\\n                B.5. Proof of Corollary 1\\n\\n\\n\\n                Proof. Let ρ = T      −1/4  .       (  ¯ T    (        √  2 )          √  |A|            √            )       −1/4\\n                                                 P    R  p >     1 +   √ K     ∆|I  p |T −1/4   + 4|I  p|   |A|∆\\x0f   L    < T                                (55)\\n\\n\\n\\n                Therefore, for any \\x0f > 0,                                  ( ¯ T           √                )\\n\\n\\nlimPR− 4|I||A|∆\\x0f> \\x0f= 0.(56)\\n                                                                T →∞           p        p            L\\n                    8We do not formally handle the case where the memories become full in this work. Intuitively, reservoir sampling should work well\\n                because it keeps an ‘unbiased’ sample of previous iterations’ regrets. We observe empirically in Figure 4 that reservoir sampling performs\\n                well while using a sliding window does not.\\n---\\nC. Network Architecture                                            Deep Counterfactual Regret Minimization\\nIn order to clarify the network architecture used in this work, we provide a PyTorch (Paszke et al., 2017) implementation\\nbelow.\\n\\n\\n\\n import         t o r c h\\n import         t o r c h . nn a s nn\\n import         t o r c h . nn . f u n c t i o n a l         a s F\\n\\n\\n\\n c l a s s    CardEmbedding ( nn . Module ) :\\n          d e f        i n i t      ( s e l f ,     dim ) :\\n                  s u pe r ( CardEmbedding ,                      s e l f ) .       i n i t      ( )\\n                   s e l f . r a n k = nn . Embedding ( 1 3 , dim )\\n                   s e l f . s u i t    = nn . Embedding ( 4 , dim )\\n                   s e l f . c a r d = nn . Embedding ( 5 2 , dim )\\n\\n\\n\\n          d e f   f o r w a r d ( s e l f ,       i n p u t ) :\\n                  B ,    n u m c a r d s = i n p u t . s h a p e\\n                  x = i n p u t . view ( −1)\\n\\n\\n\\n                  v a l i d = x . ge ( 0 ) . f l o a t ( )                # −1 means               ’ no c a r d ’\\n                  x = x . clamp ( min =0)\\n\\n\\n\\n                  embs =          s e l f . c a r d ( x ) +         s e l f . r a n k ( x      / /   4 ) +       s e l f . s u i t ( x % 4 )\\n                  embs = embs ∗                   v a l i d . u n s q u e e z e ( 1 )         # z e r o       o u t    ’ no c a r d ’ e m b e d d i n g s\\n\\n\\n\\n                  # sum a c r o s s            t h e     c a r d s    i n   t h e     h o l e / board\\n                  r e t u r n embs . view ( B ,                n u m c a r d s ,        −1).sum ( 1 )\\n\\n\\n\\n c l a s s    DeepCFRModel ( nn . Module ) :\\n          d e f        i n i t      ( s e l f ,     n c a r d t y p e s ,      n b e t s ,    n a c t i o n s ,     dim = 2 5 6 ) :\\n                  s u pe r ( DeepCFRModel ,                    s e l f ) .       i n i t      ( )\\n\\n\\n\\n                   s e l f . c a r d e m b e d d i n g s = nn . M o d u l e L i s t (\\n                            [ CardEmbedding ( dim )                     f o r        i n    range ( n c a r d t y p e s ) ] )\\n\\n\\n\\n                   s e l f . c a r d 1 = nn . L i n e a r ( dim ∗ n c a r d t y p e s ,                       dim )\\n                   s e l f . c a r d 2 = nn . L i n e a r ( dim ,                dim )\\n                   s e l f . c a r d 3 = nn . L i n e a r ( dim ,                dim )\\n\\n\\n\\n                   s e l f . b e t 1 = nn . L i n e a r ( n b e t s ∗ 2 , dim )\\n                   s e l f . b e t 2 = nn . L i n e a r ( dim ,                dim )\\n\\n\\n\\n                   s e l f . comb1 = nn . L i n e a r ( 2 ∗ dim ,                         dim )\\n                   s e l f . comb2 = nn . L i n e a r ( dim ,                    dim )\\n                   s e l f . comb3 = nn . L i n e a r ( dim ,                    dim )\\n\\n\\n\\n                   s e l f . a c t i o n h e a d = nn . L i n e a r ( dim ,                   n a c t i o n s )\\n\\n\\n\\n          d e f   f o r w a r d ( s e l f ,       c a r d s ,    b e t s ) :\\n\\n\\n\\n                  ”””\\n                  c a r d s :     ( ( N x      2 ) ,     (N x       3 ) [ ,    (N x       1 ) ,    (N x       1 ) ] )      # ( h o l e ,       board ,      [ t u r n ,  r i v e r ] )\\n                  b e t s : N x          n b e t f e a t s\\n                  ”””\\n\\n\\n\\n                  # 1 .       c a r d    b r a n c h\\n                  # embed h o l e ,               f l o p ,    and      o p t i o n a l l y      t u r n    and      r i v e r\\n                  c a r d e m b s = [ ]\\n                  f o r    embedding ,              c a r d g r o u p       i n    z i p ( s e l f . c a r d e m b e d d i n g s ,           c a r d s ) :\\n                            c a r d e m b s . a pp e n d ( embedding ( c a r d g r o u p ) )\\n                  c a r d e m b s = t o r c h . c a t ( c a r d e m b s ,                   dim =1)\\n\\n\\n\\n                  x = F . r e l u ( s e l f . c a r d 1 ( c a r d e m b s ) )\\n                  x = F . r e l u ( s e l f . c a r d 2 ( x ) )\\n---\\n                                                     Deep Counterfactual Regret Minimization\\n\\n\\n\\nx = F . r e l u ( s e l f . c a r d 3 ( x ) )\\n\\n\\n\\n#     1 .    b e t    b r a n c h\\n b e t s i z e        =    b e t s . clamp ( 0 ,            1 e6 )\\n b e t o c c u r r e d         =    b e t s . ge ( 0 )\\n b e t f e a t s        =    t o r c h . c a t ( [ b e t s i z e ,             b e t o c c u r r e d . f l o a t ( ) ] ,  dim =1)\\ny = F . r e l u ( s e l f . b e t 1 ( b e t f e a t s ) )\\ny = F . r e l u ( s e l f . b e t 2 ( y ) + y )\\n\\n\\n\\n#     3 .    c omb ine d          t r u n k\\n z =       t o r c h . c a t ( [ x ,       y ] ,     dim =1)\\n z = F . r e l u ( s e l f . comb1 ( z ) )\\n z = F . r e l u ( s e l f . comb2 ( z ) + z )\\n z = F . r e l u ( s e l f . comb3 ( z ) + z )\\n\\n\\n\\n z = n o r m a l i z e ( z )               #    ( z − mean )              /    s t d\\n r e t u r n      s e l f . a c t i o n h e a d ( z )', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b1e4f3b-dd2c-49f3-9d05-ac4ffd686a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n techniques.\n",
      "                 in two-player zero-sum games. Forms of tabular CFR have                            1www.computerpokercompetition.org\n",
      "                    *Equal contribution2    1Computer Science Department, Carnegie                  2Deep RL has also been applied successfully to some partially\n",
      "                 Mellon University      Facebook AI Research. Correspondence to:                observed games such as Doom (Lample & Chaplot, 2017), as long\n",
      "                 Noam Brown <noamb@cs.cmu.edu>.                                                 as the hidden information is not too strategically important.\n",
      "\n",
      "\n",
      "\n",
      "                 Proceedings of the 36      th International Conference on Machine\n",
      "                 Learning, Long Beach, California, PMLR 97, 2019. Copyright\n",
      "                 2019 by the author(s).\n",
      "---\n",
      "                                                    Deep Counterfactual Regret Minimization\n",
      "2. Notation and Background                                                  \n"
     ]
    }
   ],
   "source": [
    "print(documents[0].text[6000:7000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4251c14a-fb7a-4dff-a613-9ca08d5bbab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 38bac696-745f-4a2c-974a-cf3203859303\n"
     ]
    }
   ],
   "source": [
    "# As a markdown result type\n",
    "from llama_parse import LlamaParse\n",
    "documents = LlamaParse(result_type=\"markdown\").load_data(\"./attention.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f41cf125-3ee9-48a5-9769-5aee637c531a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g there can be at most 6 sequential actions, leading to 6N rounds total unique betting positions. Each betting position is encoded by a binary value specifying whether a bet has occurred, and a float value specifying the bet size.\n",
      "\n",
      "The neural network model begins with separate branches for the cards and bets, with three and two layers respectively. Features from the two branches are combined and three additional fully connected layers are applied. Each fully-connected layer consists of xi+1 = ReLU(Ax[+x]). The optional skip connection [+x] is applied only on layers that have equal input and output dimension. Normalization (to zero mean and unit variance) is applied to the last-layer features. The network architecture was not highly tuned, but normalization and skip connections were used because they were found to be important to encourage fast convergence when running preliminary experiments on pre-computed equilibrium strategies in FHP. A full network specification is provided in Appe...\n"
     ]
    }
   ],
   "source": [
    "print(documents[0].text[20000:21000] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f6e75d4-2cd3-41c5-be91-e7e5e2c2da99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import os\n",
    "from getpass import getpass\n",
    "from llama_index.extractors.entity import EntityExtractor\n",
    "from llama_index.core.node_parser import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a603e617",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = getpass()\n",
    "#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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59b8e65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:160: UserWarning: Field \"model_id\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceInferenceAPI(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x0000022C6ABEB860>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x0000022C5FDD34C0>, completion_to_prompt=<function default_completion_to_prompt at 0x0000022C5FE46A20>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, model_name='mistralai/Mixtral-8x7B-Instruct-v0.1', token='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', timeout=None, headers=None, cookies=None, task=None, context_window=3900, num_output=256, is_chat_model=False, is_function_calling_model=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create llm model\n",
    "from llama_index.llms.huggingface import HuggingFaceInferenceAPI\n",
    "llm = HuggingFaceInferenceAPI(model_name=\"mistralai/Mixtral-8x7B-Instruct-v0.1\", token=HF_TOKEN)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9c78f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.extractors import (\n",
    "    SummaryExtractor,\n",
    "    QuestionsAnsweredExtractor,\n",
    "    TitleExtractor,\n",
    "    KeywordExtractor,\n",
    "    BaseExtractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5aa2442",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(\n",
    "    separator=\" \", chunk_size=512, chunk_overlap=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ebd35f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractors = [\n",
    "    TitleExtractor(nodes=5, llm=llm),\n",
    "    QuestionsAnsweredExtractor(questions=3, llm=llm),\n",
    "    # EntityExtractor(prediction_threshold=0.5),\n",
    "    # SummaryExtractor(summaries=[\"prev\", \"self\"], llm=llm),\n",
    "    # KeywordExtractor(keywords=10, llm=llm),\n",
    "    # CustomExtractor()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4cbec9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = [text_splitter] + extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7377a382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0713c20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_docs = SimpleDirectoryReader(input_files=[\"attention.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6f19efba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ClientResponseError",
     "evalue": "429, message='Too Many Requests', url=URL('https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mClientResponseError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mingestion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IngestionPipeline\n\u001b[0;32m      2\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m IngestionPipeline(transformations\u001b[38;5;241m=\u001b[39mtransformations)\n\u001b[1;32m----> 3\u001b[0m uber_nodes \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muber_docs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\ingestion\\pipeline.py:734\u001b[0m, in \u001b[0;36mIngestionPipeline.run\u001b[1;34m(self, show_progress, documents, nodes, cache_collection, in_place, store_doc_text, num_workers, **kwargs)\u001b[0m\n\u001b[0;32m    732\u001b[0m         nodes \u001b[38;5;241m=\u001b[39m reduce(\u001b[38;5;28;01mlambda\u001b[39;00m x, y: x \u001b[38;5;241m+\u001b[39m y, nodes_parallel, [])\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 734\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[43mrun_transformations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    735\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes_to_run\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    736\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    737\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    738\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    739\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_collection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_collection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    740\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_place\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_place\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    741\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvector_store \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    745\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvector_store\u001b[38;5;241m.\u001b[39madd([n \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nodes \u001b[38;5;28;01mif\u001b[39;00m n\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m])\n",
      "File \u001b[1;32mc:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\ingestion\\pipeline.py:124\u001b[0m, in \u001b[0;36mrun_transformations\u001b[1;34m(nodes, transformations, in_place, cache, cache_collection, **kwargs)\u001b[0m\n\u001b[0;32m    122\u001b[0m         nodes \u001b[38;5;241m=\u001b[39m cached_nodes\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 124\u001b[0m         nodes \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m         cache\u001b[38;5;241m.\u001b[39mput(\u001b[38;5;28mhash\u001b[39m, nodes, collection\u001b[38;5;241m=\u001b[39mcache_collection)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\extractors\\interface.py:159\u001b[0m, in \u001b[0;36mBaseExtractor.__call__\u001b[1;34m(self, nodes, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, nodes: List[BaseNode], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[BaseNode]:\n\u001b[0;32m    152\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Post process nodes parsed from documents.\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \n\u001b[0;32m    154\u001b[0m \u001b[38;5;124;03m    Allows extractors to be chained.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m        nodes (List[BaseNode]): nodes to post-process\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\extractors\\interface.py:142\u001b[0m, in \u001b[0;36mBaseExtractor.process_nodes\u001b[1;34m(self, nodes, excluded_embed_metadata_keys, excluded_llm_metadata_keys, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_nodes\u001b[39m(\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    137\u001b[0m     nodes: List[BaseNode],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    141\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[BaseNode]:\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maprocess_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexcluded_embed_metadata_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexcluded_embed_metadata_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexcluded_llm_metadata_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexcluded_llm_metadata_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[1;32mc:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\asyncio\\futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32mc:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\asyncio\\tasks.py:314\u001b[0m, in \u001b[0;36mTask.__step_run_and_handle_result\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    312\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    313\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 314\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[1;32mc:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\extractors\\interface.py:120\u001b[0m, in \u001b[0;36mBaseExtractor.aprocess_nodes\u001b[1;34m(self, nodes, excluded_embed_metadata_keys, excluded_llm_metadata_keys, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     new_nodes \u001b[38;5;241m=\u001b[39m [deepcopy(node) \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes]\n\u001b[1;32m--> 120\u001b[0m cur_metadata_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maextract(new_nodes)\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(new_nodes):\n\u001b[0;32m    122\u001b[0m     node\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mupdate(cur_metadata_list[idx])\n",
      "File \u001b[1;32mc:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\extractors\\metadata_extractors.py:104\u001b[0m, in \u001b[0;36mTitleExtractor.aextract\u001b[1;34m(self, nodes)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maextract\u001b[39m(\u001b[38;5;28mself\u001b[39m, nodes: Sequence[BaseNode]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Dict]:\n\u001b[0;32m    103\u001b[0m     nodes_by_doc_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseparate_nodes_by_ref_id(nodes)\n\u001b[1;32m--> 104\u001b[0m     titles_by_doc_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_titles(nodes_by_doc_id)\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument_title\u001b[39m\u001b[38;5;124m\"\u001b[39m: titles_by_doc_id[node\u001b[38;5;241m.\u001b[39mref_doc_id]} \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes]\n",
      "File \u001b[1;32mc:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\extractors\\metadata_extractors.py:131\u001b[0m, in \u001b[0;36mTitleExtractor.extract_titles\u001b[1;34m(self, nodes_by_doc_id)\u001b[0m\n\u001b[0;32m    129\u001b[0m titles_by_doc_id \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, nodes \u001b[38;5;129;01min\u001b[39;00m nodes_by_doc_id\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 131\u001b[0m     title_candidates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_title_candidates(nodes)\n\u001b[0;32m    132\u001b[0m     combined_titles \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(title_candidates)\n\u001b[0;32m    133\u001b[0m     titles_by_doc_id[key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mapredict(\n\u001b[0;32m    134\u001b[0m         PromptTemplate(template\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcombine_template),\n\u001b[0;32m    135\u001b[0m         context_str\u001b[38;5;241m=\u001b[39mcombined_titles,\n\u001b[0;32m    136\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\extractors\\metadata_extractors.py:147\u001b[0m, in \u001b[0;36mTitleExtractor.get_title_candidates\u001b[1;34m(self, nodes)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_title_candidates\u001b[39m(\u001b[38;5;28mself\u001b[39m, nodes: List[BaseNode]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    140\u001b[0m     title_jobs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mapredict(\n\u001b[0;32m    142\u001b[0m             PromptTemplate(template\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_template),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes\n\u001b[0;32m    146\u001b[0m     ]\n\u001b[1;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m run_jobs(\n\u001b[0;32m    148\u001b[0m         title_jobs, show_progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_progress, workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_workers\n\u001b[0;32m    149\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:307\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.async_wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[0;32m    304\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[0;32m    305\u001b[0m )\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 307\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[1;32mc:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\async_utils.py:123\u001b[0m, in \u001b[0;36mrun_jobs\u001b[1;34m(jobs, show_progress, workers, desc)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m show_progress:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01masyncio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm_asyncio\n\u001b[1;32m--> 123\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m tqdm_asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mpool_jobs, desc\u001b[38;5;241m=\u001b[39mdesc)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    125\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mpool_jobs)\n",
      "File \u001b[1;32mc:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\tqdm\\asyncio.py:79\u001b[0m, in \u001b[0;36mtqdm_asyncio.gather\u001b[1;34m(cls, loop, timeout, total, *fs, **tqdm_kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m i, \u001b[38;5;28;01mawait\u001b[39;00m f\n\u001b[0;32m     78\u001b[0m ifs \u001b[38;5;241m=\u001b[39m [wrap_awaitable(i, f) \u001b[38;5;28;01mfor\u001b[39;00m i, f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(fs)]\n\u001b[1;32m---> 79\u001b[0m res \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mawait\u001b[39;00m f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mas_completed(ifs, loop\u001b[38;5;241m=\u001b[39mloop, timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m     80\u001b[0m                                          total\u001b[38;5;241m=\u001b[39mtotal, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtqdm_kwargs)]\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [i \u001b[38;5;28;01mfor\u001b[39;00m _, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(res)]\n",
      "File \u001b[1;32mc:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\asyncio\\tasks.py:631\u001b[0m, in \u001b[0;36mas_completed.<locals>._wait_for_one\u001b[1;34m()\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# Dummy value from _on_timeout().\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTimeoutError\n\u001b[1;32m--> 631\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\asyncio\\futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32mc:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\asyncio\\tasks.py:314\u001b[0m, in \u001b[0;36mTask.__step_run_and_handle_result\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    312\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    313\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 314\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[1;32mc:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\tqdm\\asyncio.py:76\u001b[0m, in \u001b[0;36mtqdm_asyncio.gather.<locals>.wrap_awaitable\u001b[1;34m(i, f)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_awaitable\u001b[39m(i, f):\n\u001b[1;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m i, \u001b[38;5;28;01mawait\u001b[39;00m f\n",
      "File \u001b[1;32mc:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:357\u001b[0m, in \u001b[0;36mDispatcher.async_span_with_parent_id.<locals>.outer.<locals>.async_wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[0;32m    351\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_,\n\u001b[0;32m    352\u001b[0m     bound_args\u001b[38;5;241m=\u001b[39mbound_args,\n\u001b[0;32m    353\u001b[0m     instance\u001b[38;5;241m=\u001b[39minstance,\n\u001b[0;32m    354\u001b[0m     parent_id\u001b[38;5;241m=\u001b[39mparent_id,\n\u001b[0;32m    355\u001b[0m )\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 357\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[1;32mc:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\async_utils.py:116\u001b[0m, in \u001b[0;36mrun_jobs.<locals>.worker\u001b[1;34m(job)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@dispatcher\u001b[39m\u001b[38;5;241m.\u001b[39masync_span_with_parent_id(parent_id\u001b[38;5;241m=\u001b[39mparent_span_id)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mworker\u001b[39m(job: Coroutine) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m semaphore:\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m job\n",
      "File \u001b[1;32mc:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:307\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.async_wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[0;32m    304\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[0;32m    305\u001b[0m )\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 307\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[1;32mc:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\llms\\llm.py:519\u001b[0m, in \u001b[0;36mLLM.apredict\u001b[1;34m(self, prompt, **prompt_args)\u001b[0m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    518\u001b[0m     formatted_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_prompt(prompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprompt_args)\n\u001b[1;32m--> 519\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macomplete(formatted_prompt, formatted\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    520\u001b[0m     output \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    522\u001b[0m dispatch_event(LLMPredictEndEvent())\n",
      "File \u001b[1;32mc:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\llms\\huggingface\\base.py:673\u001b[0m, in \u001b[0;36mHuggingFaceInferenceAPI.acomplete\u001b[1;34m(self, prompt, formatted, **kwargs)\u001b[0m\n\u001b[0;32m    670\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21macomplete\u001b[39m(\n\u001b[0;32m    671\u001b[0m     \u001b[38;5;28mself\u001b[39m, prompt: \u001b[38;5;28mstr\u001b[39m, formatted: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[0;32m    672\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CompletionResponse:\n\u001b[1;32m--> 673\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_async_client\u001b[38;5;241m.\u001b[39mtext_generation(\n\u001b[0;32m    674\u001b[0m         prompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_new_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_output}, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m    675\u001b[0m     )\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m CompletionResponse(text\u001b[38;5;241m=\u001b[39mresponse)\n",
      "File \u001b[1;32mc:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py:1557\u001b[0m, in \u001b[0;36mAsyncInferenceClient.text_generation\u001b[1;34m(self, prompt, details, stream, model, do_sample, max_new_tokens, best_of, repetition_penalty, return_full_text, seed, stop_sequences, temperature, top_k, top_p, truncate, typical_p, watermark, decoder_input_details)\u001b[0m\n\u001b[0;32m   1536\u001b[0m         _set_as_non_tgi(model)\n\u001b[0;32m   1537\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_generation(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m             prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[0;32m   1539\u001b[0m             details\u001b[38;5;241m=\u001b[39mdetails,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1555\u001b[0m             decoder_input_details\u001b[38;5;241m=\u001b[39mdecoder_input_details,\n\u001b[0;32m   1556\u001b[0m         )\n\u001b[1;32m-> 1557\u001b[0m     \u001b[43mraise_text_generation_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;66;03m# Parse output\u001b[39;00m\n\u001b[0;32m   1560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n",
      "File \u001b[1;32mc:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_text_generation.py:534\u001b[0m, in \u001b[0;36mraise_text_generation_error\u001b[1;34m(http_error)\u001b[0m\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhttp_error\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;66;03m# Otherwise, fallback to default error\u001b[39;00m\n\u001b[1;32m--> 534\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m http_error\n",
      "File \u001b[1;32mc:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py:1532\u001b[0m, in \u001b[0;36mAsyncInferenceClient.text_generation\u001b[1;34m(self, prompt, details, stream, model, do_sample, max_new_tokens, best_of, repetition_penalty, return_full_text, seed, stop_sequences, temperature, top_k, top_p, truncate, typical_p, watermark, decoder_input_details)\u001b[0m\n\u001b[0;32m   1530\u001b[0m \u001b[38;5;66;03m# Handle errors separately for more precise error messages\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     bytes_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost(json\u001b[38;5;241m=\u001b[39mpayload, model\u001b[38;5;241m=\u001b[39mmodel, task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, stream\u001b[38;5;241m=\u001b[39mstream)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   1533\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _import_aiohttp()\u001b[38;5;241m.\u001b[39mClientResponseError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1534\u001b[0m     error_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_error_payload\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py:257\u001b[0m, in \u001b[0;36mAsyncInferenceClient.post\u001b[1;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[0;32m    255\u001b[0m         timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout \u001b[38;5;241m-\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0), \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m error\n",
      "File \u001b[1;32mc:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py:226\u001b[0m, in \u001b[0;36mAsyncInferenceClient.post\u001b[1;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m--> 226\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _async_yield_from(client, response)\n",
      "File \u001b[1;32mc:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\aiohttp\\client_reqrep.py:1070\u001b[0m, in \u001b[0;36mClientResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1068\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreason \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m-> 1070\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ClientResponseError(\n\u001b[0;32m   1071\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_info,\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory,\n\u001b[0;32m   1073\u001b[0m     status\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[0;32m   1074\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreason,\n\u001b[0;32m   1075\u001b[0m     headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m   1076\u001b[0m )\n",
      "\u001b[1;31mClientResponseError\u001b[0m: 429, message='Too Many Requests', url=URL('https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1')"
     ]
    }
   ],
   "source": [
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "pipeline = IngestionPipeline(transformations=transformations)\n",
    "uber_nodes = pipeline.run(documents=uber_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c52413",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_nodes[1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4971c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex\n",
    "index = SummaryIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1853aede",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575edb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"Give me an introduction to CFR for my paper\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
