{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee2fe244-8fd6-4288-9ba3-8b8226a61b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create api key here\n",
    "# https://cloud.llamaindex.ai/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e0050d-efee-4611-8135-002f355f1a85",
   "metadata": {},
   "source": [
    "# LlamaParse\n",
    "- LlamaParse is an API created by LlamaIndex to efficiently parse and represent files for efficient retrieval and context augmentation using LlamaIndex frameworks.\n",
    "\n",
    "#### NOTE: Currently, only PDF files are supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b45a500b-b761-41be-adf1-42fe47535cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b75daf0-3959-4420-a176-68c964abff85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_parse import LlamaParse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04f5ce09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import merge_pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bba053aa-1150-4ffd-9008-3c3eac0df553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LLAMA_CLOUD_API_KEY\"] = \"llx-ZZPmYFkICGNlltwVQIcraiwYVGAzbV11ETl5meJyoeG9D03o\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68d00cc0-e624-472f-af10-44adbdcd50c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 3394k  100 3394k    0     0  5201k      0 --:--:-- --:--:-- --:--:-- 5238k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      " 11  944k   11  106k    0     0  51828      0  0:00:18  0:00:02  0:00:16 51857\n",
      "100  944k  100  944k    0     0   379k      0  0:00:02  0:00:02 --:--:--  379k\n"
     ]
    }
   ],
   "source": [
    "!curl -o CFR1.pdf \"https://arxiv.org/pdf/1811.00164\"\n",
    "!curl -o CFR2.pdf \"https://deliverypdf.ssrn.com/delivery.php?ID=574084113083000009096126092003023088018082052006043055030023001099121103023068051028120024016078097018073016027030089084002074078065122005024036042002081064013073083122012023093004092048087003119012096027005044037005114125021081018112090001030109103014108105010009008120030030104067081081124&EXT=pdf&INDEX=TRUE\"\n",
    "merge_pdfs(['CFR1.pdf', 'CFR2.pdf'], 'CFR.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c72491a3-6e95-4cc7-8d39-4d040ef6b665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 7b863fb0-7c58-40fd-8d96-5a7ebdc31822\n",
      "..."
     ]
    }
   ],
   "source": [
    "# llama-parse is async-first, running the sync code in a notebook requires the use of nest_asyncio\n",
    "# As a text result type\n",
    "from llama_parse import LlamaParse\n",
    "documents = LlamaParse(result_type=\"text\").load_data(\"./CFR.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fd86ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='b7202272-c800-4540-b0a0-f0522eea4ee3', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='arXiv:1811.00164v3 [cs.AI] 22 May 2019\\n\\n\\n\\n                                                     Deep Counterfactual Regret Minimization\\n\\n\\n\\n                                                                   * 1 2                   * 2               2                            1\\n\\n\\nNoam Brown Adam Lerer Sam Gross Tuomas Sandholm\\n                                               Abstract                                         been used in all recent milestones in the benchmark domain\\n                       Counterfactual Regret Minimization (CFR) is the                          of poker (Bowling et al., 2015; Morav cˇ´ık et al., 2017; Brown\\n                       leading framework for solving large imperfect-                           & Sandholm, 2017) and have been used in all competitive\\n                       information games. It converges to an equilibrium                        agents in the Annual Computer Poker Competition going\\n                       by iteratively traversing the game tree. In order                        back at least six years.      1  In order to deal with extremely\\n                       to deal with extremely large games, abstraction                          large imperfect-information games, abstraction is typically\\n                       is typically applied before running CFR. The ab-                         used to simplify a game by bucketing similar states together\\n                       stracted game is solved with tabular CFR, and its                        and treating them identically. The simplified (abstracted)\\n                       solution is mapped back to the full game. This                           game is approximately solved via tabular CFR. However,\\n                       process can be problematic because aspects of                            constructing an effective abstraction requires extensive do-\\n                       abstraction are often manual and domain specific,                        main knowledge and the abstract solution may only be a\\n                       abstraction algorithms may miss important strate-                        coarse approximation of a true equilibrium.\\n                       gic nuances of the game, and there is a chicken-                         In constrast, reinforcement learning has been successfully\\n                       and-egg problem because determining a good ab-                           extended to large state spaces by using function approx-\\n                       straction requires knowledge of the equilibrium                          imation with deep neural networks rather than a tabular\\n                       of the game. This paper introduces Deep Counter-                         representation of the policy (deep RL). This approach has\\n                       factual Regret Minimization, a form of CFR that                          led to a number of recent breakthroughs in constructing\\n                       obviates the need for abstraction by instead using                       strategies in large MDPs (Mnih et al., 2015) as well as in\\n                       deep neural networks to approximate the behavior                         zero-sum perfect-information games such as Go (Silver\\n                       of CFR in the full game. We show that Deep CFR                           et al., 2017; 2018).    2 Importantly, deep RL can learn good\\n                       is principled and achieves strong performance in                         strategies with relatively little domain knowledge for the\\n                       large poker games. This is the first non-tabular                         specific game (Silver et al., 2017). However, most popular\\n                       variant of CFR to be successful in large games.                          RL algorithms do not converge to good policies (equilibria)\\n                                                                                                in imperfect-information games in theory or in practice.\\n                 1. Introduction                                                                Rather than use tabular CFR with abstraction, this paper\\n                 Imperfect-information games model strategic interactions                       introduces a form of CFR, which we refer to as Deep Coun-\\n                 between multiple agents with only partial information. They                    terfactual Regret Minimization, that uses function approx-\\n                 are widely applicable to real-world domains such as negoti-                    imation with deep neural networks to approximate the be-\\n                 ations, auctions, and cybersecurity interactions. Typically                    havior of tabular CFR on the full, unabstracted game. We\\n                 in such games, one wishes to find an approximate equilib-                      prove that Deep CFR converges to an \\x0f-Nash equilibrium\\n                 rium in which no player can improve by deviating from the                      in two-player zero-sum games and empirically evaluate per-\\n                 equilibrium.                                                                   formance in poker variants, including heads-up limit Texas\\n                                                                                                hold’em. We show Deep CFR outperforms Neural Ficti-\\n                 The most successful family of algorithms for imperfect-                        tious Self Play (NFSP) (Heinrich & Silver, 2016), which\\n                 information games have been variants of Counterfactual                         was the prior leading function approximation algorithm for\\n                 Regret Minimization (CFR) (Zinkevich et al., 2007). CFR is                     imperfect-information games, and that Deep CFR is com-\\n                 an iterative algorithm that converges to a Nash equilibrium                    petitive with domain-specific tabular abstraction techniques.\\n                 in two-player zero-sum games. Forms of tabular CFR have                            1www.computerpokercompetition.org\\n                    *Equal contribution2    1Computer Science Department, Carnegie                  2Deep RL has also been applied successfully to some partially\\n                 Mellon University      Facebook AI Research. Correspondence to:                observed games such as Doom (Lample & Chaplot, 2017), as long\\n                 Noam Brown <noamb@cs.cmu.edu>.                                                 as the hidden information is not too strategically important.\\n\\n\\n\\n                 Proceedings of the 36      th International Conference on Machine\\n                 Learning, Long Beach, California, PMLR 97, 2019. Copyright\\n                 2019 by the author(s).\\n---\\n                                                    Deep Counterfactual Regret Minimization\\n2. Notation and Background                                                       of a strategy σ   p in a two-player zero-sum game is how much\\nIn an imperfect-information extensive-form (that is, tree-                       worse σ    p does versus BR(σ ∗       p) compared to how a∗Nash\\n                                                                                 equilibrium strategy σ        p does against BR(σ         p). Formally,\\nform) game there is a finite set of players, P. A node                           e(σ  p) = u    p( σ ∗, BR(σ    ∗) ) − u  p (σ p, BR(σ    p) ). We mea-\\n(or history) h is defined by all information of the current                                         p           p\\nsituation, including private knowledge known to only one                         sure total exploitability      ∑  p∈P   e(σ p )3.\\nplayer. A(h) denotes the actions available at a node and\\nP (h) is either chance or the unique player who acts at′that                     2.1. Counterfactual Regret Minimization (CFR)\\nnode. If action a ∈ A(h) leads from h to h , then we write                       CFR is an iterative algorithm that converges to a Nash equi-\\n            ′                       ′\\nh · a = h . We write h @ h           if a sequence of actions leads              librium in any finite two-player zero-sum game with a the-\\nfrom h to h . H′is the set of all nodes. Z ⊆ H are terminal                      oretical convergence bound of O(              √1T ). In practice CFR\\nnodes for which no actions are available. For each player                        converges much faster. We provide an overview of CFR be-\\np ∈ P, there is a payoff function u             p  : Z → R. In this              low; for a full treatment, see Zinkevich et al. (2007). Some\\npaper we assume P = {1, 2} and u               1 = −u    2 (the game is          recent forms of CFR converge in O(                1   ) in self-play set-\\ntwo-player zero-sum). We denote the range of payoffs in                                                                         T 0.75\\n                                                                                 tings (Farina et al., 2019), but are slower in practice so we\\nthe game by ∆.                                                                   do not use them in this paper.\\nImperfect information is represented by information sets                         Let σ   t be the strategy profile on iteration t. The counter-\\n(infosets) for each player p ∈ P. For any infoset I be-                          factual value v    σ (I) of player p = P (I) at I is the expected\\nlonging to p, all nodes h, h        ′ ∈ I are indistinguishable to               payoff to p when reaching I, weighted by the probability\\np. Moreover, every non-terminal node h ∈ H belongs to                            that p would reached I if she tried to do so that iteration.\\nexactly one infoset for each p. We represent the set of all                      Formally,\\ninfosets belonging to p where p acts by I               p. We call the\\nset of all terminal nodes with a prefix in I as Z              I, and we                   vσ (I ) =   ∑     π σ  (z[I ])π  σ (z[I ] → z)u     (z)       (1)\\ncall the particular prefix z[I ]. We assume the game features                                         z∈Z  I   −p                            p\\nperfect recall, which means if h and h          ′do not share a player\\np infoset then all nodes following h do not share a player p                     and v   σ (I, a) is the same except it assumes that player p\\ninfoset with any node following h           ′.                                   plays action a at infoset I with 100% probability.\\n\\n\\n\\nA strategy (or policy) σ(I) is a probability vector over ac-                     The instantaneous regret r        t(I, a) is the difference between\\ntions for acting player p in infoset I. Since all states in an                   P (I)’s counterfactual value from playing a vs. playing σ\\ninfoset belonging to p are indistinguishable, the strategies                     on iteration t\\nin each of them must be identical. The set of actions in I is\\ndenoted by A(I). The probability of a particular action a is                                        rt(I, a) = v    σt(I, a) − v   σ t(I)                (2)\\ndenoted by σ(I, a). We define σ            p to be a strategy for p in\\nevery infoset in the game where p acts. A strategy profile σ                     The counterfactual regret for infoset I action a on iteration\\nis a tuple of strategies, one for each player. The strategy of                   T is\\nevery player other than p is represented as σ          −p . u p(σ p , σ−p )                            R  T (I, a) =   ∑ T  r t(I, a)                    (3)\\nis the expected payoff for p if player p plays according to\\nσ p and the other players play according to σ            −p .                                                          t=1\\nπ σ (h) = Π       ′     σ     ′ (h  ′, a) is called reach and is the             Additionally, R     T (I, a) = max{R       T  (I, a), 0} and R    T (I ) =\\n                h ·avh    P (h )                                                                     +\\nprobability h is reached if all players play according to σ.                     max   a {R  T (I, a)}. Total regret for p in the entire game is\\nπ σ (h) is the contribution of p to this probability. π             σ   (h)      R  T  = max    σ ′ ∑  T    (u p(σ  ′, σt   ) − u  p(σ  t, σt   )).\\n  p                                                                 −p              p            p     t=1          p   −p             p    −p\\nis the contribution of chance and all players other than p.                      CFR determines an iteration’s strategy by applying any of\\nFor an infoset I belonging to p, the probability of reaching                     several regret minimization algorithms to each infoset (Lit-\\nI if p chooses actions leading toward I but chance and all                       tlestone & Warmuth, 1994; Chaudhuri et al., 2009). Typi-\\nplayers other than p play according to σ             −p   is denoted by\\nπ σ  (I) =    ∑       π σ   (h). For h v z, define π       σ (h → z) =           cally, regret matching (RM) is used as the regret minimiza-\\n  −p′        ′   h∈I    −p  ′                                                    tion algorithm within CFR due to RM’s simplicity and lack\\nΠ                σ      ′ (h , a)\\n  h  ·avz,h 6@h    P (h )                                                        of parameters (Hart & Mas-Colell, 2000).\\nA best response to σ        −p   is a player p strategy BR(σ           −p )\\nsuch that u    p (BR(σ    −p  ), σ−p  )  = max     σ′  up (σ ′, σ−p  ). A        In RM, a player picks a distribution over actions in an in-\\n                            ∗                       p        p                   foset in proportion to the positive regret on those actions.\\nNash equilibrium σ             is a strategy profile where ev-                   Formally, on each iteration t + 1, p selects actions a ∈ A(I)\\neryone plays a best response:                  ∀p, u   p (σ ∗ , σ∗  )    =\\n                                                            p    −p\\nmax   σ ′ up (σ ′ , σ∗  ) (Nash, 1950). The exploitability e(σ           p)          3Some prior papers instead measure average exploitability\\n        p       p    −p                                                          rather than total (summed) exploitability.\\n---\\n                                                     Deep Counterfactual Regret Minimization\\n\\n\\n\\naccording to probabilities                                                        who is traversing the game tree on the iteration as the tra-\\n                                         R  t (I, a)                              verser. Regrets are updated only for the traverser on an\\n                σ t+1 (I, a) =    ∑         +     t       ′             (4)       iteration. At infosets where the traverser acts, all actions are\\n                                       ′        R + (I, a )                       explored. At other infosets and chance nodes, only a single\\n                                     a ∈A(I)\\nIf ∑    ′       R  t (I, a ′) = 0 then any arbitrary strategy may                 action is explored.\\n       a ∈A(I)     +\\nbe chosen. Typically each action is assigned equal proba-                         External-sampling MCCFR probabilistically converges to\\nbility, but in this paper we choose the action with highest                       an equilibrium. For any ρ ∈ (0, 1], total regret is bounded\\n                                                                                        T      (      √ 2 )        √       √\\ncounterfactual regret with probability 1, which we find em-                       by R  p  ≤    1 +   √ ρ  |I p |∆    |A|    T with probability 1 − ρ.\\npirically helps RM better cope with approximation error\\n(see Figure 4).                                                                   3. Related Work\\nIf a player plays according to regret matching in in-             T               CFR is not the only iterative algorithm capable of solving\\nfoset I on every iteration, then on iteration T , R                 (I) ≤\\n∆  √  |A(I )| √  T (Cesa-Bianchi & Lugosi, 2006). Zinkevich                       large imperfect-information games. First-order methods\\net al. (2007) show that the sum of the counterfactual regret                      converge to a Nash equilibrium in O(1/T ) (Hoda et al.,\\nacross all infosets upper bounds the total regret. Therefore,                     2010; Kroer et al., 2018b;a), which is far better than CFR’s\\nif player p plays according to CFR on every iteration, then                       theoretical bound. However, in practice the fastest variants\\nR  T  ≤  ∑         R  T (I). So, as T → ∞,        R pT  → 0.                      of CFR are substantially faster than the best first-order meth-\\n   p         I∈I p                                 T                              ods. Moreover, CFR is more robust to error and therefore\\nThe average strategy σ¯       T (I) for an infoset I on iteration T               likely to do better when combined with function approxima-\\n                ∑  T   (  σt  p   t   )                                           tion.\\n     T             t=1   πp  (I)σ p(I)\\nis σ¯p (I) =         ∑ T   π σ t(I)     .                                         Neural Fictitious Self Play (NFSP) (Heinrich & Silver,\\n                       t=1   p\\nIn two-player zero-sum games, if both players’ average                            2016) previously combined deep learning function approx-\\n                           R T                                                    imation with Fictitious Play (Brown, 1951) to produce an\\ntotal regret satisfies      Tp  ≤ \\x0f, then their average strategies                AI for heads-up limit Texas hold’em, a large imperfect-\\n〈¯σ T, ¯σT 〉 form a 2\\x0f-Nash equilibrium (Waugh, 2009). Thus,                      information game. However, Fictitious Play has weaker\\n    1    2\\nCFR constitutes an anytime algorithm for finding an \\x0f-Nash                        theoretical convergence guarantees than CFR, and in prac-\\nequilibrium in two-player zero-sum games.                                         tice converges slower. We compare our algorithm to NFSP\\nIn practice, faster convergence is achieved by alternating                        in this paper. Model-free policy gradient algorithms have\\nwhich player updates their regrets on each iteration rather                       been shown to minimize regret when parameters are tuned\\nthan updating the regrets of both players simultaneously                          appropriately (Srinivasan et al., 2018) and achieve perfor-\\neach iteration, though this complicates the theory (Farina                        mance comparable to NFSP.\\net al., 2018; Burch et al., 2018). We use the alternating-                        Past work has investigated using deep learning to esti-\\nupdates form of CFR in this paper.                                                mate values at the depth limit of a subgame in imperfect-\\n                                                                                  information games (Morav cˇ´ık et al., 2017; Brown et al.,\\n2.2. Monte Carlo Counterfactual Regret Minimization                               2018). However, tabular CFR was used within the sub-\\nVanilla CFR requires full traversals of the game tree, which                      games themselves. Large-scale function approximated CFR\\nis infeasible in large games. One method to combat this is                        has also been developed for single-agent settings (Jin et al.,\\nMonte Carlo CFR (MCCFR), in which only a portion of                               2017). Our algorithm is intended for the multi-agent set-\\nthe game tree is traversed on each iteration (Lanctot et al.,                     ting and is very different from the one proposed for the\\n2009). In MCCFR, a subset of nodes Q               t in the game tree is          single-agent setting.\\ntraversed at each iteration, where Q         tt is sampled from some              Prior work has combined regression tree function approxi-\\ndistribution Q. Sampled regrets ˜r             are tracked rather than            mation with CFR (Waugh et al., 2015) in an algorithm called\\nexact regrets. For infosets that are sampled at iteration t,                      Regression CFR (RCFR). This algorithm defines a number\\n˜rt(I, a) is equal to r     t(I, a) divided by the probability of                 of features of the infosets in a game and calculates weights\\nhaving sampled I; for unsampled infosets r˜ (I, a) = 0.tSee                       to approximate the regrets that a tabular CFR implemen-\\nAppendix B for more details.                                                      tation would produce. Regression CFR is algorithmically\\nThere exist a number of MCCFR variants (Gibson et al.,                            similar to Deep CFR, but uses hand-crafted features similar\\n2012; Johanson et al., 2012; Jackson, 2017), but for this                         to those used in abstraction, rather than learning the features.\\npaper we focus specifically on the external sampling variant                      RCFR also uses full traversals of the game tree (which is\\ndue to its simplicity and strong performance. In external-                        infeasible in large games) and has only been evaluated on\\nsampling MCCFR the game tree is traversed for one player                          toy games. It is therefore best viewed as the first proof of\\nat a time, alternating back and forth. We refer to the player                     concept that function approximation can be applied to CFR.\\n---\\n                                                     Deep Counterfactual Regret Minimization\\n\\n\\n\\nConcurrent work has also investigated a similar combina-                          Once a player’s K traversals are completed, a new network\\ntion of deep learning with CFR, in an algorithm referred                          is trained from scratch to determine parameters θ             t  by mini-\\nto as Double Neural CFR (Li et al., 2018). However, that                          mizing MSE between predicted advantage V                 p (I,pa|θ t) and\\napproach may not be theoretically sound and the authors                           samples of instantaneous regrets from prior iterations t             ′ ≤ t\\nconsider only small games. There are important differences                        ˜t′\\n                                                                                   r (I, a) drawn from the memory. The average over all\\nbetween our approaches in how training data is collected                          sampled instantaneous advantages r˜           t′(I, a) is proportional\\nand how the behavior of CFR is approximated.                                      to the total sampled regret R       ˜ t(I, a) (across actions in an\\n                                                                                  infoset), so once a sample is added to the memory it is never\\n4. Description of the Deep Counterfactual                                         removed except through reservoir sampling, even when the\\n    Regret Minimization Algorithm                                                 next CFR iteration begins.\\nIn this section we describe Deep CFR. The goal of Deep                            One can use any loss function for the value and average\\nCFR is to approximate the behavior of CFR without calcu-                          strategy model that satisfies Bregman divergence (Banerjee\\nlating and accumulating regrets at each infoset, by general-                      et al., 2005), such as mean squared error loss.\\nizing across similar infosets using function approximation                       While almost any sampling scheme is acceptable so long\\nvia deep neural networks.                                                         as the samples are weighed properly, external sampling\\nOn each iteration t, Deep CFR conducts a constant num-                            has the convenient property that it achieves both of our\\nber K of partial traversals of the game tree, with the path                       desired goals by assigning all samples in an iteration equal\\nof the traversal determined according to external sampling                        weight. Additionally, exploring all of a traverser’s actions\\nMCCFR. At each infoset I it encounters, it plays a strategy                       helps reduce variance. However, external sampling may\\nσ t(I ) determined by regret matching on the output of a neu-                     be impractical in games with extremely large branching\\nral network V : I → R          |A| defined by parameters θ        t−1  that       factors, so a different sampling scheme, such as outcome\\ntakes as input the infoset I and outputs values V (I, a|θ         p   t−1 ).      sampling (Lanctot et al., 2009), may be desired in those\\nOur goal is for V (I, a|θ       t−1  ) to be approximately propor-                cases.\\ntional to the regret R     t−1 (I, a) that tabular CFR would have                 In addition to the value network, a separate policy network\\nproduced.                                                                         Π : I → R     |A|  approximates the average strategy at the end\\nWhen a terminal node is reached, the value is passed back up.                     of the run, because it is the average strategy played over all\\nIn chance and opponent infosets, the value of the sampled                         iterations that converges to a Nash equilibrium. To do this,\\naction is passed back up unaltered. In traverser infosets, the                    we maintain a separate memory M                Π  of sampled infoset\\nvalue passed back up is the weighted average of all action                        probability vectors for both players. Whenever an infoset\\nvalues, where action a’s weight is σ           t(I, a). This produces             I belonging to player p is traversed during the opposing\\nsamples of this iteration’s instantaneous regrets for various                     player’s traversal of the game tree via external sampling,\\nactions. Samples are added to a memory M                    v,p, where p          the infoset probability vector σ         t(I ) is added to M       Π  and\\nis the traverser, using reservoir sampling (Vitter, 1985) if                      assigned weight t.\\ncapacity is exceeded.                                                             If the number of Deep CFR iterations and the size of each\\nConsider a nice property of the sampled instantaneous re-                         value network model is small, then one can avoid training\\ngrets induced by external sampling:                                               the final policy network by instead storing each iteration’s\\n                                                                                  value network (Steinberger, 2019). During actual play, a\\nLemma 1. For external sampling MCCFR, the sampled                                 value network is sampled randomly and the player plays the\\ninstantaneous regrets are an unbiased estimator of the ad-                        CFR strategy resulting from the predicted advantages of that\\nvantage,ti.e. the difference in expected payoff for playingt                      network. This eliminates the function approximation error\\na vs σ  p (I) at I , assuming both players play σ            everywhere           of the final average policy network, but requires storing all\\nelse.                                                                             prior value networks. Nevertheless, strong performance and\\n\\n\\n\\n            [    t       ∣                ]     v σt (I, a) − v  σ t(I)           low exploitability may still be achieved by storing only a\\n               σ         ∣\\n  E Q∈Q   t  r˜p  (I, a)∣Z  I  ∩ Q  6= ∅    =          π σ t (I)         .        subset of the prior value networks (Jackson, 2016).\\n                                                         −p                       Theorem 1 states that if the memory buffer is sufficiently\\nThe proof is provided in Appendix B.2.                                            large, then with high probability Deep CFR will result in\\n                                                                                  average regret being bounded by a constant proportional to\\nRecent work in deep reinforcement learning has shown                              the square root of the function approximation error.\\nthat neural networks can effectively predict and generalize\\nadvantages in challenging environments with large state                           Theorem 1. Let T denote the number of Deep CFR itera-\\nspaces, and use that to learn good policies (Mnih et al.,                         tions, |A| the maximum number of actions at any infoset,\\n2016).                                                                            and K the number of traversals per iteration. Let L                  t  be\\n                                                                                                                                                       V\\n---\\n                                                     Deep Counterfactual Regret Minimization\\n\\n\\n\\nthe average MSE loss for V          p(I, a|θ  t) on a sample in M        V,p         Hole\\nat iteration t , and let L     t ∗  be the minimum loss achievable                                1 3\\nfor any function V . Let L     V  t − L   t ∗  ≤ \\x0f L .\\n                                  V       V                                          Board                                                  normalize     Fold\\n                                                                                                                                                          Call\\nIf the value memories are sufficiently large, then with proba-                                                     192  192                               Raise\\nbility 1 − ρ total regret at time T is bounded by                                           Rank Emb\\n                                                                                            Suit Emb      Bet Occurred                   Linear;\\n                                                                                            Card Emb           @LL                   FC := [Skip,]\\n                                                                                                           Bet Pot Frac\\n R T  ≤   ( 1 +   √ √  2  )  ∆|I   p|√  |A| √  T + 4T |I    p |√  |A|∆\\x0f   L                              Betting Position                ReLU\\n   p                 ρK                                                           Figure 1. The neural network architecture used for Deep CFR.\\n                                                                         (5)      The network takes an infoset (observed cards and bet history) as\\nwith probability 1 − ρ.                                                           input and outputs values (advantages or probability logits) for each\\n                                                      R T                         possible action.\\nCorollary 1. As T → ∞, average regret                  Tp  is bounded by\\n                            4|I  p|√  |A|∆\\x0f    L                                  (1-4), and a card embedding (1-52). These embeddings\\nwith high probability.                                                            are summed for each set of permutation invariant cards\\n                                                                                  (hole, flop, turn, river), and these are concatenated. In\\nThe proofs are provided in Appendix B.4.                                          each of the N     rounds rounds of betting there can be at most 6\\n                                                                                  sequential actions, leading to 6N           rounds total unique betting\\nWe do not provide a convergence bound for Deep CFR when                           positions. Each betting position is encoded by a binary\\nusing linear weighting, since the convergence rate of Linear                      value specifying whether a bet has occurred, and a float\\nCFR has not been shown in the Monte Carlo case. However,                          value specifying the bet size.\\nFigure 4 shows moderately faster convergence in practice.                         The neural network model begins with separate branches for\\n                                                                                  the cards and bets, with three and two layers respectively.\\n5. Experimental Setup                                                             Features from the two branches are combined and three\\nWe measure the performance of Deep CFR (Algorithm 1)                              additional fully connected layers are applied. Each fully-\\nin approximating an equilibrium in heads-up flop hold’em                          connected layer consists of x          i+1   = ReLU(Ax[+x]). The\\npoker (FHP). FHP is a large game with over 10                    12  nodes        optional skip connection [+x] is applied only on layers that\\nand over 10     9 infosets. In contrast, the network we use has                   have equal input and output dimension. Normalization (to\\n98,948 parameters. FHP is similar to heads-up limit Texas                         zero mean and unit variance) is applied to the last-layer\\nhold’em (HULH) poker, but ends after the second betting                           features. The network architecture was not highly tuned, but\\nround rather than the fourth, with only three community                           normalization and skip connections were used because they\\ncards ever dealt. We also measure performance relative to                         were found to be important to encourage fast convergence\\ndomain-specific abstraction techniques in the benchmark                           when running preliminary experiments on pre-computed\\ndomain of HULH poker, which has over 10                    17  nodes and          equilibrium strategies in FHP. A full network specification\\nover 10   14  infosets. The rules for FHP and HULH are given                      is provided in Appendix C.\\nin Appendix A.                                                                    In the value network, the vector of outputs represented pre-\\nIn both games, we compare performance to NFSP, which                              dicted advantages for each action at the input infoset. In the\\nis the previous leading algorithm for imperfect-information                       average strategy network, outputs are interpreted as logits\\ngame solving using domain-independent function approx-                            of the probability distribution over actions.\\nimation, as well as state-of-the-art abstraction techniques\\ndesigned for the domain of poker (Johanson et al., 2013;                          5.2. Model training\\nGanzfried & Sandholm, 2014; Brown et al., 2015).                                  We allocate a maximum size of 40 million infosets to each\\n                                                                                  player’s advantage memory M             V,p  and the strategy memory\\n5.1. Network Architecture                                                         M   Π . The value model is trained from scratch each CFR\\nWe use the neural network architecture shown in Figure 5.1                        iteration, starting from a random initialization. We perform\\nfor both the value network V that computes advantages for                         4,000 mini-batch stochastic gradient descent (SGD) itera-\\neach player and the network Π that approximates the final                         tions using a batch size of 10,000 and perform parameter\\naverage strategy. This network has a depth of 7 layers and                        updates using the Adam optimizer (Kingma & Ba, 2014)\\n98,948 parameters. Infosets consist of sets of cards and                          with a learning rate of 0.001, with gradient norm clipping\\nbet history. The cards are represented as the sum of three                        to 1. For HULH we use 32,000 SGD iterations and a batch\\nembeddings: a rank embedding (1-13), a suit embedding                             size of 20,000. Figure 4 shows that training the model from\\n---\\n                                                   Deep Counterfactual Regret Minimization\\n\\n\\n\\nAlgorithm 1 Deep Counterfactual Regret Minimization\\n   function DEEPCFR\\n        Initialize each player’s advantage network V (I, a|θ            p ) with parameters θ     p  so that it returns 0 for all inputs.\\n        Initialize reservoir-sampled advantage memories M                 V,1 , M  V,2 and strategy memory M          Π  .\\n        for CFR iteration t = 1 to T do\\n             for each player p do\\n                 for traversal k = 1 to K do\\n                      TRAVERSE(∅, p, θ       1, θ2 , M  V,p, M   Π )          . Collect data from a game traversal with external sampling\\n                                                                            ′          [  ′ ∑    (  t′                      ) 2]\\n                 Train θ  p from scratch on loss L(θ        p) = E   (I,t′,˜t            t         ˜r  (a) − V (I, a|θ   p )\\n                                                                            r )∼M  V,p         a\\n        Train θ  Π  on loss L(θ   Π ) = E   (I,t′,σt′)∼M   Π [ t′∑   a (σ t′(a) − Π(I, a|θ     Π )) 2 ]\\n        return θ   Π\\n\\n\\n\\nAlgorithm 2 CFR Traversal with External Sampling\\n   function TRAVERSE(h, p, θ            , θ , M    , M     , t)\\n                                      1   2      V      Π\\n        Input: History h, traverser player p, regret network parameters θ for each player, advantage memory M                              V for player\\n   p, strategy memory M         Π , CFR iteration t.\\n\\n\\n\\n        if h is terminal then\\n             return the payoff to player p\\n        else if h is a chance node then\\n             a ∼ σ(h)\\n             return TRAVERSE(h · a, p, θ         1, θ2 , M  V , M  Π , t)\\n        else if P (h) = p then                                                                                   . If it’s the traverser’s turn to act\\n             Compute strategy σ       t(I) from predicted advantages V (I (h), a|θ           p) using regret matching.\\n             for a ∈ A(h) do\\n                 v(a) ← TRAVERSE(h · a, p, θ           1, θ2 , M  V , M  Π , t)                                               . Traverse each action\\n             for a ∈ A(h) do            ∑                     ′        ′\\n                 ˜r(I, a) ← v(a) −          a′∈A(h)   σ(I, a ) · v(a )                                                       . Compute advantages\\n        else Insert the infoset and its action advantages (I, t, ˜r        t(I)) into the advantage memory M    . If it’sVthe opponent’s turn to act\\n             Compute strategy σ       t(I) from predicted advantages V (I (h),ta|θ           3−p ) using regret matching.\\n             Insert the infoset and its action probabilities (I, t, σ         (I)) into the strategy memory M            Π\\n             Sample an action a from the probability distribution σ              t(I).\\n             return TRAVERSE(h · a, p, θ         1, θ2 , M  V , M  Π , t)\\n\\n\\n\\nscratch at each iteration, rather than using the weights from                   not appear to lead to better performance asymptotically, but\\nthe previous iteration, leads to better convergence.                            does result in faster convergence in our experiments.\\n                                                                                LCFR is like CFR except iteration t is weighed by t. Specif-\\n5.3. Linear CFR                                                                 ically, we maintain a weight on each entry stored in the\\nThere exist a number of variants of CFR that achieve much                       advantage memory and the strategy memory, equal to t\\nfaster performance than vanilla CFR. However, most of                           when this entry was added. When training θ                p  each itera-\\nthese faster variants of CFR do not handle approximation                        tion T , we rescale all the batch weights by          2  and minimize\\nerror well (Tammelin et al., 2015; Burch, 2017; Brown &                         weighted error.                                       T\\nSandholm, 2019; Schmid et al., 2019). In this paper we use\\nLinear CFR (LCFR) (Brown & Sandholm, 2019), a variant                           6. Experimental Results\\nof CFR that is faster than CFR and in certain settings is\\nthe fastest-known variant of CFR (particularly in settings                      Figure 2 compares the performance of Deep CFR to\\nwith wide distributions in payoffs), and which tolerates                        different-sized domain-specific abstractions in FHP. The ab-\\napproximation error well. LCFR is not essential and does                        stractions are solved using external-sampling Linear Monte\\n---\\n                                                                         Deep Counterfactual Regret Minimization\\n\\n\\n\\n                  Carlo CFR (Lanctot et al., 2009; Brown & Sandholm, 2019),                             model. This is presumably because the model loss decreases\\n                  which is the leading algorithm in this setting. The 40,000                            as the number of training steps is increased per iteration (see\\n                  cluster abstraction means that the more than 10                   9 different         Theorem 1). Increasing the model size also decreases final\\n                  decisions in the game were clustered into 40,000 abstract                             exploitability up to a certain model size in FHP.\\n                  decisions, where situations in the same bucket are treated                            In Figure 4 we consider ablations of certain components of\\n                  identically. This bucketing is done using K-means clustering                          Deep CFR. Retraining the regret model from scratch at each\\n                  on domain-specific features. The lossless abstraction only                            CFR iteration converges to a substantially lower exploitabil-\\n                  clusters together situations that are strategically isomorphic                        ity than fine-tuning a single model across all iterations. We\\n                  (e.g., flushes that differ only by suit), so a solution to this                       suspect that this is because a single model gets stuck in bad\\n                  abstraction maps to a solution in the full game without error.                        local minima as the objective is changed from iteration to\\n                  Performance and exploitability are measured in terms of                               iteration. The choice of reservoir sampling to update the\\n                  milli big blinds per game (mbb/g), which is a standard                                memories is shown to be crucial; if a sliding window mem-\\n                  measure of win rate in poker.                                                         ory is used, the exploitability begins to increase once the\\n                  The figure shows that Deep CFR asymptotically reaches a                               memory is filled up, even if the memory is large enough to\\n                  similar level of exploitability as the abstraction that uses 3.6                      hold the samples from many CFR iterations.\\n                  million clusters, but converges substantially faster. Although                        Finally, we measure head-to-head performance in HULH.\\n                  Deep CFR is more efficient in terms of nodes touched, neu-                            We compare Deep CFR and NFSP to the approximate solu-\\n                  ral network inference and training requires considerable                              tions (solved via Linear Monte Carlo CFR) of three different-\\n                  overhead that tabular CFR avoids. However, Deep CFR                                   sized abstractions: one in which the more than 10                    14  deci-\\nExploitability (mbb/g)                                                                                                                           6\\n                  does not require advanced domain knowledge. We show                                   sions are clustered into 3.3 · 10          buckets, one in which there\\n                  Deep CFR performance for 10,000 CFR traversals per step.                              are 3.3·10    7 buckets and one in which there are 3.3·10             8  buck-\\n                  Using more traversals per step is less sample efficient and                           ets. The results are presented in Table 1. For comparison,\\n                  requires greater neural network training time but requires                            the largest abstractions used by the poker AI Polaris in its\\n                  fewer CFR steps.                                                                      2007 HULH man-machine competition against human pro-\\n                  Figure 2 also compares the performance of Deep CFR to                                 fessionals contained roughly 3·10            8 buckets. When variance-\\n                  NFSP, an existing method for learning approximate Nash                                reduction techniques were applied, the results showed that\\n                  equilibria in imperfect-information games. NFSP approx-                               the professional human competitors lost to the 2007 Polaris\\n                  imates fictitious self-play, which is proven to converge to                           AI by about 52 ± 10 mbb/g (Johanson, 2016). In contrast,\\n                  a Nash equilibrium but in practice does so far slower than                            our Deep CFR agent loses to a 3.3 · 10              8  bucket abstraction\\n                  CFR. We observe that Deep CFR reaches an exploitability                               by only −11 ± 2 mbb/g and beats NFSP by 43 ± 2 mbb/g.\\n                  of 37 mbb/g while NFSP converges to 47 mbb/g.                      4 We also                          Convergence of Deep CFR, NFSP, and Domain-Specific Abstractions\\n                  observe that Deep CFR is more sample efficient than NFSP.\\n                  However, these methods spend most of their wallclock time\\n                  performing SGD steps, so in our implementation we see a                                 103\\n                  less dramatic improvement over NFSP in wallclock time\\n                  than sample efficiency.\\n                  Figure 3 shows the performance of Deep CFR using differ-\\n                  ent numbers of game traversals, network SGD steps, and\\n                  model size. As the number of CFR traversals per iteration                               102      Deep CFR\\n                  is reduced, convergence becomes slower but the model con-                                        NFSP (1,000 infosets / update)\\n                                                                                                                   NFSP (10,000 infosets / update)\\n                  verges to the same final exploitability. This is presumably                                      Abstraction (40,000 Clusters)\\n                                                                                                                   Abstraction (368,000 Clusters)\\n                  because it takes more iterations to collect enough data to                                       Abstraction (3,644,000 Clusters)\\n                                                                                                                   Lossless Abstraction (234M Clusters)\\n                  reduce the variance sufficiently. On the other hand, reduc-                                   106           107           108          109           1010          1011\\n                  ing the number of SGD steps does not change the rate of                                                                     Nodes Touched\\n                  convergence but affects the asymptotic exploitability of the\\n                       4We run NFSP with the same model architecture as we use                          Figure 2. Comparison of Deep CFR with domain-specific tabular\\n                  for Deep CFR. In the benchmark game of Leduc Hold’em, our                             abstractions and NFSP in FHP. Coarser abstractions converge faster\\n                  implementation of NFSP achieves an average exploitability (total                      but are more exploitable. Deep CFR converges with 2-3 orders of\\n                  exploitability divided by two) of 37 mbb/g in the benchmark game                      magnitude fewer samples than a lossless abstraction, and performs\\n                  of Leduc Hold’em, which is substantially lower than originally                        competitively with a 3.6 million cluster abstraction. Deep CFR\\n                  reported in Heinrich & Silver (2016). We report NFSP’s best                           achieves lower exploitability than NFSP, while traversing fewer\\n                  performance in FHP across a sweep of hyperparameters.                                 infosets.\\n---\\nExploitability (mbb/g)                                                          Deep Counterfactual Regret Minimization\\n\\n\\n\\nExploitability (mbb/g)                                                                        Opponent Model                          Abstraction Size\\n                                      Model                     NFSP                   Deep CFR                  3.3 · 10   6              3.3 · 10  7               3.3 · 10  8\\n                                      NFSP                         -               −43 ± 2 mbb/g            −40 ± 2 mbb/g             −49 ± 2 mbb/g             −55 ± 2 mbb/g\\n                                      Deep CFR           +43 ± 2 mbb/g                       -                +6 ± 2 mbb/g             −6 ± 2 mbb/g             −11 ± 2 mbb/g\\n\\n\\n\\n                   Table 1. Head-to-head expected value of NFSP and Deep CFR in HULH against converged CFR equilibria with varying abstraction sizes.\\n                    For comparison, in 2007 an AI using abstractions of roughly 3 · 10                         8  buckets defeated human professionals by about 52 mbb/g (after\\n                    variance reduction techniques were applied).\\n\\n\\n\\n                   103                                                          103                                                                 dim=8\\n                                                               Traversals per iter                                          SGD steps per iter\\n                                                                   3,000                                                        1,000\\n                                                                   10,000                                                       2,000\\n                                                                   30,000                                                       4,000\\n                                                                   100,000                                                      8,000\\nExploitability (mbb/g)                                             300,000                                                      16,000\\n                                                                   1,000,000                                                    32,000         2\\n                   102                                             Linear CFR   102                                             Linear CFR   10               dim=16\\n\\n\\n\\n                                                                                                                                                                        dim=32   dim=64\\n\\n\\n\\n                                                                                                                                                                                        dim=128 dim=256\\n\\n\\n\\n                             101                        102                               101                        102                                      104               105               106\\nExploitability (mbb/g)                      CFR Iteration                                                CFR Iteration                                              # Model Parameters\\n                   Figure 3. Left: FHP convergence for different numbers of training data collection traversals per simulated LCFR iteration. The dotted\\n                    line shows the performance of vanilla tabular Linear CFR without abstraction or sampling. Middle: FHP convergence using different\\n                    numbers of minibatch SGD updates to train the advantage model at each LCFR iteration. Right: Exploitability of Deep CFR in FHP for\\n                    different model sizes. Label indicates the dimension (number of features) in each hidden layer of the model.\\n\\n\\n\\n                                                                Deep CFR (5 replicates)                               Deep CFR\\n                                     3                          Deep CFR without Linear Weighting             3       Deep CFR with Sliding Window Memories\\n                                  10                            Deep CFR without Retraining from Scratch    10\\nExploitability (mbb/g)                                          Deep CFR Playing Uniform when All Regrets < 0\\n\\n\\n\\n                                  10 2                                                                      102\\n\\n\\n\\n                                              101                          102                                         10 1                         10 2\\n                                                              CFR Iteration                                                             CFR Iteration\\n                   Figure 4. Ablations of Deep CFR components in FHP. Left: As a baseline, we plot 5 replicates of Deep CFR, which show consistent\\n                    exploitability curves (standard deviation at t = 450 is 2.25 mbb/g). Deep CFR without linear weighting converges to a similar\\n                    exploitability, but more slowly. If the same network is fine-tuned at each CFR iteration rather than training from scratch, the final\\n                    exploitability is about 50% higher. Also, if the algorithm plays a uniform strategy when all regrets are negative (i.e. standard regret\\n                    matching), rather than the highest-regret action, the final exploitability is also 50% higher. Right: If Deep CFR is performed using\\n                    sliding-window memories, exploitability stops converging once the buffer becomes full                             6 . However, with reservoir sampling, convergence\\n                    continues after the memories are full.\\n\\n\\n\\n                    7. Conclusions                                                                               for tabular methods and where abstraction is not straight-\\n                   We describe a method to find approximate equilibria in                                        forward. Extending Deep CFR to larger games will likely\\n                    large imperfect-information games by combining the CFR                                       require more scalable sampling strategies than those used in\\n                    algorithm with deep neural network function approxima-                                       this work, as well as strategies to reduce the high variance\\n                    tion. This method is theoretically principled and achieves                                   in sampled payoffs. Recent work has suggested promising\\n                    strong performance in large poker games relative to domain-                                  directions both for more scalable sampling (Li et al., 2018)\\n                    specific abstraction techniques without relying on advanced                                  and variance reduction techniques (Schmid et al., 2019). We\\n                    domain knowledge. This is the first non-tabular variant of                                   believe these are important areas for future work.\\n                    CFR to be successful in large games.\\n                    Deep CFR and other neural methods for imperfect-\\n                    information games provide a promising direction for tack-\\n                    ling large games whose state or action spaces are too large\\n---\\n                                                   Deep Counterfactual Regret Minimization\\nReferences                                                                      Ganzfried, S. and Sandholm, T. Potential-aware imperfect-\\nBanerjee, A., Merugu, S., Dhillon, I. S., and Ghosh, J. Clus-                      recall abstraction with earth mover’s distance in\\n   tering with bregman divergences. Journal of machine                             imperfect-information games. In AAAI Conference on\\n   learning research, 6(Oct):1705–1749, 2005.                                      Artificial Intelligence (AAAI), 2014.\\n                                                                                Gibson, R., Lanctot, M., Burch, N., Szafron, D., and Bowl-\\nBowling, M., Burch, N., Johanson, M., and Tammelin, O.                             ing, M. Generalized sampling and variance in coun-\\n   Heads-up limit hold’em poker is solved. Science, 347                            terfactual regret minimization. In Proceedins of the\\n   (6218):145–149, January 2015.                                                   Twenty-Sixth AAAI Conference on Artificial Intelligence,\\nBrown, G. W. Iterative solutions of games by fictitious play.                      pp. 1355–1361, 2012.\\n   In Koopmans, T. C. (ed.), Activity Analysis of Production                    Hart, S. and Mas-Colell, A. A simple adaptive procedure\\n   and Allocation, pp. 374–376. John Wiley & Sons, 1951.                           leading to correlated equilibrium. Econometrica, 68:\\nBrown, N. and Sandholm, T. Superhuman AI for heads-up                              1127–1150, 2000.\\n   no-limit poker: Libratus beats top professionals. Science,                   Heinrich, J. and Silver, D. Deep reinforcement learning\\n   pp. eaao1733, 2017.                                                             from self-play in imperfect-information games. arXiv\\nBrown, N. and Sandholm, T. Solving imperfect-information                           preprint arXiv:1603.01121, 2016.\\n   games via discounted regret minimization. In AAAI Con-                       Hoda, S., Gilpin, A., Pe na, J., and Sandholm, T. Smoothing˜\\n   ference on Artificial Intelligence (AAAI), 2019.                                techniques for computing Nash equilibria of sequential\\n                                                                                   games. Mathematics of Operations Research, 35(2):494–\\nBrown, N., Ganzfried, S., and Sandholm, T. Hierarchical ab-                        512, 2010. Conference version appeared in WINE-07.\\n   straction, distributed equilibrium computation, and post-                    Jackson, E. Targeted CFR. In AAAI Workshop on Computer\\n   processing, with application to a champion no-limit texas                       Poker and Imperfect Information, 2017.\\n   hold’em agent. In Proceedings of the 2015 International\\n   Conference on Autonomous Agents and Multiagent Sys-                          Jackson, E. G. Compact CFR. In AAAI Workshop on Com-\\n   tems, pp. 7–15. International Foundation for Autonomous                         puter Poker and Imperfect Information, 2016.\\n   Agents and Multiagent Systems, 2015.                                         Jin, P. H., Levine, S., and Keutzer, K. Regret minimiza-\\nBrown, N., Sandholm, T., and Amos, B. Depth-limited                                tion for partially observable deep reinforcement learning.\\n   solving for imperfect-information games. In Advances in                         arXiv preprint arXiv:1710.11424, 2017.\\n   Neural Information Processing Systems, 2018.                                 Johanson, M., Bard, N., Lanctot, M., Gibson, R., and\\nBurch, N. Time and Space: Why Imperfect Information                                Bowling, M. Efficient nash equilibrium approximation\\n   Games are Hard. PhD thesis, University of Alberta, 2017.                        through monte carlo counterfactual regret minimization.\\n                                                                                   In Proceedings of the 11th International Conference on\\nBurch, N., Moravcik, M., and Schmid, M. Revisiting cfr+                            Autonomous Agents and Multiagent Systems-Volume 2,\\n   and alternating updates. arXiv preprint arXiv:1810.11542,                       pp. 837–846. International Foundation for Autonomous\\n   2018.                                                                           Agents and Multiagent Systems, 2012.\\nCesa-Bianchi, N. and Lugosi, G. Prediction, learning, and                       Johanson, M., Burch, N., Valenzano, R., and Bowling,\\n   games. Cambridge University Press, 2006.                                        M. Evaluating state-space abstractions in extensive-form\\n                                                                                   games. In Proceedings of the 2013 International Con-\\nChaudhuri, K., Freund, Y., and Hsu, D. J. A parameter-free                         ference on Autonomous Agents and Multiagent Systems,\\n   hedging algorithm. In Advances in neural information                            pp. 271–278. International Foundation for Autonomous\\n   processing systems, pp. 297–305, 2009.                                          Agents and Multiagent Systems, 2013.\\nFarina, G., Kroer, C., and Sandholm, T. Online convex opti-                     Johanson, M. B. Robust Strategies and Counter-Strategies:\\n   mization for sequential decision processes and extensive-                       From Superhuman to Optimal Play. PhD thesis, Univer-\\n   form games. In AAAI Conference on Artificial Intelli-                           sity of Alberta, 2016.\\n   gence (AAAI), 2018.                                                          Kingma, D. P. and Ba, J. Adam: A method for stochastic\\nFarina, G., Kroer, C., Brown, N., and Sandholm, T. Stable-                         optimization. arXiv preprint arXiv:1412.6980, 2014.\\n   predictive optimistic counterfactual regret minimization.                    Kroer, C., Farina, G., and Sandholm, T. Solving large\\n   In International Conference on Machine Learning, 2019.                          sequential games with the excessive gap technique. In\\n---\\n                                                    Deep Counterfactual Regret Minimization\\n\\n\\n\\n   Advances in Neural Information Processing Systems, pp.                        Schmid, M., Burch, N., Lanctot, M., Moravcik, M., Kadlec,\\n   864–874, 2018a.                                                                  R., and Bowling, M. Variance reduction in monte carlo\\n                                                                                    counterfactual regret minimization (VR-MCCFR) for ex-\\nKroer, C., Waugh, K., Kılınc¸ -Karzan, F., and Sandholm, T.                         tensive form games using baselines. In AAAI Conference\\n   Faster algorithms for extensive-form game solving via                            on Artificial Intelligence (AAAI), 2019.\\n   improved smoothing functions. Mathematical Program-\\n   ming, pp. 1–33, 2018b.                                                        Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou,\\n                                                                                    I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M.,\\nLample, G. and Chaplot, D. S. Playing FPS games with                                Bolton, A., et al. Mastering the game of go without\\n   deep reinforcement learning. In AAAI, pp. 2140–2146,                             human knowledge. Nature, 550(7676):354, 2017.\\n   2017.                                                                         Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai,\\nLanctot, M. Monte carlo sampling and regret minimization                            M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Grae-\\n   for equilibrium computation and decision-making in large                         pel, T., et al. A general reinforcement learning algorithm\\n   extensive form games. 2013.                                                      that masters chess, shogi, and go through self-play. Sci-\\nLanctot, M., Waugh, K., Zinkevich, M., and Bowling, M.                              ence, 362(6419):1140–1144, 2018.\\n   Monte Carlo sampling for regret minimization in exten-                        Srinivasan, S., Lanctot, M., Zambaldi, V., P erolat, J., Tuyls,´\\n   sive games. In Proceedings of the Annual Conference                              K., Munos, R., and Bowling, M. Actor-critic policy opti-\\n   on Neural Information Processing Systems (NIPS), pp.                             mization in partially observable multiagent environments.\\n   1078–1086, 2009.                                                                 In Advances in Neural Information Processing Systems,\\nLi, H., Hu, K., Ge, Z., Jiang, T., Qi, Y., and Song, L. Double                      pp. 3426–3439, 2018.\\n   neural counterfactual regret minimization. arXiv preprint                     Steinberger, E. Single deep counterfactual regret minimiza-\\n   arXiv:1812.10607, 2018.                                                          tion. arXiv preprint arXiv:1901.07621, 2019.\\nLittlestone, N. and Warmuth, M. K. The weighted majority                         Tammelin, O., Burch, N., Johanson, M., and Bowling, M.\\n   algorithm. Information and Computation, 108(2):212–                              Solving heads-up limit texas hold’em. In Proceedings\\n   261, 1994.                                                                       of the International Joint Conference on Artificial Intelli-\\n                                                                                    gence (IJCAI), pp. 645–652, 2015.\\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,                      Vitter, J. S. Random sampling with a reservoir. ACM\\n   J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje-                         Transactions on Mathematical Software (TOMS), 11(1):\\n   land, A. K., Ostrovski, G., et al. Human-level control                           37–57, 1985.\\n   through deep reinforcement learning. Nature, 518(7540):\\n   529, 2015.                                                                    Waugh, K. Abstraction in large extensive games. Master’s\\nMnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap,                           thesis, University of Alberta, 2009.\\n   T., Harley, T., Silver, D., and Kavukcuoglu, K. Asyn-                         Waugh, K., Morrill, D., Bagnell, D., and Bowling, M. Solv-\\n   chronous methods for deep reinforcement learning. In                             ing games with functional regret estimation. In AAAI\\n   International conference on machine learning, pp. 1928–                          Conference on Artificial Intelligence (AAAI), 2015.\\n   1937, 2016.                                                                   Zinkevich, M., Johanson, M., Bowling, M. H., and Pic-\\nMorav cˇ´ık, M., Schmid, M., Burch, N., Lis ´y, V., Morrill, D.,                    cione, C. Regret minimization in games with incomplete\\n   Bard, N., Davis, T., Waugh, K., Johanson, M., and Bowl-                          information. In Proceedings of the Annual Conference\\n   ing, M. Deepstack: Expert-level artificial intelligence in                       on Neural Information Processing Systems (NIPS), pp.\\n   heads-up no-limit poker. Science, 2017. ISSN 0036-8075.                          1729–1736, 2007.\\n   doi: 10.1126/science.aam6960.\\n\\n\\n\\nMorrill, D. R. Using Regret Estimation to Solve Games\\n   Compactly. PhD thesis, University of Alberta, 2016.\\n\\n\\n\\nNash, J. Equilibrium points in n-person games. Proceedings\\n   of the National Academy of Sciences, 36:48–49, 1950.\\n\\n\\n\\nPaszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E.,\\n   DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer,\\n   A. Automatic differentiation in pytorch. 2017.\\n---\\n                                                        Deep Counterfactual Regret Minimization\\n   A. Rules for Heads-Up Limit Texas Hold’em and Flop Hold’em Poker\\n   Heads-up limit Texas hold’em is a two-player zero-sum game. There are two players and the position of the two players\\n   alternate after each hand. On each betting round, each player can choose to either fold, call, or raise. Folding results in the\\n   player losing and the money in the pot being awarded to the other player. Calling means the player places a number of chips\\n   in the pot equal to the opponent’s share. Raising means that player adds more chips to the pot than the opponent’s share. A\\n   round ends when a player calls (if both players have acted). There cannot be more than three raises in the first or second\\n   betting round or more than four raises in the third or fourth betting round, so there is a limited number of actions in the\\n   game. Raises in the first two rounds are $100 and raises in the second two rounds are $200.\\n   At the start of each hand of HULH, both players are dealt two private cards from a standard 52-card deck. P                                   1 must place\\n   $50 in the pot and P       2 must place $100 in the pot. A round of betting then occurs starting with P                       1 . When the round ends,\\n   three community cards are dealt face up that both players can ultimately use in their final hands. Another round of betting\\n   occurs, starting with P       2  this time. Afterward another community card is dealt face up and another betting round occurs.\\n   Then a final card is dealt face up and a final betting round occurs. At the end of the betting round, unless a player has folded,\\n   the player with the best five-card poker hand constructed from their two private cards and the five community cards wins the\\n   pot. In the case of a tie, the pot is split evenly.\\n   Flop Hold’em Poker is identical to HULH except there are only the first two betting rounds.\\n\\n\\n\\n   B. Proofs of Theorems\\n   B.1. Review of MCCFR\\n   We begin by reviewing the derivation of convergence bounds for external sampling MCCFR from Lanctot et al. 2009.\\n   An MCCFR scheme is completely specified by a set of blocks Q = {Q                              i} which each comprise a subset of all terminal\\n   histories Z. On each iteration MCCFR samples one of these blocks, and only considers terminal histories within that block.\\n   Let q  j > 0 be the probability of considering block Q               j in an iteration.\\n   Let Z   I be the set of terminal nodes that contain a prefix in I, and let z[I] be that prefix. Define π                   σ(h → z) as the probability\\n   of playing to z given that player p is at node h with both players playing σ.\\n\\n\\n\\n                                                           π σ (h → z) =       ∑     π σ (z[I])  π σ(z).\\n                                                                              z∈Z  I   π σ(I)\\n\\n\\n\\n   π σ (I → z) is undefined when π(I) = 0.\\n   Let q(z) =     ∑   j:z∈Q  j qj be the probability that terminal history z is sampled in an iteration of MCCFR. For external sampling\\n   MCCFR, q(z) = π          σ  (z).\\n   The sampled value v˜     −iσ (I|j) when sampling block j is\\n\\n\\ni\\n                                                σ               ∑          1             σ            σ\\n                                               ˜v (I|j) =                      u p (z)π     (z[I])π    (z[I] → z)                                            (6)\\n                                                p            z∈Q  j∩Z  I q(z)           −p\\n\\n\\n\\n   For external sampling, the sampled value reduces to   ˜σ               ∑                 σ\\n                                                          v (I|j) =                up (z)π    (z[I] → z)                                                     (7)\\n                                                          p            z∈Q  j∩Z  I          p\\n\\n\\n\\n   The sampled value is an unbiased estimator of the true value v                   p(I). Therefore the sampled instantaneous regret ˜r              t(I, a) =\\n   ˜vσ t(I, a) − ˜   σ t                                          t\\n     p            v  p (I) is an unbiased estimator of r           (I, a).\\n   The sampled regret is calculated as R          ˜ T (I, a) =    ∑  T    ˜t\\n                                                                     t=1   r (I, a).\\n   We first state the general bound shown in (Lanctot, 2013), Theorem 3.\\n---\\n                                                  Deep Counterfactual Regret Minimization\\n\\n\\n\\nLanctot 2013 defines B      p to be a set with one element per distinct action sequence ~a played by p, containing all infosets that\\nmay arise when p plays ~a. M       p is then defined by    ∑  B∈B   p|B|. Let ∆ be the difference between the maximum and minimum\\npayoffs in the game.\\nTheorem 2. (Lanctot 2013, Theorem 3) For any p ∈ (0, 1], when using any algorithm in the MCCFR family such that for\\nall Q ∈ Q and B ∈ B       p ,                ∑    \\uf8eb    ∑       π σ (z[I] → z)π    σ  (z[I])  \\uf8f6 2      1\\n                                             I∈B  \\uf8ed  z∈Q∩Z   I             q(z)   −p         \\uf8f8    ≤  δ2                                      (8)\\n\\n\\n\\nwhere δ ≤ 1, then with probability at least 1 − ρ, total regret is bounded by\\n                                               R T ≤   ( M    +   √  2|I p||B p|)  (  1 )  ∆ √  |A|T                                         (9)\\n.                                                p          p          √ ρ            δ\\n\\n\\n\\nFor the case of external sampling MCCFR, q(z) = π              σ  (z). Lanctot et al. 2009, Theorem 9 shows that for external sampling,\\n                                                               −i\\nfor which q(z) = π      σ  (z), the inequality in (8) holds for δ = 1, and thus the bound implied by (9) is\\n                        −i\\n\\n\\n\\n                              ¯T     (          √  2|I p||B p|)     √√ |A|\\n                             R p  ≤  ( M  p + √     )√ ρ      √  ∆      T                                                                  (10)\\n                                  ≤    1 + √     2     ∆|I  p|  √ |A|                  because |B    p| ≤ M   p ≤ |I  p|                   (11)\\n.                                              ρK                 T\\n\\n\\n\\nB.2. Proof of Lemma 1\\nWe show                                              [    t    ∣                ]        t        t\\n                                            E          ˜σ      ∣                       σ         σ\\n                                              Q  ∼Q     v  (I)∣Z  I ∩ Q  j 6= ∅   = v     (I)/π     (I).\\n                                                j       p                                        −p\\n\\n\\n\\nLet q j  = P (Q  [j).     ∣               ]        E  Q  ∼Q  [v˜σ t(I) ]\\n       E          ˜ σt    ∣                            j        p\\n         Q j∼Q     vp  (I)∣Z  I ∩ Q  6= ∅   =   P Q j∼Q  (Z I ∩ Q   j 6= ∅)\\n                                                ∑          qj ∑             u p(z)π  σ t(z[I])π  σ t(z[I] → z)/q(z)\\n                                                   Q j∈Q         z∈Z I ∩Q j          −p\\n                                            =                                   πσ t (I)\\n                                                ∑             ( ∑             qj)−pup (z)π σ t (z[I])π  σt(z[I] → z)/q(z)\\n                                            =      z∈Z  I∩Q  j     Q j:z∈Q  j      π σt (I)−p\\n                                                                                     −p\\n                                                ∑         q(z)u  p(z)π  σt (z[I])π  σ t(z[I] → z)/q(z)\\n                                                   z∈Z  I               −p\\n                                            =                            π σt (I)                                   By definition of q(z)\\n                                                v σt (I)                   −p\\n                                            =   π σt (I)\\n                                                  −p\\n\\n\\n\\nThe result now follows directly.\\n\\n\\n\\nB.3. K-external sampling\\nWe first show that performing MCCFR with K external sampling traversals per iteration (K-ES) shares a similar convergence\\nbound with standard external sampling (i.e. 1-ES). We will refer to this result in the next section when we consider the full\\n---\\n                                                              Deep Counterfactual Regret Minimization\\n\\n\\n\\n              Deep CFR algorithm. This convergence bound is rather obvious and the derivation pedantic, so the reader is welcome to\\n              skip this section.\\n              We model T rounds of K-external sampling as T × K rounds of external sampling, where at each round t · K + d (for\\n              integer t ≥ 0 and integer 0 ≤ d < K) we play\\n\\n\\n\\n                                                                              \\uf8f1  R+  (a)      +\\n                                                                              \\uf8f2   tK     if R       > 0\\n                                                                                 R +          Σ,tK\\n                                                              σ tK+d  (a) =        Σ,tK                                                              (12)\\n                                                                              \\uf8f3  arbitrary, otherwise\\n\\n\\n\\n              In prior work, σ is typically defined to play      1   when R   +   (a) ≤ 0, but in fact the convergence bounds do not constraint σ’s\\n                                                                |A|           Σ,T\\n              play in these situations, which we will demonstrate explicitly here. We need this fact because minimizing the loss L(V ) is\\n              defined only over the samples of (visited) infosets and thus does not constrain the strategy in unvisited infosets.\\n              Lemma 2. If regret matching is used in K-ES, then for 0 ≤ d < K\\n                                                                      ∑    R +  (a)r tK+d  (a) ≤ 0                                                   (13)\\n                                                                      a∈A    tK\\n\\n\\n\\n              Proof. If R   +     ≤ 0, then R   +  (a) = 0 for all a and the result follows directly. For R       +     > 0,\\n\\n\\nΣ,tK tK Σ,tK\\n                               ∑    R +  (a)r tK+d   (a) =  ∑    R + (a)(u  tK+d  (a) − u  tK+d  (σ tK ))                                            (14)\\n                               a∈A    tK                    a∈A    T\\n                                                         =  (  ∑   R  +  (a)u tK+d  (a) )  −  ( u tK+d  (σ tK )∑    R  +  (a))                       (15)\\n                                                            ( a∈A     tK                )     (                a∈A     tK   )\\n                                                               ∑      +                         ∑                                +\\n                                                         =    a∈A  R  tK (a)u tK+d  (a)    −    a∈A  σ tK+d  (a)u tK+d   (a)   R Σ,tK  (a)           (16)\\n                                                            (  ∑      +                 )     ( ∑      R +  (a)              )    +\\n                                                         =         R  tK (a)u tK+d  (a)    −          R +tK  (a) u tK+d  (a)    R Σ,tK (a)           (17)\\n                                                            ( a∈A                       )     ( a∈A     Σ,tK                  )\\n                                                         =     ∑   R  +  (a)u tK+d  (a)    −    ∑    R +   (a)(a)u  tK+d  (a)                        (18)\\n                                                         = 0  a∈A     tK                        a∈A    tK                                            (19)\\n\\n\\n\\n              Theorem 3. Playing according to Equation 12 guarantees the following bound on total regret\\n                                                                    ∑   (R +   (a)) 2  ≤ |A|∆   2K  2T                                               (20)\\n                                                                    a∈A    T K\\n\\n\\n\\n              Proof. We prove by recursion on T .    (                   K−1              ) 2\\n                          ∑    (R +   (a)) 2 ≤  ∑      R +        (a) +   ∑    rtK−d  (a)                                                            (21)\\n                          a∈A     T K           a∈A      (T −1)K          d=0\\n                                                    (                       K−1                           K−1 K−1                             )\\n                                             =  ∑     R  +       (a) 2 + 2  ∑    r d(a)R  +        (a) +  ∑     ∑    rT K−d  (a)r T K−d  ′(a)        (22)\\n                                                         (T −1)K                          (T −1)K                ′\\n                                                a∈A                         d=0                           d=0 d =0\\n\\n\\n\\n              By Lemma 2,\\n---\\n                                                Deep Counterfactual Regret Minimization\\n\\n\\n\\n                                                                                 K−1 K−1\\n                           ∑   (R  +   (a)) 2 ≤  ∑   (R  +        (a)) 2 +  ∑    ∑     ∑    rT K−d  (a)r  T K−d ′(a)                     (23)\\n                                   T K                   (T −1)K                        ′\\n                          a∈A                    a∈A                        a∈A d=0   d =0\\n\\n\\n\\nBy induction,                                     ∑   (R  +        (a)) 2 ≤ |A|∆   2(T − 1)                                              (24)\\n                                                  a∈A     (T −1)K\\n\\n\\n\\nFrom the definition, |r   T K−d  (a)| ≤ ∆\\n\\n\\n\\n                                    ∑       +       2           2                2       2          2   2\\n                                    a∈A (R  T K(a))   ≤ |A|∆     (T − 1) + K       |A|∆    = |A|∆     K   T                              (25)\\n\\n\\n\\nTheorem 4. (Lanctot 2013, Theorem 3 & Theorem 5) After T iterations of K-ES, average regret is bounded by\\n                                                  R¯ T K ≤   ( 1 + √  √ 2   ) |I p|∆  √√ |A|                                             (26)\\n                                                     p                 ρK                T\\n\\n\\n\\nwith probability 1 − ρ.\\n\\n\\n\\nProof. The proof follows Lanctot 2013, Theorem 3. Note that K-ES is only different from ES in terms of the choice of\\nσ T, and the proof in Lanctot 2013 only makes use of σ            T  via the bound on (    ∑    R T (a)) 2 that we showed in Theorem 3.\\nTherefore, we can apply the same reasoning to arrive at  R˜T K  ≤   ∆M   p√  |A|T K           a   +                                      (27)\\n(Lanctot 2013, Eq. (4.30)).                                p                δ\\n\\n\\n\\nLanctot et al. 2009 then shows that R     ˜ T K  and R  T K are similar with high probability, leading to\\n                                      \\uf8ee \\uf8eb   p           p                 \\uf8f6 2 \\uf8f9\\n                                    E \\uf8ef \\uf8ed  ∑    (R T K (I) − R ˜ T K (I)) \\uf8f8   \\uf8fa  ≤  2|I p||B p||A|T K∆    2                              (28)\\n                                      \\uf8f0   I∈I  p   p             p            \\uf8fb               δ 2\\n\\n\\n\\n(Lanctot 2013, Eq. (4.33), substituting T → T K).\\nTherefore, by Markov’s inequality, with probability at least 1 − ρ,\\n\\n\\n\\n                                           R T K  ≤  √  2|I p ||Bp ||A|T K∆    +   ∆M    √ |A|T K                                        (29)\\n                                             p                 δ √ ρ                       δ\\n, where external sampling permits δ = 1 (Lanctot, 2013).\\nUsing the fact that M ≤ |I      p| and |B p | < |I p| and dividing through by KT leads to the simplified form\\n                                                  R¯ T K ≤   ( 1 + √  √ 2   ) ∆|I  p| √√ |A|                                             (30)\\nwith probability 1 − ρ.                              p                 ρK                T\\n---\\n                                                   Deep Counterfactual Regret Minimization\\n\\n\\n\\nWe point out that the convergence of K-ES is faster as K increases (up to a point), but it still requires the same order of\\niterations as ES.\\n\\n\\n\\nB.4. Proof of Theorem 1\\nProof. Assume that an online learning scheme plays\\n\\n\\n\\n                                                            {    y t(I,a)       ∑\\n                                                               ∑   + t       if       yt (I, a) > 0\\n                                              σ t(I, a) =         a y+(I,a)         a  +                  .                                         (31)\\n                                                                arbitrary, otherwise\\n\\n\\n\\nMorrill 2016, Corollary 3.0.6 provides the following bound on the total regret as a function of the L2 distance between y                              +\\nand R  T,+   at each infoset.                                                                                                                          t\\n\\n\\n\\n                              max(R     T (I, a)) 2 ≤ |A|∆     2T + 4∆|A|      ∑T∑       √   (R t (I, a) − y   t (I, a)) 2                          (32)\\n                               a∈A                                             t=1 a∈A          +              +\\n                                                               2               ∑T∑       √      t             t        2\\n                                                    ≤ |A|∆      T + 4∆|A|      t=1 a∈A       (R  (I, a) − y (I, a))                                 (33)\\n\\n\\n\\nSince σ   t(I, a) from Eq. 31 is invariant to rescaling across all actions at an infoset, it’s also the case that for any C(I) > 0\\n\\n\\n\\n                                                                              T\\n                            max(R    T  (I, a)) 2 ≤ |A|∆    2 T + 4∆|A|     ∑ ∑ √         (R  t(I, a) − C(I)y     t(I, a)) 2                        (34)\\n                             a∈A                                             t=1 a∈A\\n\\n\\n\\nLet x t (I ) be an indicator variable that is 1 if I was traversed on iteration t. If I was traversed then ˜r              t(I) was stored in M      V,p ,\\n              t\\notherwise r˜ (I) = 0. Assume for now that M               V,p is not full, so all sampled regrets are stored in the memory.\\nLet Π  t(I) be the fraction of iterations on which x          t(I) = 1, and let\\n                                               t         ∥    [  t       t          ]               t ∥\\n                                              \\x0f (I) =    ∥E  t  r˜ (I)|x  (I) = 1     − V (I, a|θ    )∥ 2 .\\nInserting canceling factors of       ∑   t    x t′(I) and setting C(I) =        ∑  t     x t′(I), 7\\n                                         ′                                          ′\\n                                         t =1                                      t =1\\n                                                                     (               )       √   (                                 )\\n                                                                  T       t                  √                                       2\\n                                                                ∑       ∑       ′       ∑    √         R˜t(I, a)\\n                        ˜ T         2           2                              t             √                            t\\n                max( R      (I, a))    ≤|A|∆     T + 4∆|A|                   x   (I)                ∑  t       ′      − y (I, a)                    (35)\\n                 a∈A                                            t=1     t′=1            a∈A            t′=1  x t(I)\\n                                                2               ∑ T  (  ∑ t    t′    )  ∥    [  t       t          ]               t  ∥\\n                                       =|A|∆     T + 4∆|A|      t=1     t′=1 x   (I)    ∥ E t  ˜r (I)|x  (I) = 1     − V (I, a|θ    ) ∥2            (36)\\n                                                2               ∑ T      t     t\\n                                       =|A|∆     T + 4∆|A|      t=1  tΠ   (I)\\x0f (I)       by definition                                              (37)\\n                                       ≤|A|∆    2T + 4∆|A|T        ∑T   Π t(I )\\x0f t(I)                                                               (38)\\n                                                                   t=1                                                                              (39)\\n\\n\\n\\nThe first term of this expression is the same as Theorem 3, while the second term accounts for the approximation error.\\n\\n\\n\\n    7The careful reader may note that C(I) = 0 for unvisited infosets, but σ          t(I, a) can play an arbitrary strategy at these infosets so it’s\\nokay.\\n---\\n                                              Deep Counterfactual Regret Minimization\\n\\n\\n\\nIn the case of K-external sampling, the same derivation as shown in Theorem 3 leads to\\n\\n\\n\\n                                        ˜T        2          2     2        √          2 ∑T    t     t\\n                                max( R     (I, a))  ≤ |A|∆     T K   + 4∆      |A|T K        Π (I)\\x0f   (I)                    (40)\\nin this case. We elide the proof.a∈A                                                     t=1\\n\\n\\n\\nThe new regret bound in Eq. (40) can be plugged into Lanctot 2013, Theorem 3 as we do for Theorem 4, leading to\\n\\n\\n\\n                                          \\uf8eb  (             )                   √                          \\uf8f6\\n                                                     √          √              √          T\\n                                     ∑                  2          |A|      4  √         ∑\\n                             ¯ T          \\uf8ed         √            √         √   √                t     t   \\uf8f8\\n                             R p  ≤  I∈Ip      1 +    ρK     ∆      T   +    T     |A|∆  t=1  Π (I)\\x0f (I)                     (41)\\n\\n\\n\\nSimplifying the first term and rearranging,\\n\\n\\n\\n             (             )                                    √\\n                      √              √           √              √   T\\n       T                2               |A|    4   |A|∆    ∑    √∑\\n      ¯                                                         √         t    t\\n     R p  ≤  ( 1 + √   ρK  )  ∆|I p | √ T   +     √  T     I∈Ip    t=1 √Π (I)\\x0f (I)                                           (42)\\n                      √              √           √             ∑       √    T\\n                        2               |A|    4   |A|∆           I∈I p√  ∑\\n      ¯T                                                               √         t     t\\n     R p  ≤    1 + √   ρK     ∆|I p | √ T   +     √  T    |Ip | |I p|     t=1  Π (I)\\x0f   (I)        Adding canceling factors  (43)\\n             (        √    )         √           √            √\\n                                                              √  ∑T   ∑\\n                        2               |A|    4   |A|∆|I   p|√              t     t\\n          ≤    1 + √   ρK     ∆|I p | √ T   +        √ T      √   t=1 I∈I p Π (I)\\x0f (I)          by Jensen’s inequality       (44)\\n\\n\\n\\nNow, lets consider the average MSE loss L        T (M  T ) at time T over the samples in memory M         T.\\nWe start by stating two well-known lemmas:       V\\nLemma 3. The MSE can be decomposed into bias and variance components\\n\\n\\n\\n                                               E x[(x − θ)  2] = (θ − E[x])   2+ Var(θ)                                      (45)\\n\\n\\n\\nLemma 4. The mean of a random variable minimizes the MSE loss\\n\\n\\n\\n                                                     argminE    x[(x − θ) 2 ] = E[x]                                         (46)\\n                                                        θ\\n\\n\\n\\nand the value of the loss at when θ = E[x] is Var(x).\\n\\n\\n\\n                                   T               1            ∑     ∑T    t    ∥  t              T  ∥2\\n                                 L V  =  ∑        ∑  T     t              x (I)  ∥r˜ (I) − V (I|θ    )∥2                     (47)\\n                                            I∈I p    t=1 x (I)  I∈I p t=1\\n                                           1     ∑    ∑T    t    ∥  t              T  ∥2\\n                                      ≥  |Ip|T  I∈I p t=1 x (I)  ∥r˜ (I) − V (I|θ    )∥2                                     (48)\\n                                          1    ∑               [∥                    ∥ 2∣            ]\\n                                      =             Π T (I) E   ∥ ˜ t              T ∥  ∣  t\\n                                         |Ip| I∈I p           t    r (I) − V (I|θ   )  2∣x (I) = 1                           (49)\\n\\n\\n\\nLet V  ∗ be the model that minimizes L     T  on M   T . Using Lemmas 3 and 4,\\n---\\n                                                                  Deep Counterfactual Regret Minimization\\n\\n\\n\\n                                                T        1     ∑      T      ( ∥         T         [  t    ∣ t          ]∥ 2      T  )\\n                                              L   ≥                 Π   (I)    ∥ V (I|θ   ) − E     ˜      ∣             ∥\\n                                                V     |I p|T  I∈I p                              t   r (I) x   (I) = 1     2 + L  V ∗                       (50)\\n\\n\\n\\n                So,\\n\\n\\n\\n                                                                    L T  − L  T ∗ ≥     1   ∑     Π T (I) \\x0f T (I)                                           (51)\\n                                                                    ∑ V    T  V    T  |I p| I∈I p    T       T\\n                                                                   I∈I p Π   (I) \\x0f   (I) ≤ |I   p|(L V  − L  V ∗ )                                          (52)\\n\\n\\n\\n                Plugging this into Eq. 42, we arrive at\\n\\n\\n\\n                                                     (               )                                     √\\n                                                               √               √            √              √         T\\n                                                                 2                |A|      4   |A|∆|I   p |√       ∑\\n                                             R¯T  ≤     1 + √           ∆|I  p| √      +        √          √  |I p|     (L t  − L  t ∗)                     (53)\\n                                               p                ρK                 T               T                t=1    V       V\\n                                                  ≤  (  1 +  √ √ 2   )  ∆|I  p|√√ |A|  + 4|I  p |√  |A|∆\\x0f  L                                                (54)\\n\\n\\nρKT\\n                So far we have assumed that M            V  contains all sampled regrets. The number of samples in the memory at iteration t is\\n                bounded by K · |I      p | · t. Therefore, if K · |I    p| · T < |M     V | then the memory will never be full, and we can make this\\n                assumption.    8\\n\\n\\n\\n                B.5. Proof of Corollary 1\\n\\n\\n\\n                Proof. Let ρ = T      −1/4  .       (  ¯ T    (        √  2 )          √  |A|            √            )       −1/4\\n                                                 P    R  p >     1 +   √ K     ∆|I  p |T −1/4   + 4|I  p|   |A|∆\\x0f   L    < T                                (55)\\n\\n\\n\\n                Therefore, for any \\x0f > 0,                                  ( ¯ T           √                )\\n\\n\\nlimPR− 4|I||A|∆\\x0f> \\x0f= 0.(56)\\n                                                                T →∞           p        p            L\\n                    8We do not formally handle the case where the memories become full in this work. Intuitively, reservoir sampling should work well\\n                because it keeps an ‘unbiased’ sample of previous iterations’ regrets. We observe empirically in Figure 4 that reservoir sampling performs\\n                well while using a sliding window does not.\\n---\\nC. Network Architecture                                            Deep Counterfactual Regret Minimization\\nIn order to clarify the network architecture used in this work, we provide a PyTorch (Paszke et al., 2017) implementation\\nbelow.\\n\\n\\n\\n import         t o r c h\\n import         t o r c h . nn a s nn\\n import         t o r c h . nn . f u n c t i o n a l         a s F\\n\\n\\n\\n c l a s s    CardEmbedding ( nn . Module ) :\\n          d e f        i n i t      ( s e l f ,     dim ) :\\n                  s u pe r ( CardEmbedding ,                      s e l f ) .       i n i t      ( )\\n                   s e l f . r a n k = nn . Embedding ( 1 3 , dim )\\n                   s e l f . s u i t    = nn . Embedding ( 4 , dim )\\n                   s e l f . c a r d = nn . Embedding ( 5 2 , dim )\\n\\n\\n\\n          d e f   f o r w a r d ( s e l f ,       i n p u t ) :\\n                  B ,    n u m c a r d s = i n p u t . s h a p e\\n                  x = i n p u t . view ( −1)\\n\\n\\n\\n                  v a l i d = x . ge ( 0 ) . f l o a t ( )                # −1 means               ’ no c a r d ’\\n                  x = x . clamp ( min =0)\\n\\n\\n\\n                  embs =          s e l f . c a r d ( x ) +         s e l f . r a n k ( x      / /   4 ) +       s e l f . s u i t ( x % 4 )\\n                  embs = embs ∗                   v a l i d . u n s q u e e z e ( 1 )         # z e r o       o u t    ’ no c a r d ’ e m b e d d i n g s\\n\\n\\n\\n                  # sum a c r o s s            t h e     c a r d s    i n   t h e     h o l e / board\\n                  r e t u r n embs . view ( B ,                n u m c a r d s ,        −1).sum ( 1 )\\n\\n\\n\\n c l a s s    DeepCFRModel ( nn . Module ) :\\n          d e f        i n i t      ( s e l f ,     n c a r d t y p e s ,      n b e t s ,    n a c t i o n s ,     dim = 2 5 6 ) :\\n                  s u pe r ( DeepCFRModel ,                    s e l f ) .       i n i t      ( )\\n\\n\\n\\n                   s e l f . c a r d e m b e d d i n g s = nn . M o d u l e L i s t (\\n                            [ CardEmbedding ( dim )                     f o r        i n    range ( n c a r d t y p e s ) ] )\\n\\n\\n\\n                   s e l f . c a r d 1 = nn . L i n e a r ( dim ∗ n c a r d t y p e s ,                       dim )\\n                   s e l f . c a r d 2 = nn . L i n e a r ( dim ,                dim )\\n                   s e l f . c a r d 3 = nn . L i n e a r ( dim ,                dim )\\n\\n\\n\\n                   s e l f . b e t 1 = nn . L i n e a r ( n b e t s ∗ 2 , dim )\\n                   s e l f . b e t 2 = nn . L i n e a r ( dim ,                dim )\\n\\n\\n\\n                   s e l f . comb1 = nn . L i n e a r ( 2 ∗ dim ,                         dim )\\n                   s e l f . comb2 = nn . L i n e a r ( dim ,                    dim )\\n                   s e l f . comb3 = nn . L i n e a r ( dim ,                    dim )\\n\\n\\n\\n                   s e l f . a c t i o n h e a d = nn . L i n e a r ( dim ,                   n a c t i o n s )\\n\\n\\n\\n          d e f   f o r w a r d ( s e l f ,       c a r d s ,    b e t s ) :\\n\\n\\n\\n                  ”””\\n                  c a r d s :     ( ( N x      2 ) ,     (N x       3 ) [ ,    (N x       1 ) ,    (N x       1 ) ] )      # ( h o l e ,       board ,      [ t u r n ,  r i v e r ] )\\n                  b e t s : N x          n b e t f e a t s\\n                  ”””\\n\\n\\n\\n                  # 1 .       c a r d    b r a n c h\\n                  # embed h o l e ,               f l o p ,    and      o p t i o n a l l y      t u r n    and      r i v e r\\n                  c a r d e m b s = [ ]\\n                  f o r    embedding ,              c a r d g r o u p       i n    z i p ( s e l f . c a r d e m b e d d i n g s ,           c a r d s ) :\\n                            c a r d e m b s . a pp e n d ( embedding ( c a r d g r o u p ) )\\n                  c a r d e m b s = t o r c h . c a t ( c a r d e m b s ,                   dim =1)\\n\\n\\n\\n                  x = F . r e l u ( s e l f . c a r d 1 ( c a r d e m b s ) )\\n                  x = F . r e l u ( s e l f . c a r d 2 ( x ) )\\n---\\n                                                     Deep Counterfactual Regret Minimization\\n\\n\\n\\nx = F . r e l u ( s e l f . c a r d 3 ( x ) )\\n\\n\\n\\n#     1 .    b e t    b r a n c h\\n b e t s i z e        =    b e t s . clamp ( 0 ,            1 e6 )\\n b e t o c c u r r e d         =    b e t s . ge ( 0 )\\n b e t f e a t s        =    t o r c h . c a t ( [ b e t s i z e ,             b e t o c c u r r e d . f l o a t ( ) ] ,  dim =1)\\ny = F . r e l u ( s e l f . b e t 1 ( b e t f e a t s ) )\\ny = F . r e l u ( s e l f . b e t 2 ( y ) + y )\\n\\n\\n\\n#     3 .    c omb ine d          t r u n k\\n z =       t o r c h . c a t ( [ x ,       y ] ,     dim =1)\\n z = F . r e l u ( s e l f . comb1 ( z ) )\\n z = F . r e l u ( s e l f . comb2 ( z ) + z )\\n z = F . r e l u ( s e l f . comb3 ( z ) + z )\\n\\n\\n\\n z = n o r m a l i z e ( z )               #    ( z − mean )              /    s t d\\n r e t u r n      s e l f . a c t i o n h e a d ( z )\\n---\\n                       Title: Combining CFR-based algorithms with deep reinforcement learning\\n                       techniques (RLCCFR)\\n\\n\\n\\n                       First Author: Behbod Keshavarzi\\n\\n\\n\\n                               \\uf0b7 Affiliation: [Shahed university, Mathematics]\\n                               \\uf0b7   Email: Behbod.Keshavarzi@yahoo.com\\n\\n\\n\\n                       Second Author: Hamidreza Navidi\\n\\n\\n\\n                               \\uf0b7 Affiliation: [Shahed university, Mathematics]\\n                               \\uf0b7 Email: navidi@shahed.ac.ir\\n\\n\\n\\n                       Three Author: Mojtaba Tefagh\\n\\n\\n\\n                               \\uf0b7 Affiliation: [Sharif university of technology, Computer science]\\n\\n\\n\\uf0b7 Email: mtefagh@sharif.edu\\n               Preprint not peer reviewed\\n      This preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=4806706\\n---\\n                                    Combining CFR-based algorithms with deep\\n                                   reinforcement learning techniques (RLCCFR)\\n\\n\\n\\n                                    Behbod Keshavarzi      a,1, Hamidreza Navidi    a,∗, Mojtaba Tefagh    b\\n\\n\\n\\n                                                        aIran, Tehran, Shahed University\\n                                                 bIran, Tehran, Sharif University of Technology\\n\\n\\n\\n                          Abstract\\n\\n\\n\\n                          Finding a Nash equilibrium in a two-player game is a common approach to\\n                          solving two-agent decision-making issues in general. In a two-player, zero-sum\\n                          game with imperfect information, counterfactual regret minimization (CFR) is a\\n                          popular method for determining a Nash equilibrium strategy. Over the past few\\n                          years, several CFR variations have emerged, such as DCFR, LCFR, DeepCFR,\\n                          ECFR, RCFR, among others. The general classification of algorithms falls into\\n                          three categories: 1. strategy updating, regret calculation, and table CFR-based\\n                          algorithms; 2. sampling CFR-based algorithms; and 3. abstraction-based CFR-\\n                          based algorithms. In this paper, the methods in each category can be put\\n                          together with methods from other categories to make a novel algorithm. We aim\\n                          to improve convergence and speed by considering two criteria: exploitability and\\n                          speed-to-iteration ratio in this new algorithm. This process utilizes the DQN\\n                          algorithm. This method is a combination of CFR and reinforcement learning,\\n                          called reinforcement learning combined CFR (RLCCFR).\\n                          Keywords: Counterfactual regret minimization, Zero-sum games,\\n                          Reinforcement learning, imperfect information\\n\\n\\n\\n                              ∗Hamidreza Navidi\\n                               Email addresses: Behbod.keshavarzi@yahoo.com (Behbod Keshavarzi),\\n                          navidi@shahed.ac.ir (Hamidreza Navidi ), mtefagh@sharif.edu (Mojtaba Tefagh)\\n\\n\\n\\n                          Preprint submitted to Elsevier                                                April 17, 2024\\n         Preprint not peer reviewed\\nThis preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=4806706\\n---\\n                           1. Introduction\\n\\n\\n\\n                               There are two types of games: perfect information games (PIGs) and im-\\n                           perfect information games (IIGs), depending on whether the player has full\\n                           knowledge of the game state of all the other players. A game with imperfect in-\\n                       5   formation can be used to model strategic interactions involving multiple agents\\n                           with incomplete information. Every player can see every piece on the board\\n                           at all times in chess, which is a game with perfect information. Checkers, Go,\\n                           Reversi, and tic-tac-toe are other games with perfect information. Games with\\n                           imperfect information, such as poker and bridge, are examples.\\n                      10       Counterfactual regret minimization, often known as CFR, is a common ap-\\n                           proach to calculating the Nash equilibrium strategy [1]. A Nash equilibrium is\\n                           eventually reached by applying CFR to two-player zero-sum games. CFRs have\\n                           been established in a wide variety over the past few years [1, 2, 3, 4, 5]. Each\\n                           repetition of vanilla CFR adds equally to the total regret in the table. But it is\\n                      15   possible that certain iterations most notably the earlier ones contribute to far\\n                           less accurate regret estimates [1]. A modification of regret matching known as\\n\\n\\n\\n                           regret matching+ is used by CF R         + . In this version of regret matching, regrets\\n                           are required to be positive [5].\\n                               The development of Monte Carlo CFR (MCCFR) allowed sampling tech-\\n                      20   niques to be introduced that greatly reduced the size of the game tree’s traveled\\n                           region, making CFR approaches much more broadly adaptable to various tree\\n                           types and sizes [2]. The authors of this work provided three novel CFR versions\\n                           that sampled less frequently than the usual method. The researchers demon-\\n                           strated that public chance sampling converges faster than chance sampling on\\n                      25   big games, resulting in a more efficient method for approximating equilibrium\\n                           [3].Discounted CFR works by assigning a weight to each iteration, and the im-\\n                           pact of that iteration on the cumulative regret updates is proportional to the\\n                           weight [4]. The linear CFR is one of the most famous and successful types of\\n                      30   reduced CFR. Linear CFR increases the weight each iteration gets as train-\\n\\n\\n\\n        Preprint not peer reviewed                                        2\\nThis preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=4806706\\n---\\n                           ing progresses. In other words, at iteration t, cumulative regret changes are\\n                           weighted by the multiplicative factor t [6]. In deep counterfactual regret mini-\\n                           mization, deep neural networks are used instead of abstractions to approximate\\n                           CFR behavior in full games [6]. Instead of using domain-specific abstractions, it\\n                       35  uses two connected deep neural networks to learn strategy profiles directly from\\n                           the data. They have also developed novel sampling techniques for IIGs that\\n                           reduce variance and memory consumption. Their double neural counterfactual\\n                           regret minimization (DNCFR) strategy was validated by experimental results\\n                           [7]. Monte Carlo counterfactual regret minimization can now be performed with\\n                       40  reduced variance. State-action baselines appear to be more accurate than state\\n                           baselines in imperfect information games, indicating that this method is similar\\n                           to existing RL methods of state and state-action baselines [8].\\n                               The exponential CFR (ECFR) speeds up the CFR by focusing on the most\\n                           beneficial acts. The regret value of an action indicates how much better it is\\n                       45  than the predicted action in the current plan [9]. RLCFR is a system that brings\\n                           together the RL and the CFR. A Markov decision process is used to describe\\n                           the dynamic process of iterative interactive strategy update in the framework\\n                           [10]. In autoCFR, users learn how to create new versions of CFR. By using\\n                           a formal language, CFR-type algorithms can be represented as computational\\n                       50  graphs. A regularized evolution method is then used to search through the space\\n                           of computational graphs quickly [11]. Games of partial knowledge are prevalent\\n                           in a variety of leisure activities and real-world games. The authors discuss sig-\\n                           nificant achievements in the area and show that abstraction has become a key\\n                           facilitator for solving large incomplete-information games. Review the current\\n                       55  justifications for abstracting games, highlighting the problem of pathology in ab-\\n                           straction as well as the use-able methods for action and information abstraction\\n                           [12].\\n                               Using CFR-BR, a game solving algorithm that converges to one of these\\n                           least exploitable abstract strategies without causing a high memory cost that\\n                       60  previously rendered such a solution intractable, the effectiveness of this strategy\\n                           was also demonstrated in the context of two-player limit Texas hold’em. With\\n\\n\\n\\n         Preprint not peer reviewed                                        3\\nThis preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=4806706\\n---\\n                                     CFR-BR, it is possible to produce much closer approximations to the unknown,\\n                                     optimal Nash equilibrium strategy within an abstraction than is possible with\\n                                     previous state-of-the-art techniques [13]. Previous works on abstraction in big\\n                                 65  games can be seen as a principled generalization of this method. In [14], abstrac-\\n                                     tion and equilibrium are both learned through self-playalization of this method.\\n                                     Demonstrate that the method produces better tactics than the best modeling\\n                                     techniques, given the same amount of resources [14]. In this paper, we describe\\n                                     the different types of CFR algorithms in order to develop an algorithm that is\\n                                 70  suitable for each environment. The next step is to discuss the structure of the\\n                                     reinforcement learning algorithm. As a final step, a comparison of the proposed\\n                                     algorithm with other algorithms is discussed and the results are tested.\\n\\n\\n\\n                                     2. Methodology\\n\\n\\n\\n                                          The first step of this section is to analyze and classify the types of CFR al-\\n                                 75  gorithms and describe their convergence theorems, followed by the introduction\\n                                     of reinforcement learning algorithms as a way of combining the algorithms.\\n\\n\\n\\n                                                                        Table 1: Variable definitions\\n\\n\\n\\n                                       Variable                                            Definition\\n                                            Ii                                  The information set of player i.\\n                                            T                                             Iteration time.\\n                                            Z                                            Terminal states.\\n                                            ∆         The range of utilities available to the player i. ∆     u,i= maxu (Z)i− minu (Z)   i\\n\\n\\n\\n                                                                             The strategy where σ  i is the strategy of\\n                                            σ                            player i,\\n\\n\\nσis the strategy of the other player.\\n                                                                                    −i\\n                                         π σ (h)               If all players choose actions based on σ, history h may occur.\\n                                            u               In this utility function, the utility of player i is represented by u .   i\\n                                          A(I)                                           Sets of actions.\\n                                          P (h)                      The player who takes an action after the history h.\\n                                           M                                            √  |I | ≤ M   ≤ I .\\n                                              i                                              i       i    i\\n\\n\\n\\n                   Preprint not peer reviewed                                         4\\n          This preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=4806706\\n---\\n                               2.1. Strategy updating, regret calculation, and table CFR-based algorithms\\n\\n\\n\\n                                   There have been many variations of CFR variants that have been developed\\n                               since vanilla CFR was first made, for a variety of reasons [15]. There has been\\n                               a significant increase in the original convergence rate of CFR as a result of this\\n                               change [1]. CFR involves regret computation and matching. First, the regret\\n                               values of each action are proposed. A new global strategy will be updated as\\n\\n\\n\\n                               the final step. CF R     + is like CFR, but it has two small but significant changes\\n                               that make it work better [1]. It also converges orders of magnitude faster than\\n\\n\\n\\n                               CFR. R    T (I, a) = max(R     T (I, a), 0), then the cumulative regrets to obtain the\\n                                         i                    i\\n                               new strategy:\\n\\n\\n\\n                                                             \\uf8f1      R T ,+(I,a)       ∑\\n                                                             \\uf8f4        i                            T,+\\n                                                             \\uf8f2  ∑          T ,+          a∈A(I)  R i   (I, a) > 0\\n                                            σT +1 (I)(a) =        a∈A(I) R i  (I,a)                                         (1)\\n                                             i               \\uf8f4\\n                                                             \\uf8f3   1                    otherwise\\n                                                                A(I)\\n\\n\\n\\n                               Theorem 1. [1] In a zero-sum game at time T , if both players’ average overall\\n\\n\\n\\n                               regret is less than \\x0f, then σ    T  is a 2\\x0f equilibrium.\\n\\n\\n\\n                           80  Theorem 2. [1] If player i selects actions according to Equation 1 then R                T     (I) ≤\\n                               ∆    √  |A|/ √ T and consequently R       T (I) ≤ ∆     |I  |√ |A|/  √ T                 i,imm\\n                                 u,i                                     i           u,i  i\\n                               where |A | = max               |A(h)|.\\n                                          i          h:P (h)=i\\n\\n\\n\\n                               First, instead of waiting for the cumulative regret to turn positive before using an\\n\\n\\n\\n                               action again, CF R     +  resets any action with a negative cumulative regret to zero\\n                           85  at each iteration. This makes it possible to use an action again when it looks like\\n\\n\\n\\n                               it works well. Second, CF R       + doesn’t use a uniform weighted averaging strategy\\n                               like CFR. Instead, it uses a weighted averaging strategy where T iterations are\\n                               weighted. Linear CFR (LCFR) is similar to CFR, it gives the changes to the\\n                               average and regret strategy weight on iteration t In other words, the iterates are\\n                           90  linearly weighted [6]. On each repetition, one might essentially increase the total\\n\\n\\n\\n                               regret by     t . DCF R    α,β,γ  calculated by dividing the total number of positive\\n                               regrets by  t+1tα   and negative regrets by          tβ  , and (   t  )γ on each iteration t\\n\\n\\nαβ\\n                                             t +1                                 t +1           t+1\\n             Preprint not peer reviewed                                        5\\n    This preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=4806706\\n---\\n                                    contributions to the average strategy. Also LCFR is equivalent to DCF R                   1,1,1\\n                                    [4].\\n\\n\\n\\n                                95  Theorem 3. [4] Assume that T iterations of DCFR are conducted in a two-                  √\\n                                    player zero-sum game. Then the weighted average strategy profile is a 6∆|I|(                |A|+\\n                                    √ 1 )/ √ T -Nash equilibrium.\\n\\n\\nT\\n                                    exponential counterfactual regret minimization (ECFR), During the iteration\\n                                    process, a reweighting of the instantaneous regret value is accomplished through\\n                               100  the utilization of an exponential weighting technique. ECFR determines the\\n                                    weight of action a by w(I, a) = exp(r (I, a) −            1   ∑         r (I, a) [9].\\n                                                                                i                             i\\n\\n\\n|A(I)|a∈A(I)\\n                                    Theorem 4. [9] Assume that the number of iterations is T and that ECFR is\\n                                    conducted as a two-player zero-sum game. Then the weighted average strategy\\n                                    profile is a ∆|I|(  √  |A|e 2T −T 2)/√  T Nash equilibrium.\\n\\n\\n\\n                               105      Deep counterfactual regret minimization is a type of CFR that eliminates\\n                                    the requirement for abstraction by making use of deep neural networks to ap-\\n                                    proximate the behavior of CFR while it is being applied to the full game [7].\\n                                    This form of CFR is also known as deep counterfactual regret minimization.\\n                                    With this version of CFR, large games can be played successfully without using\\n                               110  tables. In deep CFR, function approximations are made using deep neural net-\\n                                    works to approximate the behavior of CFR without computing and collecting\\n                                    regrets at each infoset. The external sampling MCCFR is used to determine\\n                                    the route of each partial traversal of the game tree performed by Deep CFR on\\n                                    each iteration t [6].\\n\\n\\n\\n                               115  2.2. Sampling CFR-based Algorithms\\n\\n\\n\\n                                        The challenge of developing more general CFR strategies for imperfect infor-\\n                                    mation games is finding procedures that converge to equilibrium for games with\\n                                    unboundedly vast trees. A variety of methods can be used for sampling. There-\\n                                    fore, sampling methods are used to solve extensive games with many dimen-\\n                               120  sions. Chance-sampled CFR considers only one possible outcome per traversal,\\n\\n\\n\\n                  Preprint not peer reviewed                                       6\\n          This preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=4806706\\n---\\n                                 resulting in more rapid but less accurate iterations. CFR with chance sampling\\n                                 requires more iterations, but it converges faster in the end [3]. Monte Carlo\\n                                 CFR (MCCFR) introduced sampling methods that significantly reduced tra-\\n                                 versed areas in game trees [2].\\n                            125  The outcome sampling technique produces a single terminal history by only\\n                                 sampling one action per information set encountered during each iteration of\\n                                 the process. For each sampled information set or action pair, the tabular cu-\\n                                 mulative regrets will be updated [2, 15].\\n\\n\\n\\n                                 Theorem 5. [2, 15] For any p ∈ (0, 1], when using outcome-sampling MC-\\n\\n\\n\\n                            130  CFR where ∀z ∈ Z either π           σ (z) = 0 or q(z) ≥ δ > 0 at every timestep,\\n                                                                     −i\\n                                 with probability at least 1 − p, average overall regret is bounded by, R                 T  ≤\\n                                       √ 2   1          √       √                                                         i\\n                                 (1 +  √  )(  )∆    M      |A |/   T .\\n\\n\\nu,iii\\n                                         p   δ\\n                                     External sampling is the most popular MCCFR variant today, combining\\n                                 outcome sampling and vanilla CFR. In all game states where the regret-updating\\n                            135  player is active, all actions are traversed in External sampling. An action is\\n                                 sampled, however, according to its outcome at all other nodes (the opponent\\n                                 and chance nodes) [2, 15].\\n\\n\\n\\n                                 Theorem 6. [2] For any p ∈ (0, 1], when using external-sampling MCCFR,\\n\\n\\n\\n                                 with probability at least 1 − p, average overall regret is bounded by, R           T  ≤ (1 +\\n                                 √ 2          √        √                                                            i\\n                            140  √  )∆     M     |A |/   T .\\n                                        u,i   i     i\\n\\n\\np\\n                                     Sampling with probing: using this method, any generic estimator of the\\n                                 desired values can be used to generalize MCCFR. It is possible to minimize\\n                                 regret probabilistically by selecting an estimator that is unbiased and bound.\\n                                 Calculate regret from the estimate’s variance and the algorithm’s convergence\\n                            145  rate [16]. Estimated counterfactual regret on iteration t for action a at I to be:\\n\\n\\n\\n                                  t         t                 t\\n                                 rˆ = ˆv (σ       , I) − vˆ (σ , I).\\n                                  i     i   (I→a)         i\\n                                 Theorem 7. [16] Let p ∈ (0, 1] and suppose that there exists a bound ∆             ˆ i on the\\n                                 difference between any two estimates, vˆ(σ     i   t     , I) − vˆi(σt, I) ≤ ∆ˆ i. If strate-\\n                                                                                    (I→a)\\n                                 gies are selected according to regret matching on the estimated counterfactual\\n\\n\\n\\n               Preprint not peer reviewed                                      7\\n       This preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=4806706\\n---\\n                                150  regrets, then with probability at least 1 − p, the average regret is bounded by\\n                                     R T        √        ˆ      √                  2\\n                                      i  ≤ |I|    |A |( √∆ i +     var +  cov +  E   ) where V ar = max                           =\\n                                      T     t        i  t T        pT      p      p                          t∈1,...,t,I∈I,a∈A(a)\\n                                     V ar[r (I, a) − rˆ (I, a)], with Cov and E similarly defined.\\n\\n\\ni i\\n                                         Average Strategy Sampling (AS), This technique chooses a player’s actions\\n                                     based on three predetermined parameters and the cumulative profile. At each\\n                                155  information set I, a subset of player i’s actions are sampled instead of just one\\n                                     (OS) or every action (ES), which is comparable to OS and ES. The actions\\n                                     of player i, a ∈ A(I), are each randomly selected with probability ρ(a, I) =\\n\\n\\n\\n                                                  β+τ sT (I,a)        T\\n                                     max{\\x0f,   β+ ∑     i  sT (I,b)}, si (I, .) is the cumulative profile on iteration T, \\x0f ∈\\n                                                    b∈A(I) i\\n                                     (0, 1] is exploration parameter, τ ∈ [1, ∞) is threshold parameter and β ∈ [0, ∞)\\n                                160  is bonus parameter [17].\\n\\n\\n\\n                                     Theorem 8. [17] Let X be one of CS, ES, or OS, let p ∈ (0, 1], and let δ =\\n                                     min  z∈Z qi(z) > 0 over all 1 ≤ t ≤ T . When using X, with probability 1 − p,\\n                                                                         T                   √ 2|I ||B |        √       √\\n                                     average regret is bounded by       Ri  ≤ (M (σi  ∗ ) +     √ i  i )( 1)∆  i   |Ai|/   T .\\n\\n\\nT i pδ\\n                                         Variance Reduction in Monte Carlo Counterfactual Regret Minimization\\n                                165  (VR-MCCFR), Every iteration, estimated values and updates are reformu-\\n                                     lated as functions of sampled values and state-action baselines, similar to their\\n                                     use in policy gradient reinforcement learning. As a result of this formula-\\n                                     tion, estimates can be bootstrapped based on other estimates within the same\\n                                     episode, propagating the advantages of baselines along the sampled trajec-\\n                                170  tory. In addition to being a precise baseline, the variance of the estimated\\n\\n\\n\\n                                     values can be reduced to zero. baseline-enhanced estimate is ˆv                  b (σ, I, a) =\\n                                     vˆ (σ, I, a) −ˆb (σ, I, a) + b (σ, I, a), estimated counterfactual value is ˆv   i   , control\\n                                      i          ˆ   i             i                                                     i\\n\\n\\nvariate is band the action-dependent baseline is b[8, 18].\\n                                                  i                                               i\\n                                     2.3. Abstraction-based CFR-based Algorithms\\n\\n\\n\\n                                175      Game abstraction entails restricting players’ strategy spaces as a mode of\\n                                     abstraction. The most popular form of game abstraction is information abstrac-\\n                                     tion, in which information states are grouped together. Another type of game\\n                                     abstraction is action abstraction, in which some actions are presumed to be\\n\\n\\n\\n                  Preprint not peer reviewed                                        8\\n          This preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=4806706\\n---\\n                           useless. In such situations, lossy state-space abstraction can be used to create\\n                      180  a smaller, more tractable abstract game while hoping that the equilibrium of\\n                           the resultant abstract game is similar to the equilibrium strategy of the un-\\n                           abstracted game. As a result, CFR-BR avoids storing the opponent’s entire\\n                           strategy explicitly, which would have required enormous memory [13].\\n\\n\\n\\n                           Theorem 9. [13] After T iterations of CFR-BR, σ¯                  T  is player 1s part of an\\n                                                                   ∆|I1|√  |A1|              1\\n                      185  \\x0f − N ash equilibrium, with \\x0f =             √ T     .\\n\\n\\n\\n                               Regression CFR (RCFR): CFR is calculated for each information set using\\n                           estimations of counterfactual regret derived from a single common regressor\\n                           shared by all information sets. A common regressor employs characteristics\\n                           determined by the information-set/action combination, ϕ(I, a). This enables the\\n                      190  regression to generalize across comparable acts and contexts when calculating\\n                           regret estimates [14]. The action space in such situations has previously been\\n                           reduced using unsupervised clustering techniques applied to actions in feature\\n                           space. In this algorithm, use this feature vector to retain structure during\\n                           learning. Each action can be reduced to the standard framework by using an\\n                      195  indicator feature. Each action a ∈ A can be described by a feature vector, ϕ(a) ∈\\n                           χ. The training approach, in particular, is predicted to yield f where f (ϕ(a)) ≈\\n\\n\\n\\n                             t            ˜t\\n                           R (a)/t or, R (a) = tf (ϕ(a)). The regressor’s error in relation to the l            2 distance\\n                           between the approximation and true cumulative regret vectors: ||R − R ||         t   ˜t  2. This\\n                           kind of abstraction provides a function f for translating whole game information\\n                      200  sets into abstract game information sets. For this abstraction to be generated,\\n                           some form of clustering, such as k-means, and some concept of similarity and\\n                           compatibility between information sets are required. A function ϕ : I × A −→ χ\\n                           domain-specific features are mapped to information-set/action pairs χ. Consider\\n                           a single iteration of the counterfactual regret update in both the original game\\n                      205  and the abstract game where the players’ tactics are fixed in order to compare\\n                                                                        abstract      ∑                         t     f ull\\n                           the approach to RCFR [14]. r˜ (a|I     i              ) =       f ull −1   abstract r (a|I     ),\\n                                                                                          I   ∈f    (I        ) i\\n                           I abstract is information set in the abstract game, the regret at information set\\n\\n\\n\\n                           in the abstract game is ˜r (.|Ii    abstract).\\n\\n\\n\\n         Preprint not peer reviewed                                        9\\nThis preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=4806706\\n---\\n                          Theorem 10. [14] If for each t ∈ [T ], ||R          t− R ˜t||2 < \\x0f, then regression regret-\\n                     210  matching has regret bounded by         √ T N L + 2T    √ L\\x0f.\\n\\n\\n\\n                          Theorem 11. [14] If for each t ∈ [T ] at every information set I ∈ I , ||R (.|I)−i     t\\n                           ˜t                                                                      √               √\\n                          R (.|I)|| 2 < \\x0f, then RCFR has external regret bounded by |I           i |  T N L + 2T     L\\x0f.\\n\\n\\n\\n                          2.4. proof of convergence\\n\\n\\n\\n                              Since the CFR algorithm is proved with different combinations in theorems\\n\\n\\n\\n                     215  1 to 11, it is evident that this algorithm will converge in all cases. The CF R              +\\n                          algorithm is also very similar to the CFR algorithm, with a very small difference,\\n                          so the combination of these algorithms will also converge. In the scenario of\\n                          sampling and abstraction, the LCFR, DCFR, ECFR, and other forms of deep\\n                          CFR have not been demonstrated to converge at the same rate. Figures 2, 3,\\n                     220  and 4 show that the convergence is slightly faster in practice.\\n\\n\\n\\n                          2.5. Deep RL Algorithm\\n\\n\\n\\n                              In single-objective reinforcement learning, an agent interacts with the envi-\\n                          ronment by recognizing its state s        t ∈ S and playing an action a       t ∈ A for each\\n                          step t. Agents choose actions based on policies π. The agent receives a re-\\n                          ward r for performing an action [19]. In the next state, the agent observes s              t+1\\n                          and the process repeats. The agent’s goal is to maximize the expected reward.\\n                          R t =   ∑  ∞   γ krt+k , γ k ∈ [0, 1] is the discount factor. Actions are selected in\\n                                     k=0\\n                          Q-learning based on Q(s, a), which represents the expected discounted reward\\n                          for performing action a in state s. For the given state s, a = argmax                 aQ(s, a)\\n                          is the optimal action. Deep Q-learning uses deep neural networks to approx-\\n                          imate Q(s, a) values, as a result, many real-world applications can be met by\\n                          this method. Deep Q Network (DQN) is one of the most important deep rein-\\n                          forcement learning (DRL) methods. The DQN method was initially suggested\\n                          in 2013 by DeepMind researchers in the article Playing Atari with Deep Rein-\\n                          forcement Learning. For the regression task, the mean squared error (MSE) is\\n                          often used as the loss function. M SE =           1  ∑  K  (y  − yˆ ) 2, y is the goal value,\\n                                                                            K     i=1   i     i\\n\\n\\n\\n        Preprint not peer reviewed                                      10\\nThis preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=4806706\\n---\\n                                  yˆ is the predicted value, and the number of training samples is K [20]. That\\n                                  the Bellman optimality equation can be used to get the optimal Q value:\\n\\n\\n\\n                                                        Q ∗ (s, a) = E s′∼p[r + γmax   a′Q ∗(s′, a′)]                  (2)\\n\\n\\n\\n                                  Due to the loss function, describe the loss function L as the difference between\\n                                  the goal value and the predicted value on DQN [20].\\n\\n\\n\\n                                                                          ∗\\n\\n\\nL(θ) = Q(s, a) − Q(s, a)(3)\\n                                                                                      θ\\n                                      Due to the loss function can be expressed as [20]:\\n\\n\\n\\n                                                             L(θ) =   1  ∑K  (yi− Q  θ(si, ai))2                       (4)\\n                                                                      K  i=1\\n\\n\\n\\n                                                                      ′  ′\\n                                      where y  i= r i + γmax   a′Q θ(s , a )\\n                                      The reward will be the target value, as shown here:\\n                                                       \\uf8f1\\n                                                       \\uf8f4                                ′\\n                                                       \\uf8f2  ri                         if s is terminal\\n                                                  yi = \\uf8f4                     ′  ′       ′                              (5)\\n                             225      Initialization:  \\uf8f3  ri+ γmax    a′ Qθ(s , a ) if s  is not terminal\\n\\n\\n\\n                                      • Initialize replay memory D with capacity N\\n\\n\\n\\n                                      • Initialize action-value function Q with random weights θ\\n\\n\\n\\n                                      • Initialize target action-value function Q    ′ with weights θ  ′= θ\\n\\n\\n\\n                                      • Set exploration rate \\x0f and its parameters (e.g., start and end values, decay\\n                             230        schedule)\\n\\n\\n\\n                                      Training Loop:\\n\\n\\n\\n                                      • For episode = 1 to M do:\\n\\n\\n\\n                                           – Observe initial state s   1\\n\\n\\n\\n                                           – For t = 1 to T do:\\n\\n\\n\\n                 Preprint not peer reviewed                                  11\\n         This preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=4806706\\n---\\n                     235                 ∗ With probability \\x0f, select a random action a        t\\n                                         ∗ Otherwise, select a     = argmax     Q(s , a; θ)\\n                                                                 t            a     t\\n                                         ∗ Execute action a   t in the environment, observe reward r      tand next\\n                                            state st+1\\n                                         ∗ Store transition (s , a , r   , s   ) in D\\n                                                                 t  t   t  t+1\\n                     240                 ∗ Sample a random minibatch of transitions (s           , a , r, s   ) from\\n                                            D                                                   j   j  j  j+1\\n                                         ∗ Set target\\uf8f1for Q-learning:\\n                                                     \\uf8f4\\n                                                     \\uf8f2 rj                                for terminal s  j+1\\n                                               yj =  \\uf8f4\\n                                                     \\uf8f3                  ′        ′   ′\\n                                                       rj + γ max   a′ Q (sj+1 , a ; θ ) for non-terminal s   j+1\\n\\n\\n\\n                                         ∗ Update Q by minimizing the loss:\\n                                                        L(θ) =         1       ∑   (Q(s   , a; θ) − y )2\\n                                                                 |minibatch|     j       j  j        j\\n\\n\\n\\n                                         ∗ Every C steps, update the target network Q :        ′\\n\\n\\n\\n                                                                             θ′ ← θ\\n\\n\\n\\n                                         ∗ s t← s  t+1\\n\\n\\n\\n                          2.6. Exploitability and Reward\\n\\n\\n\\n                              A strategy σ    exploitability can be defined as follows: e (σ ) = u (σ       ∗, σ∗ ) −\\n                                             i                                                i  i      i  i   −i\\n                     245  u (σ , BR(σ     )). \\x0f-Nash equilibrium means that no player has exploitability\\n                            i  i        −i          ∑\\n                          higher than \\x0f. e(σ) =             = e (σ )/N . Another parameter is added here to\\n                                                       i∈N      i   i\\n                          optimize the algorithm, namely its convergence speed, which is represented by\\n                          S. As a result, the reward based on exploitability and speed is now defined as\\n                          follows: R = 1 − (αe + βs), so that α, β ∈ [0, 1] and respectively, the weights of\\n                     250  exploitability and, speed cannot both be zero at the same time.\\n\\n\\n\\n                          2.7. Proposed Algorithm\\n\\n\\n\\n                              The goal is to combine the algorithms and present a novel CFR algorithm\\n                          based on the types of games. The DQN algorithm is used to select the best\\n\\n\\n\\n         Preprint not peer reviewed                                   12\\nThis preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=4806706\\n---\\n                           algorithm. According to Figure 1, there are three states: State A is selecting\\n                      255  the best algorithm based on the reward received by DQN                   A  algorithm during\\n                           iterations and selecting the best strategy update; state B requires choosing the\\n                           best sampling algorithm for solving games of large dimensions in real world\\n                           examples, so the DQN        B  algorithm selects the optimal algorithm based on the\\n                           rewards it receives during each iteration. In state C, the sampling algorithm fails\\n                      260  to produce acceptable results when the game has very large dimensions. Using\\n                           the reward that the DQN        C algorithm receives in each iteration, this state selects\\n                           the best abstraction algorithm. There are three general categories of CFR-based\\n\\n\\n\\n                           algorithms, as shown in Table 1 and            †  indicates algorithms presented in this\\n                           paper.\\n\\n\\n\\n                           Table 2: Table CFR-based algorithms: The selection of algorithms is based on their specific\\n                           properties. Deep-based algorithms are chosen because they differ from current algorithms that\\n                           are based on a table. However, this restriction may be resolved by using machine learning\\n                           techniques.\\n\\n\\n\\n                                                                   CFR-based algorithms\\n                             Strategy updating, regret calcula-         Sampling                           Abstraction\\n                             tion, table and without table\\n                             CFR [1]                                    Chance sampling [2]                CFR-BR [13]\\n\\n\\n\\n                             CF R  + [1]                                MC Outcome Sampling [2]            RCFR [14]\\n\\n\\n\\n                             LCFR [6],LCF R     +                       MC external Sampling [2]           No abstraction\\n\\n\\n\\n                             DCFR [4],DCF R      +                      Sampling with probing [16]\\n\\n\\n\\n                             ECFR [9],ECF R     +  †                    MC Average Sampling [17]\\n\\n\\n\\n                             DeepCFR [6],DeepCF R       +  †            Variance Reduction MC [8]\\n\\n\\n\\n                             DeepLCFR †,DeepLCF R         +  †          No sampling\\n\\n\\n\\n                             DeepDCFR †,DeepDCF R          + †\\n\\n\\n\\n                             DeepECFR †,DeepECF R         +  †\\n\\n\\n\\n                      265      It would be better if we used multiple DQNs. The actions that we take\\n                           can be divided into three categories: [a              , a  , ..., a ], [a   , a  , ..., a ] and\\n                                                                              11   12       116      21   22       27\\n\\n\\n\\n                               †In this paper\\n\\n\\n\\n         Preprint not peer reviewed                                       13\\nThis preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=4806706\\n---\\n                                                                         Table 3: Detail of algorithms\\n\\n\\n\\n                                        Name Algorithm                    Sterategy updating                           Regret calculate\\n\\n\\n\\n                                                                        T +1               RT (I,a)                T           ∑          T\\n                                                 CFR                  σ i   (I, a) =  ∑     i  R T(I,a)          R i (I, a) =     a∈A(I) ri (I, a)\\n                                                                                        a∈A(I)   i\\n                                                      +                T +1               RT ,+(I,a)             T,+           ∑          T,+\\n                                                CF R                 σ i   (I, a) =  ∑     i  R T ,+(I,a)      R i   (I, a) =     a∈A(I) ri    (I, a)\\n                                                                                       a∈A(I)   i\\n                                                                       T +1               tR T(I,a)                T          ∑            T\\n                                                LCFR                  σi    (I, a) =  ∑      i tRT (I,a)         R i (I, a) =    a∈A(I)  tri (I, a)\\n                                                                                        a∈A(I)   i\\n                                                                                       (  t )γR T (I,a)                     ∑\\n                                                DCFR               σT +1 (I, a) =  ∑     t+1    i             R T (I, a) =            (  t )r T(I, a)\\n                                                                    i                       ( t )γ RT (I,a)     i              a∈A(I)  t+1   i\\n                                                                                     a∈A(I)  t+1    i\\n                                                                        T +1             e αR T(I,a)              T          ∑           α  T\\n                                                ECFR                  σ i   (I, a) =  ∑       iR T(I,a)         R i (I, a) =    a∈A(I)  e  ri (I, a)\\n\\n\\na∈A(I) i\\n                                      [a   , a   , a  ]. Generally, 3×7×16 = 336 algorithms can be evaluated on a game\\n                                         31   32   33\\n                                      environment and the best algorithm can be selected based on the evaluation.\\n                                              Agent   StateA      DON\\n                                                      State B     DQNB        Create\\n                                                                             Algorithmi                            Reward\\n                                                      State €     DONc                   ction                        Rewaitl  Rewaitl\\n                                                                                                                           Rewaitl\\n                                                                             Envirment\\n                                                                                 Kuhu   Leduc   Roral\\n                                                                                 poker   poker  Poker           Exploitability andSptrd\\n                                      Figure 1: The combined CFR-based algorithms using reinforcement learning are illustrated\\n                                      in the above figure.\\n\\n\\n\\n                                           [21] Provides a guide for applying multi-objective methods to challenging\\n                                 270  problems for researchers who have already used single-objective reinforcement\\n                                      learning and planning methods and who wish to adopt a multi-objective per-\\n                                      spective on their work. It is intended for researchers who are already familiar\\n                                      with single-objective reinforcement learning and planning methods but wish to\\n                                      adopt a multi-objective perspective on their research. Within [22], the multi-\\n                                 275  objective node selection issue is expressed using the Markov decision process\\n\\n\\n\\n                   Preprint not peer reviewed                                          14\\n          This preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=4806706\\n---\\n                               (MDP) with the dual goals of reducing training time and increasing model ac-\\n                               curacy. In order to find the best collection of participants for each iteration,\\n                               deep Q-networks (DQN) are suggested. In the proposed structure of the re-\\n                               inforcement learning algorithm, it consists of three parts: agent, environment,\\n                          280  and reward.\\n\\n\\n\\n                                  1. Agent: There are three categories of DQN algorithms in this part of\\n                                      the algorithm, DQN        A  algorithm aims to choose the best algorithm in\\n                                      [a   , a  , ..., a ], DQN      algorithm aims to choose the best algorithm in\\n                                        11   12       116         B\\n                          285         [a   , a  , ..., a] and DQN        algorithm aims to choose the best algorithm\\n                                        21   22       27              C\\n                                      in [a 31, a32, a33]. The output of this section is action (creat algorithm).\\n                                  2. Environment: There are all kinds of extensive form games in this section,\\n                                      some of the most famous examples of which can be seen in Figure 1. Also,\\n                                      the output of this section is two values exploitability and speed.\\n                          290     3. Reward: according to the function R = 1 − (αe + βs), α, β ∈ [0, 1], DQN\\n                                      inputs can have different values of alpha and beta. The output of this\\n                                      section is R  A , RB  and R  C . (R A  = 1−(α   A e+β   As), so that α  A , βA ∈ [0, 1].\\n                                      R    = 1 − (α    e + β   s), so that α    , β  ∈ [0, 1]. R    = 1 − (α    e + β   s), so\\n                                        B            B       B                B   B              C            C       C\\n                                      that α    , β  ∈ [0, 1]).\\n\\n\\nC C\\n                          295      The proposed technique involves considering three games by default. Each\\n                               game is then abstracted in two different ways, as seen in Table 2. This results\\n                               in a total of nine games, out of which six are abstracted versions and three are\\n                               in their original manner. In the initial stage of the algorithm, it is necessary to\\n                               specify the type of game. All nine games have already been implemented and\\n                          300  are provided as input to the algorithm. In the subsequent stage, the type of\\n                               sampling must be determined based on Table 2. Finally, the type of algorithm\\n\\n\\n\\n             Preprint not peer reviewed                                      15\\n     This preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=4806706\\n---\\n                         should be taken into account to update the strategy and regret calculate.\\n                             Initialize:\\n                             Initialize the game (Kuhn, Leduc, or Royal Poker). Choose between the\\n                             abstraction game (CFR-BR or RCFR) and the normal game (without\\n                             abstraction);\\n\\n\\n\\n                             Initialize strategies σ  tfor all players;\\n\\n\\n\\n                             Initialize cumulative regrets R    t for all players;\\n\\n\\n\\n                             Initialize cumulative strategy profiles ¯σ   t for all players;\\n                             Set external sampling probability ρ (0 < ρ ≤ 1);\\n                             for t = 1 to T do\\n                                  Simulate the game from the initial state;\\n                                  while current state is not terminal do\\n                                      if current state is a chance node then\\n                                      elseResolve chance event;\\n                                          Select current player;\\n                                          if current player is a real player then\\n\\n\\n\\n                                              Sample action according to strategy σ ;    t\\n                                              if random uniform(0, 1) < ρ then\\n                                                  // External sampling Sample regret from an external\\n                                              end state;\\n                                              else// Internal sampling Sample regret from current state;\\n                                              end\\n                                              Update cumulative regrets and strategy profiles according\\n                                      end end to Table 3;\\n                                  end Move to the next player or state;\\n                                  Update strategies based on cumulative regrets;\\n                             end                                     16\\n                            Algorithm 1: External Sampling MCCFR (ES-MCCFR) Combine with every\\n        Preprint not peer reviewed\\n                            algorithm in Table 2, column 1, 3.\\n\\n\\n\\nThis preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=4806706\\n---\\n                              Algorithm 1 demonstrates an example combination of techniques from columns\\n                     305  1 and 3 in Table 2. Each combination results in a new algorithm that utilizes\\n                          the DQN algorithm to identify the optimal combination for each game. The\\n                          implementation of the library includes all algorithm combinations, which are\\n                          categorized and chosen according to the DQN algorithm.\\n\\n\\n\\n                          3. Experiments\\n\\n\\n\\n                     310      Our evaluation is based on two metrics: speed and exploitability. First, we\\n                          present the experimental setup, which includes games, configuration games, and\\n                          training details.\\n\\n\\n\\n                          3.1. Experimental Setup\\n\\n\\n\\n                              We compare our proposed algorithm with existing CFR-based approaches\\n                     315  using three poker games: Kuhn, Leduc, Royal, and Goofspiel. The Kuhn Poker\\n                          deck contains only three cards and gives the player one chance to bet for each\\n                          player, there are no public cards, and each player has one hand. There are two\\n                          rounds in Leduc Poker, which is a larger game with a 6-card deck. In the first\\n                          round, each player has a private hand, and in the second round, each player\\n                     320  has a public hand. Three rounds are played in royal poker, and there are eight\\n                          cards. A private card is issued to each player in the first round, followed by a\\n                          public card in the second and third rounds. In Goofspiel (x), each player has x\\n                          cards, and in x rounds, they make secret bids to gain more points. Anaconda-3\\n                          Python 3.9 has been used to implement all the implemented codes on the Linux\\n                     325  Debian 9.5 operating system and the Acer Aspire 7 laptop (CPU AMD R7,\\n                          GPU 1060, RAM 16 GB), as well as the open spiel library [23] for comparing\\n                          algorithms.\\n\\n\\n\\n                          3.2. Experimental Result\\n\\n\\n\\n                              Eight state-of-the-art methods were used in the first group experiment.\\n                     330  Which are CF R , LCF R, ECF R, DCF R, DeepCF R, M CCF R and RLCCF R\\n                          respectively. We selected 1000 and 10,000 iterations for testing, and after 10,000\\n\\n\\n\\n        Preprint not peer reviewed                                     17\\nThis preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=4806706\\n---\\n                          iterations, for all methods, there was a very small reduction in exploitability. In\\n                          order to demonstrate the effectiveness of each method, we limited the testing to\\n                          10,000 iterations. Six different experimental approaches are shown in figures 2,\\n                     335  3, 4, and 5. These methods were compared to our RLCCFR method. An ad-\\n                          vantage of CFR is that regret-minimizing algorithms’ averaged strategy profiles\\n                          tend to be Nash equilibrium-like. Iteration regrets are reweighted differently\\n                          in LCFR and DCFR, both CFR-based approaches. We select DCF R                           1.5,0,2.\\n                          As iteration continues, the immediate regret value is reweighted using ECFR,\\n                     340  which is an exponential weighting method. Deep CFR introduces a value net, a\\n                          neural network that approximates R          t(I, a) and the cumulative regret value. A\\n                          Monte Carlo CFR (MCCFR) was developed to reduce the size of the game tree\\n                          traversed by introducing sampling methods; we chose external sampling.\\n                          According to Fig. 2, the Kuhn poker game evaluates seven algorithms, and\\n                     345  the proposed algorithm RLCCFR with selected parameters gives better results.\\n                          Figures 3, 4, and 5 show that some algorithms have better performance than\\n                          the proposed algorithm RLCCFR in terms of exploitability, because the speed\\n                          parameter has gained more weight, but they have worse conditions in terms of\\n                          speed. According to the selected parameters, the proposed algorithm RLCCFR\\n                     350  performs better than other algorithms in terms of convergence speed, as shown\\n                          in Table 2.\\n\\n\\n\\n                          4. Conclusion and Future Work\\n\\n\\n\\n                              This work presents a framework for RLCCFR designed as combined CFR\\n                          variants using DQN and new CFR designs. We developed three main categories\\n                     355  of CFR-based algorithms in this work. Several algorithms listed in Table 2 are\\n                          designed in this paper. A new CFR-based algorithm is constructed utilizing\\n                          the new reinforcement learning algorithm based on the best category of each\\n                          algorithm. As shown in Figures 2 through 5 and Table 3, the proposed algo-\\n                          rithm competed with the most famous algorithms in this field in four games of\\n                     360  convergence and convergence speed. With this method, new algorithms are cre-\\n\\n\\n\\n        Preprint not peer reviewed                                      18\\nThis preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=4806706\\n---\\nExploitability                            ated that are optimal in terms of speed and approximation of Nash equilibrium.\\nExploitability                            According to the experiments conducted and the results obtained in different\\n                                          environments, our method shows that it is better in terms of convergence and\\n                                          speed of convergence. Future work will include classifying and combining other\\n                                    365   algorithms used to solve extended games that are not CFR-based, such as ficti-\\n                                          tious self-play(FSP).\\n\\n\\n\\n                                             0.40                                    CFR                       0.200                                    CFR\\n                                             0.35                                    DCFR                      0.175                                    DCFR\\n                                             0.30                                    LCFR                      0.150                                    LCFR\\n                                                                                     DeepCFR                                                            DeepCFR\\n                                             0.25                                    MCCFR                     0.125                                    MCCFR\\n                                                                                     ECFR                                                               ECFR\\n                                             0.20                                    RLCCFR                    0.100                                    RLCCFR\\n                                             0.15                                                              0.075\\n                                             0.10                                                              0.050\\n                                             0.05                                                              0.025\\nExploitabilityExploitability                     0       200     400      600     800      1000                    0       2000    4000     6000    8000     10000\\n                                                                    Iteration                                                          Iteration\\n\\n\\n\\n                                          Figure 2: Tested on Kuhn poker and RLCCFR with parameters α                        A   = 1 and β    A  = 0.4,\\n                                          α B  = 0.8 and β     B  = 0.6, α  C   = 0.9 and β    C  = 0.5. The X-axis represents the number of\\n                                          iterations, and the Y-axis represents the exploitability. RLCCFR: A smaller exploitability is\\n                                          better in general.\\n\\n\\n\\n                                             0.040                                   CFR                       0.014                                    CFR\\n                                             0.035                                   DCFR                                                               DCFR\\n                                             0.030                                   LCFR                      0.012                                    LCFR\\n                                                                                     DeepCFR                                                            DeepCFR\\n                                             0.025                                   MCCFR                     0.010                                    MCCFR\\n                                                                                     ECFR                                                               ECFR\\n                                             0.020                                   RLCCFR                    0.008                                    RLCCFR\\n                                             0.015                                                             0.006\\n                                             0.010                                                             0.004\\n                                             0.005                                                             0.002\\n\\n\\n\\n                                                 0       200     400      600     800      1000                    0       2000    4000     6000    8000     10000\\n                                                                    Iteration                                                          Iteration\\n\\n\\n\\n                                          Figure 3: Tested on leduc poker and RLCCFR with parameters α                       A  = 0.6 and β     A  = 1,\\n                                          α B  = 0.4 and β     B  = 0.9, α  C   = 0.2 and β    C  = 0.8. The X-axis represents the number of\\n                                          iterations, and the Y-axis represents the exploitability. DCFR is better than other algorithms.\\n\\n\\n\\n                     Preprint not peer reviewed                                                 19\\n           This preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=4806706\\n---\\nExploitability                               0.010                                                              0.009Exploitability\\n                                                                                      CFR                       0.008                     CFR\\n                                                                                      DCFR                                                DCFR\\n                                             0.008                                    LCFR                      0.007                     LCFR\\n                                                                                      DeepCFR                                             DeepCFR\\n                                                                                      MCCFR                     0.006                     MCCFR\\n                                             0.006                                    ECFR                      0.005                     ECFR\\n                                             0.004                                    RLCCFR                    0.004                     RLCCFR\\n                                                                                                                0.003\\n                                             0.002                                                              0.002\\n                                                  0      200      400      600      800     1000                0.0010      2000     4000    6000     8000     10000\\n                                                                     Iteration                                                          Iteration\\n\\n\\n\\n                                           Figure 4: Tested on royal poker and RLCCFR with parameters α                       A = 0.7 and β     A  = 0.8,\\n                                           α B  = 0.6 and β    B   = 0.6, α  C   = 0.4 and β    C  = 0.9. The X-axis represents the number of\\n                                           iterations, and the Y-axis represents the exploitability. RLCCFR: A smaller exploitability is\\n                                           better in general.\\n\\n\\n\\nExploitabilityExploitability                  0.05                                    CFR                       0.030                                     CFR\\n                                                                                      DCFR                      0.025                                     DCFR\\n                                              0.04                                    LCFR                                                                LCFR\\n                                                                                      DeepCFR                                                             DeepCFR\\n                                                                                      MCCFR                     0.020                                     MCCFR\\n                                              0.03                                    ECFR                                                                ECFR\\n                                              0.02                                    RLCCFR                    0.015                                     RLCCFR\\n                                                                                                                0.010\\n                                              0.01                                                              0.005\\n\\n\\n\\n                                                  0      200      400      600      800     1000                     0      2000     4000    6000     8000     10000\\n                                                                     Iteration                                                          Iteration\\n\\n\\n\\n                                           Figure 5: Tested on goofspiel v4 and RLCCFR with parameters α                      A  = 0.6 and β    A  = 0.6,\\n                                           α B  = 0.5 and β    B   = 0.8, α  C   = 0.2 and β    C  = 0.7. The X-axis represents the number of\\n                                           iterations, and the Y-axis represents the exploitability. ECFR is better than other algorithms.\\n\\n\\n\\n                                                —————\\n\\n\\n\\n                      Preprint not peer reviewed                                                 20\\n            This preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=4806706\\n---\\n                                   Table 4: Tested on Kuhn poker for evaluate speed (Iteration per Speed) with parameters\\n                                   α   = 0.3 and β   = 0.8, α  = 0.4 and β    = 0.7, α  = 0.8 and β   = 1.\\n\\n\\nA A B B C C\\n                                          Name Algorithm       Speed Iteration 10  3 itr/s    Speed Iteration 10  4 itr/s\\n                                          CFR                  161.51                         153.66\\n                                          DCFR                 110.12                         114.94\\n                                          LCFR                 118.46                         124.16\\n                                          MCCFR                208.34                         212.74\\n                                          DeepCFR              90.31                          95.64\\n                                          ECFR                 127.91                         133.64\\n                                          RLCCFR               212.32                         215.54\\n\\n\\n\\n                                   References\\n\\n\\n\\n                                     [1] M. Zinkevich, M. Johanson, M. Bowling, C. Piccione, Regret minimization\\n                              370        in games with incomplete information, Advances in neural information pro-\\n                                         cessing systems 20 (2007) 1729–1736.\\n\\n\\n\\n                                     [2] M. Lanctot, K. Waugh, M. Bowling, M. Zinkevich, Monte carlo sampling\\n                                         for regret minimization in extensive games, Advances in neural information\\n                                         processing systems 22 (2009) 1078–1086.\\n\\n\\n\\n                              375    [3] N. B. M. L. R. G. M Johanson, M. Bowling, Efficient nash equilibrium ap-\\n                                         proximation through monte carlo counterfactual regret minimization, Aa-\\n                                         mas 2 (2012) 837–846.\\n\\n\\n\\n                                     [4] N. Brown, T. Sandholm, Solving imperfect-information games via dis-\\n                                         counted regret minimization, Proceedings of the AAAI Conference on Ar-\\n                              380        tificial Intelligence 33 (2019) 1829–1836.\\n\\n\\n\\n                                     [5] M. Zinkevich, N. Burch, M. Johanson, O. Tammelin, Heads-up limit holdem\\n                                         poker is solved, Science 347 (2015) 145–149.\\n\\n\\n\\n                                     [6] N. Brown, A. Lerer, S. Gross, T. Sandholm, Deep counterfactual regret\\n                                         minimization, International Conference on Machine Learning (2019) 739–\\n                              385        802.\\n\\n\\n\\n                  Preprint not peer reviewed                                    21\\n         This preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=4806706\\n---\\n                           [7] H. Li, K. Hu, Z. Ge, T. Jiang, Y. Qi, L. Song, Double neural counterfactual\\n                                regret minimization, arXiv preprint arXiv:1812.10607.\\n\\n\\n\\n                           [8] M. Schmid, N. Burch, M. Lanctot, M. Moravcik, R. Kadlec, M. Bowling,\\n                                Variance reduction in monte carlo counterfactual regret minimization (vr-\\n                     390        mccfr) for extensive form games using baselines, Proceedings of the AAAI\\n                                Conference on Artificial Intelligence 33 (2019) 2157–2164.\\n\\n\\n\\n                           [9] H. Li, X. Wang, S. Qi, J. Zhang, Y. Liu, Y. Wu, F. Jia, Solving imperfect-\\n                                information games via exponential counterfactual regret minimization,\\n                                arXiv preprint arXiv:2008.02679.\\n\\n\\n\\n                     395  [10] H. Li, X. Wang, S. Qi, J. Zhang, Y. Liu, Y. Wu, F. Jia, Rlcfr: Minimize\\n                                counterfactual regret by deep reinforcement learning, Expert Systems with\\n                                Applications 187 (2022) 115953. doi:https://doi.org/10.1016/j.eswa.\\n                                2021.115953.\\n\\n\\n\\n                          [11] H. Xu, K. Li, H. Fu, Q. Fu, J. Xing, Autocfr: Learning to design counterfac-\\n                     400        tual regret minimization algorithms, Proceedings of the AAAI Conference\\n                                on Artificial Intelligence 36 (2022) 5244–5251.\\n\\n\\n\\n                          [12] T. Sandholm, Abstraction for solving large incomplete-information games,\\n                                Proceedings of the AAAI Conference on Artificial Intelligence 29. doi:\\n                                https://doi.org/10.1609/aaai.v29i1.9757.\\n\\n\\n\\n                     405  [13] M. Johanson, N. Bard, N. Burch, M. Bowling, Finding optimal abstract\\n                                strategies in extensive-form games, Proceedings of the AAAI Conference\\n                                on Artificial Intelligence 26 (2012) 1371–1379. doi:https://doi.org/10.\\n                                1609/aaai.v26i1.8269.\\n\\n\\n\\n                          [14] K. Waugh, D. Morrill, J. Bagnell, M. Bowling, Solving games with func-\\n                     410        tional regret estimation, Proceedings of the AAAI Conference on Artificial\\n                                Intelligence 29. doi:10.1609/aaai.v29i1.9445.\\n\\n\\n\\n                          [15] M. G. Meyer, Toward generality: Building better counterfactual regret\\n                                minimization for imperfect information games, Ph.D. thesis, Harvard Uni-\\n\\n\\n\\n        Preprint not peer reviewed                                     22\\nThis preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=4806706\\n---\\n                               versity (2022). doi:https://nrs.harvard.edu/URN-3:HUL.INSTREPOS:\\n                     415       37370951.\\n\\n\\n\\n                          [16] R. Gibson, M. Lanctot, N. Burch, D. Szafron, M. Bowling, Generalized\\n                               sampling and variance in counterfactual regret minimization, Proceedings\\n                               of the AAAI Conference on Artificial Intelligence 26 (2012) 1355–1361.\\n                               doi:https://doi.org/10.7939/R3M61BP9B.\\n\\n\\n\\n                     420  [17] R. Gibson, N. Burch, M. Lanctot, D. Szafron, Efficient monte carlo coun-\\n                               terfactual regret minimization in games with many player actions, Ad-\\n                               vances in neural information processing systems 25 (2012) 1880–1888.\\n                               doi:/doi/10.5555/3326943.3327000.\\n\\n\\n\\n                          [18] P. Kuchar, Reducing variance in monte carlo counterfactual regret mini-\\n                     425       mization, Faculty of Electrical Engineering Department of computer sci-\\n                               ence.\\n\\n\\n\\n                          [19] S. Ravichandiran, Hands-On Reinforcement Learning with Python, Packt\\n                               Publishing Ltd, 2018.\\n\\n\\n\\n                          [20] Deep Reinforcement Learning with Python: Master classic RL, deep RL,\\n                     430       distributional RL, inverse RL, and more with OpenAI Gym and Tensor-\\n                               Flow, Packt Publishing Ltd, 2020.\\n\\n\\n\\n                          [21] C. F. Hayes1, R. Radulescu, A practical guide to multi.objective reinforce-\\n                               ment learning and planning, Autonomous Agents and Multi-Agent Systems\\n                               36 (2022) 26. doi:doi.org/10.1007/s10458-022-09552-y.\\n\\n\\n\\n                     435  [22] T. Xu, Y. Liu, Z. Ma, Y. Huang, P. Liu, A dqn-based multi-\\n                               objective participant selection for efficient federated learningdoi:10.\\n                               20944/preprints202304.0734.v1.\\n\\n\\n\\n                          [23] M. Lanctot, E. Lockhart, J. Lespiau, V. Zambaldi, S. Upadhyay, Open-\\n                               spiel: A framework for reinforcement learning in games, arXiv preprint\\n                     440       arXiv:1908.09453.\\n\\n\\n\\n        Preprint not peer reviewed                                    23\\nThis preprint research paper has not been peer reviewed. Electronic copy available at: https://ssrn.com/abstract=4806706', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b1e4f3b-dd2c-49f3-9d05-ac4ffd686a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n techniques.\n",
      "                 in two-player zero-sum games. Forms of tabular CFR have                            1www.computerpokercompetition.org\n",
      "                    *Equal contribution2    1Computer Science Department, Carnegie                  2Deep RL has also been applied successfully to some partially\n",
      "                 Mellon University      Facebook AI Research. Correspondence to:                observed games such as Doom (Lample & Chaplot, 2017), as long\n",
      "                 Noam Brown <noamb@cs.cmu.edu>.                                                 as the hidden information is not too strategically important.\n",
      "\n",
      "\n",
      "\n",
      "                 Proceedings of the 36      th International Conference on Machine\n",
      "                 Learning, Long Beach, California, PMLR 97, 2019. Copyright\n",
      "                 2019 by the author(s).\n",
      "---\n",
      "                                                    Deep Counterfactual Regret Minimization\n",
      "2. Notation and Background                                                  \n"
     ]
    }
   ],
   "source": [
    "print(documents[0].text[6000:7000])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
