{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8f2b7228-4fb3-4f06-b3ec-6ca0c2e3e50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "783e5990-fe21-44b7-9824-1a514588722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "44022779-7159-444d-a142-c9864cae7649",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = getpass()\n",
    "#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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "32be5697-7000-4c36-9b0c-ce186367e5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceInferenceAPI(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000001DBC6AA5AC0>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x000001DBBAE5B100>, completion_to_prompt=<function default_completion_to_prompt at 0x000001DBBAEC2660>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, model_name='mistralai/Mixtral-8x7B-Instruct-v0.1', token='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', timeout=None, headers=None, cookies=None, task=None, context_window=3900, num_output=256, is_chat_model=False, is_function_calling_model=False)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create llm model\n",
    "from llama_index.llms.huggingface import HuggingFaceInferenceAPI\n",
    "llm = HuggingFaceInferenceAPI(model_name=\"mistralai/Mixtral-8x7B-Instruct-v0.1\", token=HF_TOKEN)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0bb8f4ae-ba77-4870-a53e-a769c7a93e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import MetadataMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6cc35a21-49a8-4102-b865-f631e8c18b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.extractors import (\n",
    "    SummaryExtractor,\n",
    "    QuestionsAnsweredExtractor,\n",
    "    TitleExtractor,\n",
    "    KeywordExtractor,\n",
    "    BaseExtractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d6dfacbd-94e8-47d5-b855-5c981336e827",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.extractors.entity import EntityExtractor\n",
    "from llama_index.core.node_parser import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a9de50eb-0186-463d-ab52-a207d7745eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(\n",
    "    separator=\" \", chunk_size=512, chunk_overlap=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4373fbc3-1848-456d-a9ef-80f3a3f49e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractors = [\n",
    "    TitleExtractor(nodes=5, llm=llm),\n",
    "    QuestionsAnsweredExtractor(questions=3, llm=llm),\n",
    "    # EntityExtractor(prediction_threshold=0.5),\n",
    "    # SummaryExtractor(summaries=[\"prev\", \"self\"], llm=llm),\n",
    "    # KeywordExtractor(keywords=10, llm=llm),\n",
    "    # CustomExtractor()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9aa72381-54c8-4c9c-b034-5ec6a3d79b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = [text_splitter] + extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "debfdeba-474e-424d-a760-974f0b9f76a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8cde79d5-b0a5-4e0f-9dc5-59ead0ad5d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-54' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=ClientResponseError(RequestInfo(url=URL('https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B'), method='POST', headers=<CIMultiDictProxy('Host': 'api-inference.huggingface.co', 'user-agent': 'unknown/None; hf_hub/0.20.3; python/3.12.3; torch/2.3.0', 'authorization': 'Bearer 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', 'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate', 'Content-Length': '1762', 'Content-Type': 'application/json')>, real_url=URL('https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B')), (), status=403, message='Forbidden', headers=<CIMultiDictProxy('Date': 'Thu, 02 May 2024 13:52:55 GMT', 'Content-Type': 'application/json', 'Content-Length': '222', 'Connection': 'keep-alive', 'Access-Control-Allow-Credentials': 'true', 'Vary': 'Origin, Access-Control-Request-Method, Access-Control-Request-Headers', 'x-request-id': 'qGsOjGNEyYZGk9PxAHPnw')>)>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py\", line 357, in async_wrapper\n",
      "    result = await func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\async_utils.py\", line 116, in worker\n",
      "    return await job\n",
      "           ^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py\", line 307, in async_wrapper\n",
      "    result = await func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\llms\\llm.py\", line 519, in apredict\n",
      "    response = await self.acomplete(formatted_prompt, formatted=True)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\llms\\huggingface\\base.py\", line 673, in acomplete\n",
      "    response = await self._async_client.text_generation(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py\", line 1557, in text_generation\n",
      "    raise_text_generation_error(e)\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_text_generation.py\", line 534, in raise_text_generation_error\n",
      "    raise http_error\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py\", line 1532, in text_generation\n",
      "    bytes_output = await self.post(json=payload, model=model, task=\"text-generation\", stream=stream)  # type: ignore\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py\", line 257, in post\n",
      "    raise error\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py\", line 226, in post\n",
      "    response.raise_for_status()\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\aiohttp\\client_reqrep.py\", line 1070, in raise_for_status\n",
      "    raise ClientResponseError(\n",
      "aiohttp.client_exceptions.ClientResponseError: 403, message='Forbidden', url=URL('https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B')\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-52' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=ClientResponseError(RequestInfo(url=URL('https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B'), method='POST', headers=<CIMultiDictProxy('Host': 'api-inference.huggingface.co', 'user-agent': 'unknown/None; hf_hub/0.20.3; python/3.12.3; torch/2.3.0', 'authorization': 'Bearer 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', 'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate', 'Content-Length': '2715', 'Content-Type': 'application/json')>, real_url=URL('https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B')), (), status=403, message='Forbidden', headers=<CIMultiDictProxy('Date': 'Thu, 02 May 2024 13:52:55 GMT', 'Content-Type': 'application/json', 'Content-Length': '222', 'Connection': 'keep-alive', 'Vary': 'Origin, Access-Control-Request-Method, Access-Control-Request-Headers', 'x-request-id': '0d2HmtCfhlv3jJz5ZbeGl', 'Access-Control-Allow-Credentials': 'true')>)>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py\", line 357, in async_wrapper\n",
      "    result = await func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\async_utils.py\", line 116, in worker\n",
      "    return await job\n",
      "           ^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py\", line 307, in async_wrapper\n",
      "    result = await func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\llms\\llm.py\", line 519, in apredict\n",
      "    response = await self.acomplete(formatted_prompt, formatted=True)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\llms\\huggingface\\base.py\", line 673, in acomplete\n",
      "    response = await self._async_client.text_generation(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py\", line 1557, in text_generation\n",
      "    raise_text_generation_error(e)\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_text_generation.py\", line 534, in raise_text_generation_error\n",
      "    raise http_error\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py\", line 1532, in text_generation\n",
      "    bytes_output = await self.post(json=payload, model=model, task=\"text-generation\", stream=stream)  # type: ignore\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py\", line 257, in post\n",
      "    raise error\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py\", line 226, in post\n",
      "    response.raise_for_status()\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\aiohttp\\client_reqrep.py\", line 1070, in raise_for_status\n",
      "    raise ClientResponseError(\n",
      "aiohttp.client_exceptions.ClientResponseError: 403, message='Forbidden', url=URL('https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B')\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-64' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=ClientResponseError(RequestInfo(url=URL('https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B'), method='POST', headers=<CIMultiDictProxy('Host': 'api-inference.huggingface.co', 'user-agent': 'unknown/None; hf_hub/0.20.3; python/3.12.3; torch/2.3.0', 'authorization': 'Bearer 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', 'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate', 'Content-Length': '2629', 'Content-Type': 'application/json')>, real_url=URL('https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B')), (), status=403, message='Forbidden', headers=<CIMultiDictProxy('Date': 'Thu, 02 May 2024 13:53:16 GMT', 'Content-Type': 'application/json', 'Content-Length': '222', 'Connection': 'keep-alive', 'x-request-id': 'PNEVCvcD6M4jePJd8Un_K', 'Vary': 'Origin, Access-Control-Request-Method, Access-Control-Request-Headers', 'Access-Control-Allow-Credentials': 'true')>)>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py\", line 357, in async_wrapper\n",
      "    result = await func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\async_utils.py\", line 116, in worker\n",
      "    return await job\n",
      "           ^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py\", line 307, in async_wrapper\n",
      "    result = await func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\llms\\llm.py\", line 519, in apredict\n",
      "    response = await self.acomplete(formatted_prompt, formatted=True)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\llms\\huggingface\\base.py\", line 673, in acomplete\n",
      "    response = await self._async_client.text_generation(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py\", line 1557, in text_generation\n",
      "    raise_text_generation_error(e)\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_text_generation.py\", line 534, in raise_text_generation_error\n",
      "    raise http_error\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py\", line 1532, in text_generation\n",
      "    bytes_output = await self.post(json=payload, model=model, task=\"text-generation\", stream=stream)  # type: ignore\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py\", line 257, in post\n",
      "    raise error\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py\", line 226, in post\n",
      "    response.raise_for_status()\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\aiohttp\\client_reqrep.py\", line 1070, in raise_for_status\n",
      "    raise ClientResponseError(\n",
      "aiohttp.client_exceptions.ClientResponseError: 403, message='Forbidden', url=URL('https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B')\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-63' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\tqdm\\asyncio.py:75> exception=ClientResponseError(RequestInfo(url=URL('https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B'), method='POST', headers=<CIMultiDictProxy('Host': 'api-inference.huggingface.co', 'user-agent': 'unknown/None; hf_hub/0.20.3; python/3.12.3; torch/2.3.0', 'authorization': 'Bearer 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', 'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate', 'Content-Length': '2715', 'Content-Type': 'application/json')>, real_url=URL('https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B')), (), status=403, message='Forbidden', headers=<CIMultiDictProxy('Date': 'Thu, 02 May 2024 13:53:16 GMT', 'Content-Type': 'application/json', 'Content-Length': '222', 'Connection': 'keep-alive', 'Vary': 'Origin, Access-Control-Request-Method, Access-Control-Request-Headers', 'Access-Control-Allow-Credentials': 'true', 'x-request-id': 'WdPp6AULE7WblFwBOb_FA')>)>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\tqdm\\asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "              ^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py\", line 357, in async_wrapper\n",
      "    result = await func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\async_utils.py\", line 116, in worker\n",
      "    return await job\n",
      "           ^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py\", line 307, in async_wrapper\n",
      "    result = await func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\core\\llms\\llm.py\", line 519, in apredict\n",
      "    response = await self.acomplete(formatted_prompt, formatted=True)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\llama_index\\llms\\huggingface\\base.py\", line 673, in acomplete\n",
      "    response = await self._async_client.text_generation(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py\", line 1557, in text_generation\n",
      "    raise_text_generation_error(e)\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_text_generation.py\", line 534, in raise_text_generation_error\n",
      "    raise http_error\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py\", line 1532, in text_generation\n",
      "    bytes_output = await self.post(json=payload, model=model, task=\"text-generation\", stream=stream)  # type: ignore\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py\", line 257, in post\n",
      "    raise error\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py\", line 226, in post\n",
      "    response.raise_for_status()\n",
      "  File \"c:\\Users\\emanu\\miniconda3\\envs\\RAGenv\\Lib\\site-packages\\aiohttp\\client_reqrep.py\", line 1070, in raise_for_status\n",
      "    raise ClientResponseError(\n",
      "aiohttp.client_exceptions.ClientResponseError: 403, message='Forbidden', url=URL('https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B')\n"
     ]
    }
   ],
   "source": [
    "uber_docs = SimpleDirectoryReader(input_files=[\"CFR.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "97c8fd17-79e8-4877-b25a-a6c52cb5f2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_front_pages = uber_docs[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1dd23602-9b2b-4251-b031-974530394ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_content = uber_docs[63:69]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "677f29c3-3881-4092-b8c7-bc68bd86488b",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_docs = uber_front_pages + uber_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "bb590513-fe29-49d2-aca1-12695b938051",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  7.45it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00,  5.05it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  9.97it/s]\n",
      "100%|██████████| 12/12 [00:53<00:00,  4.47s/it]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "pipeline = IngestionPipeline(transformations=transformations)\n",
    "uber_nodes = pipeline.run(documents=uber_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "db57fcd6-1f54-4209-9e0a-ad8ac429f439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_label': '1',\n",
       " 'file_name': 'CFR.pdf',\n",
       " 'file_path': 'CFR.pdf',\n",
       " 'file_type': 'application/pdf',\n",
       " 'file_size': 4407076,\n",
       " 'creation_date': '2024-05-02',\n",
       " 'last_modified_date': '2024-05-02',\n",
       " 'document_title': ' Deep Counterfactual Regret Minimization for Imperfect-Information Games.',\n",
       " 'questions_this_excerpt_can_answer': \"1. What is the CFR algorithm and how does it converge to a Nash equilibrium in two-player zero-sum games?\\n2. How have forms of tabular CFR been used in the benchmark domain of poker and in the Annual Computer Poker Competition?\\n3. What are the limitations of using tabular CFR with abstraction and how does Deep Counterfactual Regret Minimization address these limitations?\\n\\n1. The CFR algorithm is an iterative method for finding a Nash equilibrium in two-player zero-sum games. It works by having each player simulate playing the game many times, keeping track of the regret they feel for not playing a different strategy. Over time, the regret decreases and the players' strategies converge to a Nash equilibrium.\\n2. Tabular CFR has been used in all recent milestones in the benchmark domain of poker and in all competitive agents in the Annual Computer Poker Competition for at least six years. It has been used to solve large imperfect-information games by abstracting them and solving the simplified game using tabular CFR. However, this approach requires extensive domain knowledge and the abstract solution may only be a coarse approximation of a\"}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uber_nodes[1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8b6cae55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex\n",
    "index = SummaryIndex.from_documents(uber_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "31626f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "12feae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"Give me an introduction to CFR for my paper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e94cef8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response=\"\\n\\nCounterfactual Regret Minimization (CFR) is an iterative algorithm that converges to a Nash equilibrium in any ﬁnite two-player zero-sum game. It is a theoretically grounded algorithm with a convergence bound of O(1√\\nT) and has been used in practice to solve large imperfect-information games. CFR works by iteratively traversing the game tree and computing the counterfactual value of each player's actions at each infoset. The counterfactual value is the expected payoff to the player if they had taken a particular action, weighted by the probability that the player would have reached that infoset if they had tried to do so. The algorithm then updates the player's strategy based on the counterfactual values, using a regret minimization algorithm such as regret matching.\\n\\nIn practice, faster convergence is achieved by using Monte Carlo CFR (MCCFR), in which only a portion of the game tree is traversed on each iteration. In MCCFR, a subset of nodes Qtin the game tree is sampled at each iteration, and sampled regrets ˜rtare tracked rather than exact regrets. For infosets that\", source_nodes=[NodeWithScore(node=TextNode(id_='ac24b165-8cb8-441d-b06f-62439a7d966c', embedding=None, metadata={'page_label': '1', 'file_name': 'CFR.pdf', 'file_path': 'CFR.pdf', 'file_type': 'application/pdf', 'file_size': 4407076, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='428330ae-6768-4bba-91ca-56ea5f61dcc9', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 'CFR.pdf', 'file_path': 'CFR.pdf', 'file_type': 'application/pdf', 'file_size': 4407076, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02'}, hash='cd69276d532b96adb186b78a811fa5ae419d1b77e84137bbd222b3a80febcf8f'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='23caef9e-a2c7-4246-9438-a61b6d276b59', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='a52be9cd92a53d16b9993d90ac1ace6e35ec3576144017db469fbedd866517f7')}, text='Deep Counterfactual Regret Minimization\\nNoam Brown* 1 2Adam Lerer* 2Sam Gross2Tuomas Sandholm1\\nAbstract\\nCounterfactual Regret Minimization (CFR) is the\\nleading framework for solving large imperfect-\\ninformation games. It converges to an equilibrium\\nby iteratively traversing the game tree. In order\\nto deal with extremely large games, abstraction\\nis typically applied before running CFR. The ab-\\nstracted game is solved with tabular CFR, and its\\nsolution is mapped back to the full game. This\\nprocess can be problematic because aspects of\\nabstraction are often manual and domain speciﬁc,\\nabstraction algorithms may miss important strate-\\ngic nuances of the game, and there is a chicken-\\nand-egg problem because determining a good ab-\\nstraction requires knowledge of the equilibrium\\nof the game. This paper introduces Deep Counter-\\nfactual Regret Minimization , a form of CFR that\\nobviates the need for abstraction by instead using\\ndeep neural networks to approximate the behavior\\nof CFR in the full game. We show that Deep CFR\\nis principled and achieves strong performance in\\nlarge poker games. This is the ﬁrst non-tabular\\nvariant of CFR to be successful in large games.\\n1. Introduction\\nImperfect-information games model strategic interactions\\nbetween multiple agents with only partial information. They\\nare widely applicable to real-world domains such as negoti-\\nations, auctions, and cybersecurity interactions. Typically\\nin such games, one wishes to ﬁnd an approximate equilib-\\nrium in which no player can improve by deviating from the\\nequilibrium.\\nThe most successful family of algorithms for imperfect-\\ninformation games have been variants of Counterfactual\\nRegret Minimization (CFR) (Zinkevich et al., 2007). CFR is\\nan iterative algorithm that converges to a Nash equilibrium\\nin two-player zero-sum games. Forms of tabular CFR have\\n*Equal contribution1Computer Science Department, Carnegie\\nMellon University2Facebook AI Research. Correspondence to:\\nNoam Brown <noamb@cs.cmu.edu >.\\nProceedings of the 36thInternational Conference on Machine\\nLearning , Long Beach, California, PMLR 97, 2019. Copyright\\n2019 by the author(s).been used in all recent milestones in the benchmark domain\\nof poker (Bowling et al., 2015; Morav ˇc´ık et al., 2017; Brown\\n& Sandholm, 2017) and have been used in all competitive\\nagents in the Annual Computer Poker Competition going\\nback at least six years.1In order to deal with extremely\\nlarge imperfect-information games, abstraction is typically\\nused to simplify a game by bucketing similar states together\\nand treating them identically. The simpliﬁed (abstracted)\\ngame is approximately solved via tabular CFR. However,\\nconstructing an effective abstraction requires extensive do-\\nmain knowledge and the abstract solution may only be a\\ncoarse approximation of a true equilibrium.\\nIn constrast, reinforcement learning has been successfully\\nextended to large state spaces by using function approx-\\nimation with deep neural networks rather than a tabular\\nrepresentation of the policy (deep RL). This approach has\\nled to a number of recent breakthroughs in constructing\\nstrategies in large MDPs (Mnih et al., 2015) as well as in\\nzero-sum perfect-information games such as Go (Silver\\net al., 2017; 2018).2Importantly, deep RL can learn good\\nstrategies with relatively little domain knowledge for the\\nspeciﬁc game (Silver et al., 2017). However, most popular\\nRL algorithms do not converge to good policies (equilibria)\\nin imperfect-information games in theory or in practice.\\nRather than use tabular CFR with abstraction, this paper\\nintroduces a form of CFR, which we refer to as Deep Coun-\\nterfactual Regret Minimization , that uses function approx-\\nimation with deep neural networks to approximate the be-\\nhavior of tabular CFR on the full, unabstracted game. We\\nprove that Deep CFR converges to an ϵ-Nash equilibrium\\nin two-player zero-sum games and empirically evaluate per-\\nformance in poker variants, including heads-up limit Texas\\nhold’em. We show Deep CFR outperforms Neural Ficti-\\ntious Self Play (NFSP) (Heinrich & Silver, 2016), which\\nwas the prior leading function approximation algorithm for\\nimperfect-information games, and that Deep CFR is com-\\npetitive with domain-speciﬁc tabular abstraction techniques.', start_char_idx=0, end_char_idx=4247, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None), NodeWithScore(node=TextNode(id_='23caef9e-a2c7-4246-9438-a61b6d276b59', embedding=None, metadata={'page_label': '1', 'file_name': 'CFR.pdf', 'file_path': 'CFR.pdf', 'file_type': 'application/pdf', 'file_size': 4407076, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='428330ae-6768-4bba-91ca-56ea5f61dcc9', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 'CFR.pdf', 'file_path': 'CFR.pdf', 'file_type': 'application/pdf', 'file_size': 4407076, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02'}, hash='cd69276d532b96adb186b78a811fa5ae419d1b77e84137bbd222b3a80febcf8f'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='ac24b165-8cb8-441d-b06f-62439a7d966c', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': 'CFR.pdf', 'file_path': 'CFR.pdf', 'file_type': 'application/pdf', 'file_size': 4407076, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02'}, hash='73631cf34ab11f68d09f6c868e39424365f4452ac42265c15c87487d457bac3c'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='9ac6e6cc-df2b-44e6-bbba-cef753334b1e', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='5743a6a225779bb8835fc042eeeb7b5004716b3e891f429ac66d02c6023ded66')}, text='Rather than use tabular CFR with abstraction, this paper\\nintroduces a form of CFR, which we refer to as Deep Coun-\\nterfactual Regret Minimization , that uses function approx-\\nimation with deep neural networks to approximate the be-\\nhavior of tabular CFR on the full, unabstracted game. We\\nprove that Deep CFR converges to an ϵ-Nash equilibrium\\nin two-player zero-sum games and empirically evaluate per-\\nformance in poker variants, including heads-up limit Texas\\nhold’em. We show Deep CFR outperforms Neural Ficti-\\ntious Self Play (NFSP) (Heinrich & Silver, 2016), which\\nwas the prior leading function approximation algorithm for\\nimperfect-information games, and that Deep CFR is com-\\npetitive with domain-speciﬁc tabular abstraction techniques.\\n1www.computerpokercompetition.org\\n2Deep RL has also been applied successfully to some partially\\nobserved games such as Doom (Lample & Chaplot, 2017), as long\\nas the hidden information is not too strategically important.arXiv:1811.00164v3  [cs.AI]  22 May 2019', start_char_idx=3503, end_char_idx=4507, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None), NodeWithScore(node=TextNode(id_='9ac6e6cc-df2b-44e6-bbba-cef753334b1e', embedding=None, metadata={'page_label': '2', 'file_name': 'CFR.pdf', 'file_path': 'CFR.pdf', 'file_type': 'application/pdf', 'file_size': 4407076, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='7d3ac9ef-23ef-4b70-9f85-8ef79f7d290e', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '2', 'file_name': 'CFR.pdf', 'file_path': 'CFR.pdf', 'file_type': 'application/pdf', 'file_size': 4407076, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02'}, hash='c65621582c2f040475c693d47863818986e4445aa1d3cb4728b9a2ca055773e4'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='23caef9e-a2c7-4246-9438-a61b6d276b59', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': 'CFR.pdf', 'file_path': 'CFR.pdf', 'file_type': 'application/pdf', 'file_size': 4407076, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02'}, hash='376c8a4776bca60f06565f27bd3f435b83e738bfe9ddb50d68fb726feec7ebd7'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='5dc5e362-436a-4be7-bb3a-bb6c293e8150', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='1dcd6604151bb0d36d86136e9b20df53535e0f9e0211a802cd4df41e1c14299d')}, text='Deep Counterfactual Regret Minimization\\n2. Notation and Background\\nIn an imperfect-information extensive-form (that is, tree-\\nform) game there is a ﬁnite set of players, P. A node\\n(or history) his deﬁned by all information of the current\\nsituation, including private knowledge known to only one\\nplayer.A(h)denotes the actions available at a node and\\nP(h)is either chance or the unique player who acts at that\\nnode. If action a∈A(h)leads fromhtoh′, then we write\\nh·a=h′. We writeh⊏h′if a sequence of actions leads\\nfromhtoh′.His the set of all nodes. Z⊆Hare terminal\\nnodes for which no actions are available. For each player\\np∈P , there is a payoff function up:Z→R. In this\\npaper we assumeP={1,2}andu1=−u2(the game is\\ntwo-player zero-sum). We denote the range of payoffs in\\nthe game by ∆.\\nImperfect information is represented by information sets\\n(infosets) for each player p∈P . For any infoset Ibe-\\nlonging top, all nodesh,h′∈Iare indistinguishable to\\np. Moreover, every non-terminal node h∈Hbelongs to\\nexactly one infoset for each p. We represent the set of all\\ninfosets belonging to pwherepacts byIp. We call the\\nset of all terminal nodes with a preﬁx in IasZI, and we\\ncall the particular preﬁx z[I]. We assume the game features\\nperfect recall , which means if handh′do not share a player\\npinfoset then all nodes following hdo not share a player p\\ninfoset with any node following h′.\\nA strategy (or policy) σ(I)is a probability vector over ac-\\ntions for acting player pin infosetI. Since all states in an\\ninfoset belonging to pare indistinguishable, the strategies\\nin each of them must be identical. The set of actions in Iis\\ndenoted byA(I). The probability of a particular action ais\\ndenoted byσ(I,a). We deﬁne σpto be a strategy for pin\\nevery infoset in the game where pacts. A strategy proﬁle σ\\nis a tuple of strategies, one for each player. The strategy of\\nevery player other than pis represented as σ−p.up(σp,σ−p)\\nis the expected payoff for pif playerpplays according to\\nσpand the other players play according to σ−p.\\nπσ(h) = Πh′·a⊑hσP(h′)(h′,a)is called reach and is the\\nprobabilityhis reached if all players play according to σ.\\nπσ\\np(h)is the contribution of pto this probability. πσ\\n−p(h)\\nis the contribution of chance and all players other than p.\\nFor an infoset Ibelonging to p, the probability of reaching\\nIifpchooses actions leading toward Ibut chance and all\\nplayers other than pplay according to σ−pis denoted by\\nπσ\\n−p(I) =∑\\nh∈Iπσ\\n−p(h). Forh⊑z, deﬁneπσ(h→z) =\\nΠh′·a⊑z,h′̸⊏hσP(h′)(h′,a)\\nAbest response toσ−pis a playerpstrategyBR(σ−p)\\nsuch thatup(\\nBR(σ−p),σ−p)\\n= maxσ′pup(σ′\\np,σ−p). A\\nNash equilibrium σ∗is a strategy proﬁle where ev-\\neryone plays a best response: ∀p,up(σ∗\\np,σ∗\\n−p) =\\nmaxσ′pup(σ′\\np,σ∗\\n−p)(Nash, 1950). The exploitability e(σp)of a strategy σpin a two-player zero-sum game is how much\\nworseσpdoes versus BR(σp)compared to how a Nash\\nequilibrium strategy σ∗\\npdoes against BR(σ∗\\np). Formally,\\ne(σp) =up(\\nσ∗\\np,BR(σ∗\\np))\\n−up(\\nσp,BR(σp))\\n.', start_char_idx=0, end_char_idx=2965, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None), NodeWithScore(node=TextNode(id_='5dc5e362-436a-4be7-bb3a-bb6c293e8150', embedding=None, metadata={'page_label': '2', 'file_name': 'CFR.pdf', 'file_path': 'CFR.pdf', 'file_type': 'application/pdf', 'file_size': 4407076, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='7d3ac9ef-23ef-4b70-9f85-8ef79f7d290e', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '2', 'file_name': 'CFR.pdf', 'file_path': 'CFR.pdf', 'file_type': 'application/pdf', 'file_size': 4407076, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02'}, hash='c65621582c2f040475c693d47863818986e4445aa1d3cb4728b9a2ca055773e4'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='9ac6e6cc-df2b-44e6-bbba-cef753334b1e', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '2', 'file_name': 'CFR.pdf', 'file_path': 'CFR.pdf', 'file_type': 'application/pdf', 'file_size': 4407076, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02'}, hash='88e352475b879808d8b428a8cdbdb1759647749cb66aeab71820bacf6ee4307d'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='9337b3d6-f064-4598-a2c4-8ef835eee54c', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='97e9deb51f46beecbfeac2c1217a51105512088b98d09275caa103c3807daf78')}, text='A\\nNash equilibrium σ∗is a strategy proﬁle where ev-\\neryone plays a best response: ∀p,up(σ∗\\np,σ∗\\n−p) =\\nmaxσ′pup(σ′\\np,σ∗\\n−p)(Nash, 1950). The exploitability e(σp)of a strategy σpin a two-player zero-sum game is how much\\nworseσpdoes versus BR(σp)compared to how a Nash\\nequilibrium strategy σ∗\\npdoes against BR(σ∗\\np). Formally,\\ne(σp) =up(\\nσ∗\\np,BR(σ∗\\np))\\n−up(\\nσp,BR(σp))\\n. We mea-\\nsure total exploitability∑\\np∈Pe(σp)3.\\n2.1. Counterfactual Regret Minimization (CFR)\\nCFR is an iterative algorithm that converges to a Nash equi-\\nlibrium in any ﬁnite two-player zero-sum game with a the-\\noretical convergence bound of O(1√\\nT). In practice CFR\\nconverges much faster. We provide an overview of CFR be-\\nlow; for a full treatment, see Zinkevich et al. (2007). Some\\nrecent forms of CFR converge in O(1\\nT0.75)in self-play set-\\ntings (Farina et al., 2019), but are slower in practice so we\\ndo not use them in this paper.\\nLetσtbe the strategy proﬁle on iteration t. The counter-\\nfactual value vσ(I)of playerp=P(I)atIis the expected\\npayoff topwhen reaching I, weighted by the probability\\nthatpwould reached Iif she tried to do so that iteration.\\nFormally,\\nvσ(I) =∑\\nz∈ZIπσ\\n−p(z[I])πσ(z[I]→z)up(z) (1)\\nandvσ(I,a)is the same except it assumes that player p\\nplays action aat infosetIwith 100% probability.\\nTheinstantaneous regret rt(I,a)is the difference between\\nP(I)’s counterfactual value from playing avs. playing σ\\non iterationt\\nrt(I,a) =vσt(I,a)−vσt(I) (2)\\nThecounterfactual regret for infosetIactionaon iteration\\nTis\\nRT(I,a) =T∑\\nt=1rt(I,a) (3)\\nAdditionally, RT\\n+(I,a) = max{RT(I,a),0}andRT(I) =\\nmaxa{RT(I,a)}.Total regret forpin the entire game is\\nRT\\np= maxσ′p∑T\\nt=1(\\nup(σ′\\np,σt\\n−p)−up(σt\\np,σt\\n−p))\\n.\\nCFR determines an iteration’s strategy by applying any of\\nseveral regret minimization algorithms to each infoset (Lit-\\ntlestone & Warmuth, 1994; Chaudhuri et al., 2009). Typi-\\ncally, regret matching (RM) is used as the regret minimiza-\\ntion algorithm within CFR due to RM’s simplicity and lack\\nof parameters (Hart & Mas-Colell, 2000).\\nIn RM, a player picks a distribution over actions in an in-\\nfoset in proportion to the positive regret on those actions.\\nFormally, on each iteration t+ 1,pselects actions a∈A(I)\\n3Some prior papers instead measure average exploitability\\nrather than total (summed) exploitability.', start_char_idx=2598, end_char_idx=4896, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None), NodeWithScore(node=TextNode(id_='9337b3d6-f064-4598-a2c4-8ef835eee54c', embedding=None, metadata={'page_label': '3', 'file_name': 'CFR.pdf', 'file_path': 'CFR.pdf', 'file_type': 'application/pdf', 'file_size': 4407076, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='2a954076-d0fa-4c98-afc0-39c3c177c8f6', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '3', 'file_name': 'CFR.pdf', 'file_path': 'CFR.pdf', 'file_type': 'application/pdf', 'file_size': 4407076, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02'}, hash='030588681084e58d5a07686f9cb79bbd19cdd3d744c5adce19ab99050ee43abe'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='5dc5e362-436a-4be7-bb3a-bb6c293e8150', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '2', 'file_name': 'CFR.pdf', 'file_path': 'CFR.pdf', 'file_type': 'application/pdf', 'file_size': 4407076, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02'}, hash='178ad236b27918fe7dc6b70b42ece917b5c5d4d50026aeeed239872441db1b88'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='133f7218-fd02-4a69-a8dc-5816cdbf56db', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='5d55dccb6c4dc4b4aa51183370c104f43f3158ffa6c8ba045329e16ebd37d3e4')}, text='Deep Counterfactual Regret Minimization\\naccording to probabilities\\nσt+1(I,a) =Rt\\n+(I,a)∑\\na′∈A(I)Rt\\n+(I,a′)(4)\\nIf∑\\na′∈A(I)Rt\\n+(I,a′) = 0 then any arbitrary strategy may\\nbe chosen. Typically each action is assigned equal proba-\\nbility, but in this paper we choose the action with highest\\ncounterfactual regret with probability 1, which we ﬁnd em-\\npirically helps RM better cope with approximation error\\n(see Figure 4).\\nIf a player plays according to regret matching in in-\\nfosetIon every iteration, then on iteration T,RT(I)≤\\n∆√\\n|A(I)|√\\nT(Cesa-Bianchi & Lugosi, 2006). Zinkevich\\net al. (2007) show that the sum of the counterfactual regret\\nacross all infosets upper bounds the total regret. Therefore,\\nif playerpplays according to CFR on every iteration, then\\nRT\\np≤∑\\nI∈IpRT(I). So, asT→∞ ,RT\\np\\nT→0.\\nThe average strategy ¯σT\\np(I)for an infoset Ion iteration T\\nis¯σT\\np(I) =∑T\\nt=1(\\nπσt\\np(I)σt\\np(I))\\n∑T\\nt=1πσt\\np(I).\\nIn two-player zero-sum games, if both players’ average\\ntotal regret satisﬁesRT\\np\\nT≤ϵ, then their average strategies\\n⟨¯σT\\n1,¯σT\\n2⟩form a 2ϵ-Nash equilibrium (Waugh, 2009). Thus,\\nCFR constitutes an anytime algorithm for ﬁnding an ϵ-Nash\\nequilibrium in two-player zero-sum games.\\nIn practice, faster convergence is achieved by alternating\\nwhich player updates their regrets on each iteration rather\\nthan updating the regrets of both players simultaneously\\neach iteration, though this complicates the theory (Farina\\net al., 2018; Burch et al., 2018). We use the alternating-\\nupdates form of CFR in this paper.\\n2.2. Monte Carlo Counterfactual Regret Minimization\\nVanilla CFR requires full traversals of the game tree, which\\nis infeasible in large games. One method to combat this is\\nMonte Carlo CFR (MCCFR), in which only a portion of\\nthe game tree is traversed on each iteration (Lanctot et al.,\\n2009). In MCCFR, a subset of nodes Qtin the game tree is\\ntraversed at each iteration, where Qtis sampled from some\\ndistributionQ. Sampled regrets ˜rtare tracked rather than\\nexact regrets. For infosets that are sampled at iteration t,\\n˜rt(I,a)is equal tort(I,a)divided by the probability of\\nhaving sampled I; for unsampled infosets ˜rt(I,a) = 0 . See\\nAppendix B for more details.\\nThere exist a number of MCCFR variants (Gibson et al.,\\n2012; Johanson et al., 2012; Jackson, 2017), but for this\\npaper we focus speciﬁcally on the external sampling variant\\ndue to its simplicity and strong performance. In external-\\nsampling MCCFR the game tree is traversed for one player\\nat a time, alternating back and forth. We refer to the playerwho is traversing the game tree on the iteration as the tra-\\nverser . Regrets are updated only for the traverser on an\\niteration. At infosets where the traverser acts, all actions are\\nexplored. At other infosets and chance nodes, only a single\\naction is explored.\\nExternal-sampling MCCFR probabilistically converges to\\nan equilibrium. For any ρ∈(0,1], total regret is bounded\\nbyRT\\np≤(\\n1 +√\\n2√ρ)\\n|Ip|∆√\\n|A|√\\nTwith probability 1−ρ.\\n3. Related Work\\nCFR is not the only iterative algorithm capable of solving\\nlarge imperfect-information games. First-order methods\\nconverge to a Nash equilibrium in O(1/T)(Hoda et al.,\\n2010; Kroer et al., 2018b;a), which is far better than CFR’s\\ntheoretical bound.', start_char_idx=0, end_char_idx=3224, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None), NodeWithScore(node=TextNode(id_='133f7218-fd02-4a69-a8dc-5816cdbf56db', embedding=None, metadata={'page_label': '3', 'file_name': 'CFR.pdf', 'file_path': 'CFR.pdf', 'file_type': 'application/pdf', 'file_size': 4407076, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='2a954076-d0fa-4c98-afc0-39c3c177c8f6', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '3', 'file_name': 'CFR.pdf', 'file_path': 'CFR.pdf', 'file_type': 'application/pdf', 'file_size': 4407076, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02'}, hash='030588681084e58d5a07686f9cb79bbd19cdd3d744c5adce19ab99050ee43abe'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='9337b3d6-f064-4598-a2c4-8ef835eee54c', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '3', 'file_name': 'CFR.pdf', 'file_path': 'CFR.pdf', 'file_type': 'application/pdf', 'file_size': 4407076, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02'}, hash='0a4884f362d79b34474c19892499ccdd403f96f5832aef2319045685d9f9014b')}, text='Regrets are updated only for the traverser on an\\niteration. At infosets where the traverser acts, all actions are\\nexplored. At other infosets and chance nodes, only a single\\naction is explored.\\nExternal-sampling MCCFR probabilistically converges to\\nan equilibrium. For any ρ∈(0,1], total regret is bounded\\nbyRT\\np≤(\\n1 +√\\n2√ρ)\\n|Ip|∆√\\n|A|√\\nTwith probability 1−ρ.\\n3. Related Work\\nCFR is not the only iterative algorithm capable of solving\\nlarge imperfect-information games. First-order methods\\nconverge to a Nash equilibrium in O(1/T)(Hoda et al.,\\n2010; Kroer et al., 2018b;a), which is far better than CFR’s\\ntheoretical bound. However, in practice the fastest variants\\nof CFR are substantially faster than the best ﬁrst-order meth-\\nods. Moreover, CFR is more robust to error and therefore\\nlikely to do better when combined with function approxima-\\ntion.\\nNeural Fictitious Self Play (NFSP) (Heinrich & Silver,\\n2016) previously combined deep learning function approx-\\nimation with Fictitious Play (Brown, 1951) to produce an\\nAI for heads-up limit Texas hold’em, a large imperfect-\\ninformation game. However, Fictitious Play has weaker\\ntheoretical convergence guarantees than CFR, and in prac-\\ntice converges slower. We compare our algorithm to NFSP\\nin this paper. Model-free policy gradient algorithms have\\nbeen shown to minimize regret when parameters are tuned\\nappropriately (Srinivasan et al., 2018) and achieve perfor-\\nmance comparable to NFSP.\\nPast work has investigated using deep learning to esti-\\nmate values at the depth limit of a subgame in imperfect-\\ninformation games (Morav ˇc´ık et al., 2017; Brown et al.,\\n2018). However, tabular CFR was used within the sub-\\ngames themselves. Large-scale function approximated CFR\\nhas also been developed for single-agent settings (Jin et al.,\\n2017). Our algorithm is intended for the multi-agent set-\\nting and is very different from the one proposed for the\\nsingle-agent setting.\\nPrior work has combined regression tree function approxi-\\nmation with CFR (Waugh et al., 2015) in an algorithm called\\nRegression CFR (RCFR) . This algorithm deﬁnes a number\\nof features of the infosets in a game and calculates weights\\nto approximate the regrets that a tabular CFR implemen-\\ntation would produce. Regression CFR is algorithmically\\nsimilar to Deep CFR, but uses hand-crafted features similar\\nto those used in abstraction, rather than learning the features.\\nRCFR also uses full traversals of the game tree (which is\\ninfeasible in large games) and has only been evaluated on\\ntoy games. It is therefore best viewed as the ﬁrst proof of\\nconcept that function approximation can be applied to CFR.', start_char_idx=2601, end_char_idx=5232, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None)], metadata={'ac24b165-8cb8-441d-b06f-62439a7d966c': {'page_label': '1', 'file_name': 'CFR.pdf', 'file_path': 'CFR.pdf', 'file_type': 'application/pdf', 'file_size': 4407076, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02'}, '23caef9e-a2c7-4246-9438-a61b6d276b59': {'page_label': '1', 'file_name': 'CFR.pdf', 'file_path': 'CFR.pdf', 'file_type': 'application/pdf', 'file_size': 4407076, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02'}, '9ac6e6cc-df2b-44e6-bbba-cef753334b1e': {'page_label': '2', 'file_name': 'CFR.pdf', 'file_path': 'CFR.pdf', 'file_type': 'application/pdf', 'file_size': 4407076, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02'}, '5dc5e362-436a-4be7-bb3a-bb6c293e8150': {'page_label': '2', 'file_name': 'CFR.pdf', 'file_path': 'CFR.pdf', 'file_type': 'application/pdf', 'file_size': 4407076, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02'}, '9337b3d6-f064-4598-a2c4-8ef835eee54c': {'page_label': '3', 'file_name': 'CFR.pdf', 'file_path': 'CFR.pdf', 'file_type': 'application/pdf', 'file_size': 4407076, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02'}, '133f7218-fd02-4a69-a8dc-5816cdbf56db': {'page_label': '3', 'file_name': 'CFR.pdf', 'file_path': 'CFR.pdf', 'file_type': 'application/pdf', 'file_size': 4407076, 'creation_date': '2024-05-02', 'last_modified_date': '2024-05-02'}})"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "de6370ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "\n",
       "Counterfactual Regret Minimization (CFR) is an iterative algorithm that converges to a Nash equilibrium in any ﬁnite two-player zero-sum game. It is a theoretically grounded algorithm with a convergence bound of O(1√\n",
       "T) and has been used in practice to solve large imperfect-information games. CFR works by iteratively traversing the game tree and computing the counterfactual value of each player's actions at each infoset. The counterfactual value is the expected payoff to the player if they had taken a particular action, weighted by the probability that the player would have reached that infoset if they had tried to do so. The algorithm then updates the player's strategy based on the counterfactual values, using a regret minimization algorithm such as regret matching.\n",
       "\n",
       "In practice, faster convergence is achieved by using Monte Carlo CFR (MCCFR), in which only a portion of the game tree is traversed on each iteration. In MCCFR, a subset of nodes Qtin the game tree is sampled at each iteration, and sampled regrets ˜rtare tracked rather than exact regrets. For infosets that</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
